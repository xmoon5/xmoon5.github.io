[{"categories":["深度学习"],"content":"在卷积网络六大模块中的BN（批批标准化）所指的就是Batch Normalization，该算法15年提出，现在已经成为深度学习中经常使用的技术，可以极大的提高网络的处理能力。 ","date":"2020-08-13","objectID":"/batchnormalization/:0:0","tags":["Batch Normalization","算法"],"title":"经典算法：Batch Normalization","uri":"/batchnormalization/"},{"categories":["深度学习"],"content":"Feature Scaling 特征缩放 在没有进行Feature Scaling之前，如果两个输入数据$x_1,x_2$的distribution很不均匀的话，导致对$w_2$计算结果的影响比较大，所以训练的时候，横纵方向上需要给与一个不同的training rate，在$w_1$方向需要一个更大的learning rate，$w_2$方向给与一个较小的learning rate，不过这样做的办法却不见得很简单。所以对不同Feature做了normalization之后，使得error surface看起来比较接近正圆的话，就可以使训练容易得多。 优势： 经过处理后error surface更接近一个圆，gradient在横的方向上和纵的方向上变化差不多，使得training变得更容易，如果error suface是一个椭圆，不同的方向上要非常不一样的学习率，例如：在横的方向上给比较大的学习率，纵的方向上给比较小的学习率，给不同的参数不同的学习率是有办法的，但不见得那么好做。如果可以把不同的feature做Normalization，让error surface看起来比较接近正圆的话，是会让training容易得多。 ","date":"2020-08-13","objectID":"/batchnormalization/:1:0","tags":["Batch Normalization","算法"],"title":"经典算法：Batch Normalization","uri":"/batchnormalization/"},{"categories":["深度学习"],"content":"经典的Feature Scaling 现在有一大堆的数据，训练数据总共有$R$笔data。然后对每一个dimension去计算dimension的mean跟dimension的standard deviation，假设下图的input是39维，所以就算出39个mean跟39个standard deviation；然后对每一维中的数值，$\\frac{x^r_i-m_i}{\\sigma_i}$作为一个Normalization，你就使第$i$维的feature的分布为$mean=0，variance=1$。 ","date":"2020-08-13","objectID":"/batchnormalization/:2:0","tags":["Batch Normalization","算法"],"title":"经典算法：Batch Normalization","uri":"/batchnormalization/"},{"categories":["深度学习"],"content":"Internal Covariate Shift 如上图所示：每一个人代表1个layer，然后他们中间是用话筒连在一起，而当一个人手上的两边的话筒被接在一起的时候，整个network的传输才会顺利，才会得到好的效果。 看到上面一排中间那个人，左手边的话筒比较高，右手边的话筒比较低。在训练的时候为了将两个话筒拉到同一个水平高度，它会将左手边的话筒放低一点，同时右手的话筒放高一点，因为是同时两边都变，所以就可能出现了下面的图，最后还是没对上。 在过去的解决方法是调小learning rate，因为没对上就是因为学习率太大导致的，小的learnin rate又会导致训练速度变得很慢。 ","date":"2020-08-13","objectID":"/batchnormalization/:3:0","tags":["Batch Normalization","算法"],"title":"经典算法：Batch Normalization","uri":"/batchnormalization/"},{"categories":["深度学习"],"content":"Batch Normalization原理 batch Normalization就是对每一个layer做Feature Scaling，就可以解决Internal Covariate Shift问题。 训练过程参数在调整的时候前一个层是后一个层的输入，当前一个层的参数改变之后也会改变后一层的参数。当后面的参数按照前面的参数学好了之后前面的layer就变了，因为前面的layer也是不断在变的。如果输入normalization的数据，因为输入是固定下来的，具有相同的均值和方差，training就会更容易。 定义网络总共有$L$层（不包含输入层） 相关参数： $l$ ：网络中的层标号 $L$ ：网络中的最后一层或总层数 $d_l$ ：第 $l$ 层的维度，即神经元结点数 $W^{[l]}$ ：第 $l$ 层的权重矩阵， $W^{[l]} \\in \\R^{d_l\\times d_{l-1}}$ $b^{[l]}$ ：第 $l$ 层的偏置向量， $b^{l}\\in \\R^{d_l\\times 1}$ $Z^{[l]}$ ：第$l$ 层的线性计算结果，$Z^{[l]}=W^{[l]}\\times input +b^{[l]}$ $g^{[l]}(\\cdot)$ ：第 $l$ 层的激活函数 $A^{[l]}$ ：第 $l$ 层的非线性激活结果，$A^{[l]} = g^{[l]}(Z^{[l]})$ 相关样本： $M$ ：训练样本的数量 $N$ ：训练样本的特征数 $X$ ：训练样本集，$X=\\lbrace x^{(1)},x^{(2)}, … ,x^{(M)}\\rbrace$ （注意这里 $M$ 的一列是一个样本） $m$ ：batch size，即每个batch中样本的数量 $X^{(i)}$：第 $i$ 个mini-batch的训练数据， $X=\\lbrace x^{(1)},x^{(2)}, … ,x^{(k)}\\rbrace$，其中 $X^{(i)}\\in \\R^{N\\times m}$ 计算： 对每个特征进行独立的normalization。考虑一个batch的训练，传入m个训练样本，并关注网络中的某一层，忽略上标 $l$ $$Z\\in \\R^{d_l\\times m}$$ 当前层的第 $j$ 个维度，也就是第 $j$ 个神经元结点 ，则有$Z\\in \\R^{1\\times m}$。 当前维度进行规范化： $$\\mu_j = \\frac1m\\sum^m_{i=1}Z^{(i)}_j$$ $$\\sigma^2_j = \\frac 1m\\sum^m_{i=1}(Z^{(i)}_j-\\mu_j)^2$$ $$\\hat Z_j=\\frac{Z_j-\\mu _j}{\\sqrt{\\sigma^2_j+\\epsilon}}$$ 其中$\\epsilon$是为了防止方差为0产生无效计算。 结合个具体的例子来进行计算： 下图只关注第 $l$ 层的计算结果，左边的矩阵是 $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ 线性计算结果，还未进行激活函数的非线性变换。此时每一列是一个样本，图中可以看到共有8列，代表当前训练样本的batch中共有8个样本，每一行代表当前 $l$ 层神经元的一个节点，可以看到当前 $l$ 层共有4个神经元结点，即第 $l$ 层维度为4。 对于第一个神经元，我们求得 $\\mu_1 = 1.65, \\sigma^2_1=0.44$ （其中 $\\epsilon = 10^{-8}$ ），此时我们利用 $\\mu_1 ;\\sigma^2_1$ 对第一行数据（第一个维度）进行normalization得到新的值 $[-0.98, -0.23, -0.68, -1.13, 0.08, 2.19, 0.08]$ 。同理计算出其他输入维度归一化后的值。如下图： 通过上面的变换,用更加简化的方式来对数据进行规范化，使得第 $l$ 层的输入每个特征的分布均值为0，方差为1。 Normalization操作虽然缓解了ICS（Internal Covariate Shift）问题，让每一层网络的输入数据分布都变得稳定，但却导致了数据表达能力的缺失。也就是通过变换操作改变了原有数据的信息表达（representation ability of the network），使得底层网络学习到的参数信息丢失。另一方面，通过让每一层的输入分布均值为0，方差为1，会使得输入在经过sigmoid或tanh激活函数时，容易陷入非线性激活函数的线性区域。 为了解决这个问题，BN又引入了两个可学习（learnable）的参数 $\\gamma$ 与 $\\beta$ 。这两个参数的引入是为了恢复数据本身的表达能力，对规范化后的数据进行线性变换，即 $\\hat Z_j=\\gamma_j\\hat Z_j+\\beta_j$ 。特别地，当 $\\gamma^2=\\sigma^2,\\beta=\\mu$ 时，可以实现等价变换（identity transform）并且保留了原始输入特征的分布信息。 补充： 在进行normalization的过程中，由于我们的规范化操作会对减去均值，因此，偏置项 $b$ 可以被忽略掉或可以被置为0，即 $BN(Wu+b)=BN(Wu)$ 算法总结： ","date":"2020-08-13","objectID":"/batchnormalization/:4:0","tags":["Batch Normalization","算法"],"title":"经典算法：Batch Normalization","uri":"/batchnormalization/"},{"categories":["深度学习"],"content":"Batch Normalization的优势 解决了Internal Covariate Shift的问题：Internal Covariate Shift让学习率需要设很小，Batch Normalization以后学习率可以设大一点，所以training就快一点。 对防止梯度消失是有帮助的：用sigmoid函数，你很容易遇到gradient vanish的问题。如果有加Batch Normalization，就可以确保说激活函数的input都在零附近，都是斜率比较大的地方，就是gradient比较大的地方就不会有gradient vanish的问题，所以他特别对sigmoid，tanh这种特别有帮助。 对参数的定义的initialization影响是比较小的：很多方法对参数的initialization非常明显，但是当加了Batch Normalization以后，参数的initialization的影响比较小 假设把 $W^1$ 都乘 $k$ 倍， $z$ 当然也就乘上 $k$ ，Normalization的时候， $\\mu,\\sigma$ 也是乘上 $k$. 分子乘 $k$ 倍，分母 $k$ 乘，做完Normalization以后没有变化。所以如果在initialize的时候， $W$ 的参数乘上 $k$ 倍，对它的output的结果是没有影响。 能够缓解部分过拟合：在Batch Normalization的时候等同于是做了regularization，这个也是很直观，因为现在如果把所有的feature都固定到一样的mean，variance，如果在test的时候有一个异常数据进来，导致mean有一个变化，但做Normalization就会解决这个问题，所以batch Normalization有一些对抗Over Fitting的效果。所以如果training已经很好，而testing不好，可能也有很多其他的方法可以改进，不见得要Batch Normalization。 参考资源： 李宏毅深度学习 Batch Normalization原理与实战 ","date":"2020-08-13","objectID":"/batchnormalization/:5:0","tags":["Batch Normalization","算法"],"title":"经典算法：Batch Normalization","uri":"/batchnormalization/"},{"categories":["计算机视觉"],"content":"目标：将数张有重叠部分的图像通过特征点检测，匹配，图像变换拼成一幅无缝的全景图或高分辨率图像 在图像拼接中首先利用SIFT算法提取图像特征进而进行特征匹配，继而使用RANSAC算法对特征匹配的结果进行优化，接着利用图像变换结构进行图像映射，最终进行图像融合。 在图像拼接过程中，运用SIFT局部描述算子检测图像中的关键点和特征，SIFT特征是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。对于光线、噪声、些微视角改变的容忍度也相当高，所以用来检测要拼接图像的特征及关键点就很有优势。而接下来即步骤三是找到重叠的图片部分，连接所有图片之后就可以形成一个基本的全景图了。匹配图片最常用的方式是采用RANSAC（RANdom SAmple Consensus, 随机抽样一致），用此排除掉不符合大部分几何变换的匹配。之后利用这些匹配的点来估算单应矩阵”（Homography Estimation），也就是将其中一张图像通过关联性和另一张匹配。 使用的算法： RANSAC SIFT 1. 利用SIFT方法检测特征点 def detectAndDescribe(image): # 将彩色图片转换成灰度图 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # 建立SIFT生成器 descriptor = cv2.xfeatures2d.SIFT_create() # 检测SIFT特征点，并计算描述子 (kps, features) = descriptor.detectAndCompute(image, None) # 将结果转换成NumPy数组 kps = np.float32([kp.pt for kp in kps]) # 返回特征点集，及对应的描述特征 return (kps, features) 2. 将检测到的特征点进行匹配 def matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh): # 建立暴力匹配器 matcher = cv2.BFMatcher() # 使用KNN检测来自A、B图的SIFT特征匹配对，K=2 rawMatches = matcher.knnMatch(featuresA, featuresB, 2) matches = [] for m in rawMatches: # 当最近距离跟次近距离的比值小于ratio值时，保留此匹配对 if len(m) == 2 and m[0].distance \u003c m[1].distance * ratio: # 存储两个点在featuresA, featuresB中的索引值 matches.append((m[0].trainIdx, m[0].queryIdx)) # 当筛选后的匹配对大于4时，计算视角变换矩阵 if len(matches) \u003e 4: # 获取匹配对的点坐标 ptsA = np.float32([kpsA[i] for (_, i) in matches]) ptsB = np.float32([kpsB[i] for (i, _) in matches]) # 计算视角变换矩阵 (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh) # 返回结果 return (matches, H, status) # 如果匹配对小于4时，返回None return None 3. 将匹配的特征点可视化 def drawMatches(imageA, imageB, kpsA, kpsB, matches, status): # 初始化可视化图片，将A、B图左右连接到一起 (hA, wA) = imageA.shape[:2] (hB, wB) = imageB.shape[:2] vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\") vis[0:hA, 0:wA] = imageA vis[0:hB, wA:] = imageB # 联合遍历，画出匹配对 for ((trainIdx, queryIdx), s) in zip(matches, status): # 当点对匹配成功时，画到可视化图上 if s == 1: # 画出匹配对 ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1])) ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1])) cv2.line(vis, ptA, ptB, (0, 255, 0), 1) # 返回可视化结果 return vis 4. 图像拼接 def stitch(images, ratio=0.75, reprojThresh=4.0,showMatches=False): #获取输入图片 (imageB, imageA) = images #检测A、B图片的SIFT关键特征点，并计算特征描述子 (kpsA, featuresA) = detectAndDescribe(imageA) (kpsB, featuresB) = detectAndDescribe(imageB) # 匹配两张图片的所有特征点，返回匹配结果 M = matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh) # 如果返回结果为空，没有匹配成功的特征点，退出算法 if M is None: return None # 否则，提取匹配结果 # H是3x3视角变换矩阵 (matches, H, status) = M # 将图片A进行视角变换，result是变换后图片 result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0])) cv_show('result', result) # 将图片B传入result图片最左端 result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB cv_show('result', result) # 检测是否需要显示图片匹配 if showMatches: # 生成匹配图片 vis = drawMatches(imageA, imageB, kpsA, kpsB, matches, status) # 返回结果 return (result, vis) # 返回匹配结果 return result ","date":"2020-08-09","objectID":"/panorama/:0:0","tags":["实战","OpenCV","SIFT","RANSAC"],"title":"Python实现图像全景拼接","uri":"/panorama/"},{"categories":["计算机视觉"],"content":"图像分割是将图片将相似的部分分割成相同的块 ","date":"2020-07-31","objectID":"/segmentstionclustering/:0:0","tags":["RANSAC","拟合","霍夫变换"],"title":"图像分割(Segmentation)——K-Means, 最小割, 归一化图割","uri":"/segmentstionclustering/"},{"categories":["计算机视觉"],"content":"Gestalt理论 解释物体分割的底层原理 将同一个东西群组在一起,集合中的元素可以具有由关系产生的属性 Gestalt中常见的一些分组的情况 现实生活中的分组现象 将这种思想转化为算法 ","date":"2020-07-31","objectID":"/segmentstionclustering/:1:0","tags":["RANSAC","拟合","霍夫变换"],"title":"图像分割(Segmentation)——K-Means, 最小割, 归一化图割","uri":"/segmentstionclustering/"},{"categories":["计算机视觉"],"content":"K-Means聚类 主要思想：相似的像素应该属于同一类 像素表达：每个像素可以使用一个多维向量来表示，如(R, G, B)的三维向量，(R, G, B, x, y)的五维向量 K-Means算法： “物以类聚、人以群分”： 首先输入k的值，即我们希望将数据集经过聚类得到k个分组。 从数据集中随机选择k个数据点作为初始大哥（质心，Centroid） 对集合中每一个小弟，计算与每一个大哥的距离（距离的含义后面会讲），离哪个大哥距离近，就跟定哪个大哥。 这时每一个大哥手下都聚集了一票小弟，这时候召开人民代表大会，每一群选出新的大哥（其实是通\u003e过算法选出新的质心）。 如果新大哥和老大哥之间的距离小于某一个设置的阈值（表示重新计算的质心的位置变化不大，趋于\u003e稳定，或者说收敛），可以认为我们进行的聚类已经达到期望的结果，算法终止。 如果新大哥和老大哥距离变化很大，需要迭代3~5步骤。 基于灰度值或颜色的K-means聚类本质上是对图像属性的矢量量化 语义分割：将不同位置上同样的物品分成同一类（如上图三）使用(R, G, B)的三维向量描述像素 实列分割：将不同位置上同样的物品分成不同类，此时就需要考虑像素位置对分割的影响，使用(R, G, B, x, y)的五维向量描述像素 ","date":"2020-07-31","objectID":"/segmentstionclustering/:2:0","tags":["RANSAC","拟合","霍夫变换"],"title":"图像分割(Segmentation)——K-Means, 最小割, 归一化图割","uri":"/segmentstionclustering/"},{"categories":["计算机视觉"],"content":"Mean shift 聚类 K-Means算法可以很简单的进行分割，但是初始值对结果的影响非常大，Mean Shift 算法处理时将像素密度的峰值作为初始值，在特征空间中寻找密度的模态或局部最大值 所有轨迹通向相同模态的区域 Mean Shift： 定义特征值：(color, gradients, texture, etc) 在单个特征点上初始化窗口 对每个窗口执行Mean Shift直到收敛 合并接近相同“峰值”或模式的窗口 分割实例： 优势： 算法计算量不大，在目标区域已知的情况下完全可以做到实时跟踪； 采用核函数直方图模型，对边缘遮挡、目标旋转、变形和背景运动不敏感。 缺点: 缺乏必要的模板更新； 跟踪过程中由于窗口宽度大小保持不变，当目标尺度有所变化时，跟踪就会失败； 当目标速度较快时，跟踪效果不好； 直方图特征在目标颜色特征描述方面略显匮乏，缺少空间信息。 ","date":"2020-07-31","objectID":"/segmentstionclustering/:3:0","tags":["RANSAC","拟合","霍夫变换"],"title":"图像分割(Segmentation)——K-Means, 最小割, 归一化图割","uri":"/segmentstionclustering/"},{"categories":["计算机视觉"],"content":"Image as Graphs 节点代表像素 每对节点之间代表边缘(或每对“足够接近”的像素) 每条边都根据两个节点的亲和力或相似性进行加权 找一条边将，将像素之间的联系切断 将切断的边的权重相加，使和最小（相似的像素应该在相同的部分，不同的像素应该在不同的部分） 定义权值 $$ exp(-\\frac{1}{2\\sigma^2}dist(X_i,X_j)^2) $$ 假设用特征向量x表示每个像素，并定义一个适合于该特征表示的距离函数: $dist(X_i,X_j)^2$ 然后利用广义高斯核将两个特征向量之间的距离转化为相似性 小的$\\sigma$：仅群聚邻近点；大的$\\sigma$：群聚距离远的点 最小割：将权值小的边去掉 最小割中趋向于将点单独的割出来，就会导致分割出很多“小块” 归一化图割 $$ \\frac{W(A,B)}{W(A,V)}+\\frac{W(A,B)}{W(B,V)} $$ $W(A,B)$：表示集合$A, B$中所有边的权重的和 $W(A,V)，W(B,V)$：分别表示分割后集合$A, B$内边的个数，当总的边数一定，其中一个$W$很大时，另一个就会很小，导致整个算式所计算的权重就会很大 定义一个邻接关系矩阵$W$，$(i,j)$表示第i个像素与第j个像素之间的权重，是一个对称的矩阵 定义矩阵$D$，是一个对角的矩阵$D_{ii} = \\Sigma_jW(i, j)$ $$ \\frac{y^T(D-W)y}{y^TDy} $$ 归一化图割权重计算：其中，y是指示向量，如果第i个特征点属于A，那么它在第i个位置的值应该为1，反之则为0；利用拉格朗日算法求权重第二小（最小的是0）的特征值对应的特征向量y：$(D-W)y=\\lambda y$ 所求出的y不是[0,…,1,0,0…]而是存在小数的向量，需要设置一个门限，将大于门限的设置为1，小于门限的设置为0 使用特征向量y的项对图进行二分割 改变门限，迭代将图像分成多个区块，也可以使用K-Means将得到的特征向量分类，达到分割多块的效果 分割效果： 学习资源：北京邮电大学计算机视觉——鲁鹏 ","date":"2020-07-31","objectID":"/segmentstionclustering/:4:0","tags":["RANSAC","拟合","霍夫变换"],"title":"图像分割(Segmentation)——K-Means, 最小割, 归一化图割","uri":"/segmentstionclustering/"},{"categories":["计算机视觉"],"content":"任务说明：编写一个钱币定位系统，其不仅能够检测出输入图像中各个钱币的边缘，同时，还能给出各个钱币的圆心坐标与半径。 效果 ","date":"2020-07-29","objectID":"/cannyhough/:0:0","tags":["Canny","Hough","算法"],"title":"python实现Canny与Hough算法","uri":"/cannyhough/"},{"categories":["计算机视觉"],"content":"代码实现 Canny边缘检测： # Author: Ji Qiu （BUPT） # filename: my_canny.py import cv2 import numpy as np class Canny: def __init__(self, Guassian_kernal_size, img, HT_high_threshold, HT_low_threshold): ''' :param Guassian_kernal_size: 高斯滤波器尺寸 :param img: 输入的图片，在算法过程中改变 :param HT_high_threshold: 滞后阈值法中的高阈值 :param HT_low_threshold: 滞后阈值法中的低阈值 ''' self.Guassian_kernal_size = Guassian_kernal_size self.img = img self.y, self.x = img.shape[0:2] self.angle = np.zeros([self.y, self.x]) self.img_origin = None self.x_kernal = np.array([[-1, 1]]) self.y_kernal = np.array([[-1], [1]]) self.HT_high_threshold = HT_high_threshold self.HT_low_threshold = HT_low_threshold def Get_gradient_img(self): ''' 计算梯度图和梯度方向矩阵。 :return: 生成的梯度图 ''' print ('Get_gradient_img') new_img_x = np.zeros([self.y, self.x], dtype=np.float) new_img_y = np.zeros([self.y, self.x], dtype=np.float) for i in range(0, self.x): for j in range(0, self.y): if j == 0: new_img_y[j][i] = 1 else: new_img_y[j][i] = np.sum(np.array([[self.img[j - 1][i]], [self.img[j][i]]]) * self.y_kernal) if i == 0: new_img_x[j][i] = 1 else: new_img_x[j][i] = np.sum(np.array([self.img[j][i - 1], self.img[j][i]]) * self.x_kernal) gradient_img, self.angle = cv2.cartToPolar(new_img_x, new_img_y)#返回幅值和相位 self.angle = np.tan(self.angle) self.img = gradient_img.astype(np.uint8) return self.img def Non_maximum_suppression (self): ''' 对生成的梯度图进行非极大化抑制，将tan值的大小与正负结合，确定离散中梯度的方向。 :return: 生成的非极大化抑制结果图 ''' print ('Non_maximum_suppression') result = np.zeros([self.y, self.x]) for i in range(1, self.y - 1): for j in range(1, self.x - 1): if abs(self.img[i][j]) \u003c= 4: result[i][j] = 0 continue elif abs(self.angle[i][j]) \u003e 1: gradient2 = self.img[i - 1][j] gradient4 = self.img[i + 1][j] # g1 g2 # C # g4 g3 if self.angle[i][j] \u003e 0: gradient1 = self.img[i - 1][j - 1] gradient3 = self.img[i + 1][j + 1] # g2 g1 # C # g3 g4 else: gradient1 = self.img[i - 1][j + 1] gradient3 = self.img[i + 1][j - 1] else: gradient2 = self.img[i][j - 1] gradient4 = self.img[i][j + 1] # g1 # g2 C g4 # g3 if self.angle[i][j] \u003e 0: gradient1 = self.img[i - 1][j - 1] gradient3 = self.img[i + 1][j + 1] # g3 # g2 C g4 # g1 else: gradient3 = self.img[i - 1][j + 1] gradient1 = self.img[i + 1][j - 1] temp1 = abs(self.angle[i][j]) * gradient1 + (1 - abs(self.angle[i][j])) * gradient2 temp2 = abs(self.angle[i][j]) * gradient3 + (1 - abs(self.angle[i][j])) * gradient4 if self.img[i][j] \u003e= temp1 and self.img[i][j] \u003e= temp2: result[i][j] = self.img[i][j] else: result[i][j] = 0 self.img = result return self.img def Hysteresis_thresholding(self): ''' 对生成的非极大化抑制结果图进行滞后阈值法，用强边延伸弱边，这里的延伸方向为梯度的垂直方向， 将比低阈值大比高阈值小的点置为高阈值大小，方向在离散点上的确定与非极大化抑制相似。 :return: 滞后阈值法结果图 ''' print ('Hysteresis_thresholding') for i in range(1, self.y - 1): for j in range(1, self.x - 1): if self.img[i][j] \u003e= self.HT_high_threshold: if abs(self.angle[i][j]) \u003c 1: if self.img_origin[i - 1][j] \u003e self.HT_low_threshold: self.img[i - 1][j] = self.HT_high_threshold if self.img_origin[i + 1][j] \u003e self.HT_low_threshold: self.img[i + 1][j] = self.HT_high_threshold # g1 g2 # C # g4 g3 if self.angle[i][j] \u003c 0: if self.img_origin[i - 1][j - 1] \u003e self.HT_low_threshold: self.img[i - 1][j - 1] = self.HT_high_threshold if self.img_origin[i + 1][j + 1] \u003e self.HT_low_threshold: self.img[i + 1][j + 1] = self.HT_high_threshold # g2 g1 # C # g3 g4 else: if self.img_origin[i - 1][j + 1] \u003e self.HT_low_threshold: self.img[i - 1][j + 1] = self.HT_high_threshold if self.img_origin[i + 1][j - 1] \u003e self.HT_low_threshold: self.img[i + 1][j - 1] = self.HT_high_threshold else: if self.img_origin[i][j - 1] \u003e self.HT_low_threshold: self.img[i][j - 1] = self.HT_high_threshold if self.img_origin[i][j + 1] \u003e self.HT_low_threshold: self.img[i][j + 1] = self.HT_high_threshold # g1 # g2 C g4 # g3 if self.angle[i][j] \u003c 0: if self.img_origin[i - 1][j - 1] \u003e self.HT_low_threshold: self.img[i - 1][j - 1] = self.HT_high_threshold if self.img_origin[i + 1][j + 1] \u003e self.HT_low_threshold: self.img[i + 1][j + 1] = self.HT_high_threshold # g3 # g2 C ","date":"2020-07-29","objectID":"/cannyhough/:1:0","tags":["Canny","Hough","算法"],"title":"python实现Canny与Hough算法","uri":"/cannyhough/"},{"categories":["计算机视觉"],"content":"纹理是由一些基元以某种方式组合起来，虽然看起来很“乱”，但任然存在一些规律 规则的纹理与不规则的纹理 ","date":"2020-07-26","objectID":"/texture/:0:0","tags":["Texture","K均值"],"title":"纹理表示(Texture)","uri":"/texture/"},{"categories":["计算机视觉"],"content":"纹理描述 使用高斯偏导核，对图像进行卷积，x方向的偏导得到的是竖直纹理，y方向的偏导得到的是水平纹理 统计各个方向的纹理数量，在图中表示出来，不同的区域映射的是不同的纹理特性 如下图所示进行K均值聚类 距离显示了窗口a的纹理和窗口b的纹理有多么不同。 对于区块核大小的选择 在图像中往往不知道选取多大的高斯偏导核来对图像进行描述 通过寻找纹理描述不变的窗口比例来进行比例选择，由小到大不断改变窗口的大小，直至增大的窗口纹理特性不再改变 ","date":"2020-07-26","objectID":"/texture/:1:0","tags":["Texture","K均值"],"title":"纹理表示(Texture)","uri":"/texture/"},{"categories":["计算机视觉"],"content":"滤波器组 可以描述不同方向，不同类型（边状，条状，点）的纹理特性 通过设置斜方差矩阵$\\Sigma$，改变高斯核的形状 利用不同的核卷积图像 将响应结果与纹理匹配 将对应卷积核的响应结果求均值，所得的结果组成一个7维向量，每个向量对应一个纹理 使用更高维的向量描述 纹理检索与分类 实际运用过程中，将采取的纹理与数据库中的纹理进行对比 学习资源：北京邮电大学计算机视觉——鲁鹏 ","date":"2020-07-26","objectID":"/texture/:2:0","tags":["Texture","K均值"],"title":"纹理表示(Texture)","uri":"/texture/"},{"categories":["计算机视觉"],"content":"针对Harris无法拟合尺度问题而提出 目标:独立检测同一图像缩放版本的对应区域 需要通过尺度选择机制来寻找与图像变换协变的特征区域大小 “当尺度改变时控制每个圆内的内容不变” ","date":"2020-07-22","objectID":"/edgedetection2/:0:0","tags":["Laplacian","Blob","SIFT","区域检测"],"title":"区域检测——Blob \u0026 SIFT","uri":"/edgedetection2/"},{"categories":["计算机视觉"],"content":"Laplacian核 具体的算法是在边缘检测中使用的高斯一阶偏导核转换为高斯二阶偏导核 使用Laplacian核与图像进行卷积操作 **边缘：**出现波纹的地方 **尺度信息：**当波纹重叠并出现极值的地方 空间选择:如果Laplacian的尺度与blob的尺度“匹配”，则Laplacian响应的幅度将在blob的中心达到最大值 在实际运用的过程中是使用模板匹配信号，即不断改变Laplacian的参数$\\sigma$取处理后的结果达到峰值时的$\\sigma$，随着参数的增大会导致后面的特征消失（高斯偏导的面积公式中的$\\sigma$在分母） 为了保持响应不变(尺度不变)，必须将高斯导数乘以$\\sigma$ 拉普拉斯导数是二阶高斯导数，所以它必须乘以$\\sigma^2$ ","date":"2020-07-22","objectID":"/edgedetection2/:1:0","tags":["Laplacian","Blob","SIFT","区域检测"],"title":"区域检测——Blob \u0026 SIFT","uri":"/edgedetection2/"},{"categories":["计算机视觉"],"content":"二维空间的Blob的检测 高斯的拉普拉斯算子:用于二维检测的圆对称算子 $$\\nabla^2 g=\\frac{\\partial^2 g}{\\partial x^2}+\\frac{\\partial^2 g}{\\partial y^2}\\Longrightarrow \\nabla_{norm}^2 g=\\sigma^2(\\frac{\\partial^2 g}{\\partial x^2}+\\frac{\\partial^2 g}{\\partial y^2})$$ Laplcain算子中的$\\sigma$与检测对象画出的圆的半径$r$的关系 为了得到最大响应，Laplacian的零点必须与圆对齐 令:$$\\nabla_{norm}^2 g=0即：\\sigma^2(\\frac{\\partial^2 g}{\\partial x^2}+\\frac{\\partial^2 g}{\\partial y^2})=0$$ 化简后： $$ (x^2+y^2-2\\sigma^2)e^{-\\frac{x^2+y^2}{2\\sigma^2}}=0 $$ $$ \\Downarrow $$ $$ x^2+y^2-2\\sigma^2=0 $$ 得到：$r=\\sqrt{2}\\sigma$ 特征尺度 将图像的特征尺度r定义为blob中心产生拉普拉斯响应峰值的尺度 示例： 尺度选择过程中将逐步增加参数$\\sigma$，每个$\\sigma$逐像素计算最大响应，每相邻取九个像素取响应值最大的像素，再与上下两层不同尺度的最大相应取最大（即在一个3x3x3共27个的响应值中取最大的响应值对应的像素点和尺度值） ","date":"2020-07-22","objectID":"/edgedetection2/:2:0","tags":["Laplacian","Blob","SIFT","区域检测"],"title":"区域检测——Blob \u0026 SIFT","uri":"/edgedetection2/"},{"categories":["计算机视觉"],"content":"SIFT特征 在实际运用过程中，使用Laplacian核可以很好的处理尺度变换的问题，但是需要大量的计算，使用SIFT方法可以简化计算 DoG模板 DoG的函数图像与Laplacian核很相似，具有相似的性质，但使用的时两个高斯差分来定义，大的高斯核可以使用小的高斯核来计算，大大减少了计算量 $$G(x,y,k\\sigma)-G(x,y,\\sigma)\\approx(k-1)\\sigma^2\\nabla^2G$$ 高斯空间中的模板利用DoG算法直接从前一层的基础上计算，这样就形成一个DoG空间，得到的模板与与高斯空间相差一个常数项$(k-1)$ 计算大尺度的模板时不改变参数值，改变图像大小，例如：将图像缩小一倍，不改变模板尺度得到效果和增大模板尺度不改变图像大小的效果相同，计算四倍尺度的值就将图像缩小四倍，$\\sqrt{2}\\sigma$的尺度在缩小一倍的图像上的对应尺度为$2\\sqrt{2}\\sigma$ $k=2^{1/s}$：$s$表示要输出的尺度有多少个，利用$s$来计算$k$,例如下图是输出尺度为$s=2$时的示例，此时$k=\\sqrt{2}$,二倍尺度状态下的起始模板可以由一倍尺度的$k^2\\sigma=2\\sigma$下采样得到 模板尺度通常取$2$的等比数列$(1,2,4,8,16……)$ SIFT仿射变换 当视角改变时，即使是同一个圆，其中的内容也有很大的差异 使用$M$矩阵将圆具有自适应性，使结果更具鲁棒特性 先确定一个圆 将圆内的所有像素拿出来计算$M$矩阵 比较计算出来的$\\lambda_1,\\lambda_2$ 将较小的$\\lambda$的方向进行缩小 再将上一步缩小后的区域（椭圆）内的像素拿出来计算$M$矩阵 重复上述步骤，逐步迭代。直至$\\lambda_1,\\lambda_2$近似相等，说明区域边缘的梯度变化近似一致 将椭圆转换到一样大小的圆中 梯度方向法 通过仿射自适应变换后，内容基本一致，但方向不同，对应的像素差异较大，无法识别。 计算圆内每个像素的梯度强度和方向 将梯度方向量化成八份，给对应的直方图投票，票数就是梯度的大小 统计完之后选择票数最高的方向作为，作为圆内像素整体的梯度方向，将方向转换到$0^\\circ$，将整个圆进行相同的旋转 决绝明暗不一致：将圆均分成16格，每个格代表一个区域，统计每个区域的方向量化梯度（两化成八个角度，长度代表梯度大小），每个区域中由一个“8位”向量表示，将16个区域的向量拉直就得到一个 $8\\times16=128$ 的向量来描述这个圆内的内容，最后比较每个圆的128个数来判断两个圆内容的相似程度 总结：SIFT算法 可以解决方向，视角，明暗，位置等常见图像变化的问题 学习资源：北京邮电大学计算机视觉——鲁鹏 ","date":"2020-07-22","objectID":"/edgedetection2/:3:0","tags":["Laplacian","Blob","SIFT","区域检测"],"title":"区域检测——Blob \u0026 SIFT","uri":"/edgedetection2/"},{"categories":["计算机视觉"],"content":" 对于图像处理时经常需要提取特征点分析图片结构，将照片进行拼接，实现全景拍摄，那么在照片特征点提取时所采用的具体算法是什么呢？ ","date":"2020-07-17","objectID":"/localfeature/:0:0","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"解决思路 提取特征点 匹配特征点 使用RANSAC方法将两张图片的对应的特征点转换的方式拟合出来，在对图片采用相同的转换方式进行转换，在进行拼接 ","date":"2020-07-17","objectID":"/localfeature/:1:0","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"特征点——Corner ","date":"2020-07-17","objectID":"/localfeature/:2:0","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"好的特征点是什么样的？ 可重复性：在一张图可以被观测到的，在其他同场景的图也可以被观测到 显著性：检测的特征点需要是在某一类图像中“独有的”，尽量剔除“普遍性”的点，目的是为了将不同类的图区分开 简洁和高效：尽可能的减少计算量，提高计算效率 局部性：匹配特征时要匹配特征点之间的相对关系，通过局部特征相对位置来判断是否为同一张图，来拟合转动镜头角度，图像位置 ","date":"2020-07-17","objectID":"/localfeature/:2:1","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"什么样的点满足条件 通过观察图片的特征，发现存在“角”的地方承载着更多的信息，角点是梯度在两个或以上方向上有变化的点。 ","date":"2020-07-17","objectID":"/localfeature/:2:2","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"Basic Idea 使用一个较小的窗口在图像上延各个方向滑动 不同的变化趋势显示了不同的特征 图像内部所在的窗口延各个方向都没有变化；边缘所在的窗口延边缘方向无变化；角点所在窗口会在各个方向上都有显著的变化 ","date":"2020-07-17","objectID":"/localfeature/:2:3","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"数学描述 $u$和$v$是平移量 求平移后的窗口与平移前的窗口的对应位置差的平方，再累加 乘上窗口权重，考虑每个点对窗口影响的不同程度，例如第二种的高斯函数权重，就是考虑中间的点的差值在整个窗口的影响度更大 二阶泰勒展开：为了能够直接观察到$E(u,v)$与$[u,v]$之间的联系 取$E(u,v)$在$(0,0)$的二阶展开作为近似解 计算化简泰勒展开式 其中$I_x,I_y$分别表示点$(x,y)$在$x$方向,$y$方向的偏导，$M$是由一个二阶矩矩阵加权求和得到 ","date":"2020-07-17","objectID":"/localfeature/:3:0","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"矩阵$M$ 类比方程$y=ax+b$决定方程特性的是$a,b$。则决定$E(u,v)$特性的是$M$，分析矩阵$M$就可以得到$E(u,v)$的特性 函数图像延竖直方向截取为一个椭圆，当梯度为零时，截面为圆，此时窗口位于图像内部；当延某一方向梯度为零时，界面为一个“正椭圆”，此时窗口位于边；当窗口位于角时，界面椭圆的形状反映了当前窗口下角的特性 正交矩阵$R$：使所截取的椭圆旋转$R$角度，变为一个“正椭圆” 当$I_x,I_y$任意一个趋于0，用$\\lambda$表示，任意一个$\\lambda$趋于0，都表示这个点不是角点 椭圆的半轴长度反应的是梯度变化的快慢，越长则梯度变化越快（将$E(u,v)$展开就可以得到椭圆的半轴表示为$\\lambda^{-\\frac12}$） 可视化 特征值简化——角点响应函数R 将$\\lambda_1,\\lambda_2$特征转化给$R$，最后判断只需判断$R$就可以确定是否为角点 ","date":"2020-07-17","objectID":"/localfeature/:3:1","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"Harris检测器 计算每个像素处的高斯导数 计算每个像素周围的高斯窗口中的二阶矩矩阵M 计算角点响应函数R 设置门限R 寻找响应函数的局部最大值(非最大抑制) Harris的特性 当光线强度，明暗改变时，只是改变了部分角点的值，还有大部分的点可以用于检测，可以进行检测， 当改变位置，角度时，没有改变相对位置，可以检测 当改变窗口大小时，大窗口下是角点，而小窗口下是线或者边缘，无法检测 学习资源：北京邮电大学计算机视觉——鲁鹏 ","date":"2020-07-17","objectID":"/localfeature/:4:0","tags":["Harris","特征提取","区域检测"],"title":"区域检测——Harris角点","uri":"/localfeature/"},{"categories":["计算机视觉"],"content":"提取完边缘后如何使用数学模型来描述边缘？ 例如：在桌子上有几枚硬币，在经过边缘提取后，需要描述出硬币的圆心坐标和圆的大小 ","date":"2020-07-15","objectID":"/fitting/:0:0","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"难点 噪声：噪声的存在使拟合的模型偏离真实的线 外点：在目标图形以外的线，如上图中的目标图形为“车”，左边的“栅栏”就是外点 目标图形部分被遮挡，使部分图形消失 ","date":"2020-07-15","objectID":"/fitting/:1:0","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"最小二乘（Least Square） 针对点都在线上的一些简单模型 最小二乘 能量函数$E$描述的是所有的点与拟合的线在$y$方向上的差值的和，最后的目标是求出差值最小时的$(m,b)$即矩阵$B$作为这个模型的解 权最小二乘 当拟合的直线是平行$y$轴时就无法按照上面的公式计算$E$（最小二乘对旋转没有效果） 权最小二乘将点在$y$方向的距离改为对直线距离的平方，就可以避免旋转产生的问题 ，它的几何描述就是所有的向量$(x_i-\\bar x,y_i-\\bar y)$在向量$(a,b)$的投影的值最小 极大似然估计 使用概率分布的思想来理解权最小二乘，概率越大拟合效果越好，极大似然估计，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，它提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 如果通过极大似然估计，得到模型中参数$\\mu$和$\\sigma$的值，那么这个模型的均值和方差以及其它所有的信息我们就知道了。 ","date":"2020-07-15","objectID":"/fitting/:2:0","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"Roubst Fitting \u0026 RANSAC 当存在外点时，普通的（权）最小二乘就无法很好的拟合模型 ","date":"2020-07-15","objectID":"/fitting/:3:0","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"Roubst fitting（鲁棒拟合） 通过函数$\\rho(u;\\sigma)$将点到直线的距离$u$在较大的范围时对直线影响的贡献值缩小，$\\sigma$为设置的参数，当参数越小对所取的预拟合模型的区域越小，例如当$\\sigma=0.1$时，对超过2以后的区域几乎就不做考虑了 鲁棒函数估计 对于一个非线性的优化问题就不能使用前面的方程求解，需要使用迭代的方式求解类似于梯度下降 先不考虑鲁棒拟合的问题，利用最小二乘得到一个初始解 根据经验将尺度参数$\\sigma$设置成$1.5$倍的平均残差 处理后的效果 ","date":"2020-07-15","objectID":"/fitting/:3:1","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"RANSAC（随机采样一致性） 当存在许多的点都不在模型上，或者是图片被遮挡，这种时候就需要使用较少的点来拟合出模型 RANSAC 选择一个最小的集合$s$（估计一条直线需要两个点） 拟合出一个模型 设置一个门限$t$ 用门限$t$内剩余的点给这个模型“投票”，即“离得近”就 得分 重复上述过程，取“得分”最高的模型,设置迭代次数$N$ 选择参数 $e$：外点率，$s$：模型的最小范围，$N$：最大迭代次数，$p$：正确率 自适应的参数提取 在实际问题中，只知道参数$t,s$,无法知道外点率$e$也就无法确认迭代次数$N$。 解决方法如下： 使用RANSAC思想进行指纹识别 ","date":"2020-07-15","objectID":"/fitting/:3:2","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"霍夫变换（Hough Transfrom） 于对存在大量的线的模型，即使设置了较小的门限也无法有效的区分“谁是谁的内点” 主要的改进策略：将点不在是对某一条直线投票，将投票离散化，使图像空间对参数空间进行转换 图像空间的一条直线在参数空间是一个点，参数空间多条线的交点是图像空间的边 如果使用直角坐标那么参数范围不好界定，穷举就很困难，例如当图像空间的一条竖直方向的直线，此时$x$取定值，$y$取任意值，在参数空间中就无法表示；在训练过程中也无法给$m,b$划分范围。 使用极坐标系问题就解决了：极坐标的$\\theta$可以取$[0 ,180]$，可以完整的与图像空间对应 一些例子 霍夫变换处理噪声 在Candy算子中得到点时就知道了梯度方向，相应的边缘方向的范围就大概确认了，这是就是可以缩小$\\theta$的范围，从而解决了噪声的影响，也简化了计算 霍夫变换拟合圆 确定一个圆需要圆心坐标$(x, y)$和半径$r$，有三个参数，参数空间就需要是一个三维空间，取圆上的一个点，则可以由梯度方向确定半径方向，穷举所用的$r$(大于0，小于图像长度)，遍历圆上的点对$r$进行投票，最后在参数空间会得到一个票数高的三维空间，这个三维空间的中的一点$(x,y,r)$就可以作为拟合的圆心和半径。 学习资源：北京邮电大学计算机视觉——鲁鹏 ","date":"2020-07-15","objectID":"/fitting/:4:0","tags":["RANSAC","拟合","霍夫变换"],"title":"拟合(Fitting)","uri":"/fitting/"},{"categories":["计算机视觉"],"content":"边缘提取 在大多数时候图像的边缘可以承载大部分的信息，并且提取边缘可以除去很多干扰信息，提高处理数据的效率 ","date":"2020-07-10","objectID":"/edgedetection/:1:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":"目标 识别图像中的突然变化(不连续) 图像的大部分语义信息和形状信息都可以编码在边缘上 理想:艺术家使用线条勾勒画(但艺术家也使用对象层次的知识) ","date":"2020-07-10","objectID":"/edgedetection/:2:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":"边缘的种类 表面形状的突变 深度方向的不连续 表面颜色的突变 光线阴影的不连续 ","date":"2020-07-10","objectID":"/edgedetection/:3:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":"边缘的特征 边缘是图像强度函数中快速变化的地方，变化的地方就存在梯度，对灰度值求导，导数为0的点即为边界点 卷积的导数 偏导数公式： $$\\frac {\\partial f(x,y)}{\\partial x} = \\lim_{\\varepsilon \\rightarrow 0} \\frac{f(x+\\varepsilon ,y)-f(x,y)}{\\varepsilon}$$ 在卷积中为描述数据，采取 近似化处理： $$\\frac {\\partial f(x,y)}{\\partial x} \\approx \\frac{f(x+1,y)-f(x,y)}{1}$$ 显然在x方向的导数就是与该像素自身与右边相邻像素的差值 卷积描述偏导 使用卷积核处理 对灰度图的x和y方向分别处理后的效果如下图： 有限差分滤波器（卷积核） Roberts 算子 Roberts 算子是一种最简单的算子，是一种利用局部差分算子寻找边缘的算子。他采用对角线方向相邻两象素之差近似梯度幅值检测边缘。检测垂直边缘的效果好于斜向边缘，定位精度高，对噪声敏感，无法抑制噪声的影响。 1963年， Roberts 提出了这种寻找边缘的算子。 Roberts 边缘算子是一个 2x2 的模版，采用的是对角方向相邻的两个像素之差。 Roberts 算子的模板分为水平方向和垂直方向，如下所示，从其模板可以看出， Roberts 算子能较好的增强正负 45 度的图像边缘。 $$ dx = \\left[ \\begin{matrix} -1 \u0026 0\\\\ 0 \u0026 1 \\\\ \\end{matrix} \\right] $$ $$ dy = \\left[ \\begin{matrix} 0 \u0026 -1\\\\ 1 \u0026 0 \\\\ \\end{matrix} \\right] $$ Prewitt算子 Prewitt 算子是一种一阶微分算子的边缘检测，利用像素点上下、左右邻点的灰度差，在边缘处达到极值检测边缘，去掉部分伪边缘，对噪声具有平滑作用。Prewitt算子适合用来识别噪声较多、灰度渐变的图像。 $$ dx = \\left[ \\begin{matrix} 1 \u0026 0 \u0026 -1\\\\ 1 \u0026 0 \u0026 -1\\\\ 1 \u0026 0 \u0026 -1\\\\ \\end{matrix} \\right] $$ $$ dy = \\left[ \\begin{matrix} -1 \u0026 -1 \u0026 -1\\\\ 0 \u0026 0 \u0026 0\\\\ 1 \u0026 1 \u0026 1\\\\ \\end{matrix} \\right] $$ Sobel算子 Sobel算子是一种用于边缘检测的离散微分算子，它结合了高斯平滑和微分求导。Sobel 算子在 Prewitt 算子的基础上增加了权重的概念，认为相邻点的距离远近对当前像素点的影响是不同的，距离越近的像素点对应当前像素的影响越大，从而实现图像锐化并突出边缘轮廓。 $$ dx = \\left[ \\begin{matrix} 1 \u0026 0 \u0026 -1\\\\ 2 \u0026 0 \u0026 -2\\\\ 1 \u0026 0 \u0026 -1\\\\ \\end{matrix} \\right] $$ $$ dy = \\left[ \\begin{matrix} -1 \u0026 -2 \u0026 -1\\\\ 0 \u0026 0 \u0026 0\\\\ 1 \u0026 2 \u0026 1\\\\ \\end{matrix} \\right] $$ ","date":"2020-07-10","objectID":"/edgedetection/:4:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":"图像梯度 $$\\nabla f=[\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}]$$ 梯度指向强度增长最快的方向 梯度的角度 边的方向与梯度方向垂直 $$\\theta = tan^{-1} (\\frac{\\partial f}{\\partial y}/\\frac{\\partial f}{\\partial x})$$ 梯度的模长（幅值） 可以说明是边缘的可能性大小 $$||\\nabla f|| = \\sqrt{(\\frac{\\partial f}{\\partial x})^2+(\\frac{\\partial f}{\\partial y})^2}$$ 处理图像后： ","date":"2020-07-10","objectID":"/edgedetection/:5:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":"高斯滤波器 当图像的像素存在大量噪点时，相邻的像素差异大，所求梯度也会偏大，无法提取边缘信息。 解决方案 平滑处理：使用平滑滤波器去噪，使图像信号变得平滑 再对处理后的信号求导，取极值 根据卷积的计算性质：$\\frac{d}{dx}(f*g) = f * \\frac{d}{dx}g$，先对平滑核求导，再进行卷积相乘来简化运算，减少运算量 高斯滤波器 高斯滤波器的导数 参数选择的越小则保留的细节越多 ","date":"2020-07-10","objectID":"/edgedetection/:6:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":"Canny 边缘检测 门限化 经过处理后，可以得到边缘图，但存在很多高频噪点，通过设置更高的门限，过滤噪点，使得到的边缘更“纯粹” 非最大化抑制 在通过高斯滤波后可以得到图像的大致轮廓线，由于图像的像素变换通常是缓慢改变的， 在处理后的图像中仍然存在大量的粗的“边” 方案 检查像素是否沿梯度方向为局部最大值，选择沿边缘宽度的最大值作为边缘 处理后 经过上面的处理后，已经可以较为粗糙的得到图像的边缘图，但仍然存在问题，在有些部分的边 缘不连续，失去了很多信息如上图的 黄色区域 ，这是由于在门限化的过程中，设置过小，导致将需要的边缘滤除。 双门限法 先使用高门限将较粗的边检测出来，这些边都是比较鲁棒的，是噪声的可能性极低 再降低门限，将较细的边显现出来 将与高门限过滤出的边连接的低门限边保留，滤除没有连接的（不连续的）噪声 处理后可以得到更好的边缘效果 学习资源：北京邮电大学计算机视觉——鲁鹏 ","date":"2020-07-10","objectID":"/edgedetection/:7:0","tags":["高斯滤波","Canny edge detector"],"title":"边缘检测(Edge Detection)","uri":"/edgedetection/"},{"categories":["计算机视觉"],"content":" PAPER: Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks ","date":"2020-07-03","objectID":"/hanet/:0:0","tags":["神经网络","paper阅读","HANet"],"title":"高驱动注意网络(HANET)","uri":"/hanet/"},{"categories":["计算机视觉"],"content":"CityScape数据集 介绍 ​Cityscapes是关于城市街道场景的语义理解图片数据集。它主要包含来自50个不同城市的街道场景，拥有5000张在城市环境中驾驶场景的高质量像素级注释图像（其中 2975 for train，500 for val,1525 for test， 共有19个类别）；此外，它还有20000张粗糙标注的图像(gt coarse)。 数据集结构 cityscapes └ leftImg8bit_trainvaltest └ leftImg8bit └ train └ val └ test └ gtFine_trainvaltest └ gtFine └ train └ val └ test ","date":"2020-07-03","objectID":"/hanet/:1:0","tags":["神经网络","paper阅读","HANet"],"title":"高驱动注意网络(HANET)","uri":"/hanet/"},{"categories":["计算机视觉"],"content":"HANet介绍 高驱动注意网络（height-driven attention networks）是根据城市数据集的内在特征而提出的通用网络附加模型，提高了城市环境的语义分割的accuracy，容易嵌入各个网络，且对于mIoU有着较为明显的提高 通过观察可以发现城市CityScape数据的数据在高度方向包含的信息密度要远少于水平方向的信息密度，并且有着较为明显的结构性，此方法将数据沿着高度方向分为上、中、下三部分。 Cityscapes dataset中一个像素超过19个类的概率分布X的熵计算为: $$H(x) = H(p_{road}, p_{building},…,p_{motorcy})=-\\sum _i p_i logp_i$$ ","date":"2020-07-03","objectID":"/hanet/:2:0","tags":["神经网络","paper阅读","HANet"],"title":"高驱动注意网络(HANET)","uri":"/hanet/"},{"categories":["计算机视觉"],"content":"网络结构 $X_l，X_h$: 语义分割特征图中的底层和高层特征图 （a）宽度池化： 获得一个channel-wise attention map 为矩阵$Z$, $Z = G_{pool}(X_l)$, 池化方式为平均池化，$Z$中的第h行计算方式为$$Z_{:,h}=[\\frac1W\\sum_{i=1}^WX_{1,h,i};…;\\frac1W\\sum_{i=1}^WX_{C,h,i}]$$ （b）下采样（插入Coarse Attention）： 将（a）中得到的$Z(C_l \\times H_l \\times 1)$进行下采样得到$\\hat{Z}(C_l \\times \\hat{H} \\times 1)$其中超参数$\\hat{H}$设置为16 （c）计算： 由于每一行都与其相邻的行相关，在估计注意力图时，采用卷积层来考虑相邻行之间的关系。 注意图A表示在每一行中哪些通道是关键的。中间层的每一行可能存在多个信息特征;在最后一层中，每一行都可以与多个标签(如道路、汽车、人行道等)相关联。为了实现这些多重特征和标签，在计算注意力地图时使用了一个sigmoid函数，而不是softmax函数。对于N个卷积层，这些操作可以写成 采用三层卷积操作： 降低通道数：$G_{conv}^1(\\hat{Z})= Q^1 \\in \\R^{\\frac{C_l}r \\times \\hat{H}}$ $G_{conv}^2(\\delta(Q^1 ))=Q^2 \\in \\R^{2\\cdot \\frac{C_l}r \\times \\hat{H}}$ 生成注意力图：$G_{conv}^3(\\delta(Q^2 ))=\\hat{A} \\in \\R^{C_h \\times \\hat{H}}$ 其中$r$为压缩比，降低了参数量的同时还产生了一种正则化的效果 (d)上采样： 保持与$X_h$高度一致 (e)结合位置编码： 人在驾驶时有一些先验知识，比如知道路在下面，天空在上面，因此在中间层特征图$Q^i$的第$i$层的添加正弦位置编码 位置编码的维数与中间特征图$Q^i$的通道数$C$相同。位置编码定义为 $$PE_{(p,2i)} = sin(p/100^{2i/C})$$ $$PE_{(p,2i+1)} = cos(p/100^{2i/C})$$ p为注意力从0到$\\hat{H}-1$的整幅图像中的垂直位置指数，i为注意力的维数 垂直位置的数量设置为$\\hat{H}$，作为corse attention中的行数 更新位置编码：$\\hat{Q} = Q\\oplus PE$ 超参数的选取： ","date":"2020-07-03","objectID":"/hanet/:3:0","tags":["神经网络","paper阅读","HANet"],"title":"高驱动注意网络(HANET)","uri":"/hanet/"},{"categories":["计算机视觉"],"content":"将HANet插入ResNet101 结构 配置 PyTorch v1.4 在第一层网络中，使用3个3 x 3的卷积代替一个7 x 7的卷积 在中间特征图和类均匀采样中还采用了一种辅助的交叉损失熵 采用SGD优化器，初始学习率为0.01，学习率调度遵循多项式学习率策略，动量为0.9 权重衰减分别为5e-4和1e-4用于主网络和HANet 为了避免过拟合，使用了语义图像分割模型中典型的数据增强方法，包括随机水平翻转、在[0.5,2]范围内随机缩放、高斯模糊、颜色抖动和随机裁剪。 ","date":"2020-07-03","objectID":"/hanet/:4:0","tags":["神经网络","paper阅读","HANet"],"title":"高驱动注意网络(HANET)","uri":"/hanet/"},{"categories":["计算机视觉"],"content":"GitHub项目地址 https://github.com/shachoi/HANet ","date":"2020-07-03","objectID":"/hanet/:5:0","tags":["神经网络","paper阅读","HANet"],"title":"高驱动注意网络(HANET)","uri":"/hanet/"},{"categories":["计算机视觉"],"content":"半监督学习 在有标签数据+无标签数据混合成的训练数据中使用的机器学习算法。一般假设，无标签数据比有标签数据多，甚至多得多。 要求： 无标签数据一般是有标签数据中的某一个类别的（不要不属于的，也不要属于多个类别的）； 有标签数据的标签应该都是对的； 无标签数据一般是类别平衡的（即每一类的样本数差不多）； 无标签数据的分布应该和有标签的相同或类似 。 半监督学习算法 简单自训练（simple self-training）：用有标签数据训练一个分类器，然后用这个分类器对无标签数据进行分类，这样就会产生伪标签（pseudo label）或软标签（soft label），挑选认为分类正确的无标签样本（此处应该有一个挑选准则），把选出来的无标签样本用来训练分类器 协同训练（co-training）：是 self-training 的一种，但其思想是好的。假设每个数据可以从不同的角度（view）进行分类，不同角度可以训练出不同的分类器，然后用这些从不同角度训练出来的分类器对无标签样本进行分类，再选出认为可信的无标签样本加入训练集中。由于这些分类器从不同角度训练出来的，可以形成一种互补，而提高分类精度；就如同从不同角度可以更好地理解事物一样。 半监督字典学习：是 self-training 的一种，先是用有标签数据作为字典，对无标签数据进行分类，挑选出认为分类正确的无标签样本，加入字典中（此时的字典就变成了半监督字典了） 标签传播算法（Label Propagation Algorithm）：是一种基于图的半监督算法，通过构造图结构（数据点为顶点，点之间的相似性为边）来寻找训练数据中有标签数据和无标签数据的关系。只是训练数据中，这是一种直推式的半监督算法，即只对训练集中的无标签数据进行分类，这其实感觉很像一个有监督分类算法…，但其实并不是，因为其标签传播的过程，会流经无标签数据，即有些无标签数据的标签的信息，是从另一些无标签数据中流过来的，这就用到了无标签数据之间的联系 半监督支持向量机：监督支持向量机是利用了结构风险最小化来分类的，半监督支持向量机还用上了无标签数据的空间分布信息，即决策超平面应该与无标签数据的分布一致（应该经过无标签数据密度低的地方）（这其实是一种假设，不满足的话这种无标签数据的空间分布信息会误导决策超平面，导致性能比只用有标签数据时还差） 半监督学习分类 纯（pure）半监督学习：假定训练数据中的未标记样本并非待测的数据 直推学习：假定学习过程中所考虑的未标记样本恰是待预测数据，学习的目的就是在这些未标记样本上获得最优泛化性能 ","date":"2020-07-02","objectID":"/semisupervisedlearning/:1:0","tags":["半监督","深度学习"],"title":"半监督深度学习","uri":"/semisupervisedlearning/"},{"categories":["计算机视觉"],"content":"半监督深度学习 半监督深度学习算法： 无标签数据预训练网络后有标签数据微调（fine-tune）； 有标签数据训练网络，利用从网络中得到的深度特征来做半监督算法； 让网络 work in semi-supervised fashion。 对于神经网络来说，一个好的初始化可以使得结果更稳定，迭代次数更少。目前我见过的初始化方式有两种： 无监督预训练：用所有训练数据训练自动编码器（AutoEncoder），然后把自编码网络的参数作为初始参数，用有标签数据微调网络（验证集）。 伪有监督预训练：通过半监督算法或聚类算法等方式，给无标签数据附上伪标签信息，先用这些伪标签信息来预训练网络，然后再用有标签数据来微调网络（验证集）。 有标签数据提取特征的半监督学习 先用有标签数据训练网络（此时网络一般过拟合…） 通过隐藏层提取特征，以这些特征来用某种分类算法对无标签数据进行分类 挑选认为分类正确的无标签数据加入到训练集 重复上述过程 网络本身的半监督学习（端到端的半监督深度模型） ICML 2013 的文章Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks：该文章简单的说就是在伪标签学习中使用深度学习网络作为分类器，就是把网络对无标签数据的预测，作为无签数据的伪标签（Pseudo label），用来对网络进行训练。方法虽然简单，但是效果很好，比单纯用有标签数据有不少的提升 其主要的贡献在于损失函数的构造： $$L=\\sum_{m=1}^n\\sum_{i=1}^C L(y_i^m, f_i^m)+\\alpha(t)\\sum_{m=1}^{n^\\prime}\\sum_{i=1}^CL({y^\\prime}_i^m, {f^\\prime}_i^m)$$ 损失函数的第一项是有标签数据的损失，第二项是无标签数据的损失 在无标签数据的损失中， $y'$为无标签数据预测得到的伪标签，是直接取网络对无标签数据的预测的最大值为标签。 其中 $\\alpha (t)$决定着无标签数据的代价在网络更新的作用，选择合适的 $\\alpha (t)$ 很重要，太大性能退化，太小提升有限。 在网络初始时，网络的预测时不太准确的，因此生成的伪标签的准确性也不高。 在初始训练时， $\\alpha (t)$ 要设为 0，然后再慢慢增加，论文中给出其增长函数。 Semi-Supervised Learning with Ladder Networks： ladderNet 是有监督算法和无监督算法的有机结合。前面提及到的无监督预训练+有监督微调的思想中所有监督和无监督是分开的，两个阶段的训练相互独立，并不能称之为真正的半监督学习。 无监督学习是用重构样本进行训练，其编码（学习特征）的目的是尽可能地保留原始数据的信息；而有监督学习是用于分类，希望只保留其本质特征，去除不必要的特征。 举例来说：分类任务判断一张人脸图片是单眼皮，还是双眼皮；那么有监督学习经过训练完毕后，就会尽可能的把与这个分类任务无关的信息过滤掉，过滤的越好，那么分类的精度将会更高。 比如一个人的嘴巴、鼻子信息这些都是与这个分类任务无关的，那么就要尽量的过滤掉，因此，基于这个原因以至于一直以来有监督学习和无监督学习不能很好的兼容在一起。 ladderNet 成功的原因在于损失函数和 skip connection 。通过在每层的编码器和解码器之间添加跳跃连接（skip connection），减轻模型较高层表示细节的压力，使得无监督学习和有监督学习能结合在一起，并在最高层添加分类器。 $$Cost=-\\sum_{n=1}^NlogP(\\hat{y}(n)=y^*(n)\\mid x(n))+\\sum_{n=N+1}^M\\lambda_l ReconsructionCost({z^{(l)}}_{(n)},{\\hat{z}^{(l)}}_{(n)})$$ 损失函数的第一项是有标签样本数据的交叉熵损失函数，第二项是无监督各层噪声解码器重构误差欧式损失函数 ","date":"2020-07-02","objectID":"/semisupervisedlearning/:2:0","tags":["半监督","深度学习"],"title":"半监督深度学习","uri":"/semisupervisedlearning/"},{"categories":["计算机视觉"],"content":"参考 半监督学习 半监督深度学习小结 Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks Semi-Supervised Learning with Ladder Networks ","date":"2020-07-02","objectID":"/semisupervisedlearning/:3:0","tags":["半监督","深度学习"],"title":"半监督深度学习","uri":"/semisupervisedlearning/"},{"categories":["TF2.1学习笔记"],"content":"Embedding 独热码：数量大，过于稀疏，映射之间是独立的，没有表现出关联性 Embedding：一种单词编码方法，以低维向量实现了编码，这种编码通过神经网络训练优化，能表达出单词的相关性。 TF描述Embedding层 tf.keras.layers.Embedding(词汇表大小，编码维度) # 编码维度就是用几个数字表达一个单词 # 对1-100进行编码， [4] 编码为 [0.25, 0.1, 0.11] tf.keras.layers.Embedding(100, 3 ) 入Embedding时， x_train维度：[送入样本数， 循环核时间展开步数] ","date":"2020-06-24","objectID":"/rnn2/:1:0","tags":["循环神经网络","LSTM","GRU"],"title":"TensorFlow2.1入门学习笔记(16)——实战使用RNN，LSTM，GRU实现股票预测","uri":"/rnn2/"},{"categories":["TF2.1学习笔记"],"content":"RNN使用Embedding 编码，预测字母 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, SimpleRNN, Embedding import matplotlib.pyplot as plt import os input_word = \"abcdefghijklmnopqrstuvwxyz\" w_to_id = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25} # 单词映射到数值id的词典 training_set_scaled = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25] x_train = [] y_train = [] for i in range(4, 26): x_train.append(training_set_scaled[i - 4:i]) y_train.append(training_set_scaled[i]) np.random.seed(7) np.random.shuffle(x_train) np.random.seed(7) np.random.shuffle(y_train) tf.random.set_seed(7) # 使x_train符合Embedding输入要求：[送入样本数， 循环核时间展开步数] ， # 此处整个数据集送入所以送入，送入样本数为len(x_train)；输入4个字母出结果，循环核时间展开步数为4。 x_train = np.reshape(x_train, (len(x_train), 4)) y_train = np.array(y_train) model = tf.keras.Sequential([ Embedding(26, 2), SimpleRNN(10), Dense(26, activation='softmax') ]) model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/rnn_embedding_4pre1.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True, monitor='loss') # 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型 history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback]) model.summary() file = open('./weights.txt', 'w') # 参数提取 for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和验证集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] loss = history.history['loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.title('Training Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.title('Training Loss') plt.legend() plt.show() ################# predict ################## preNum = int(input(\"input the number of test alphabet:\")) for i in range(preNum): alphabet1 = input(\"input test alphabet:\") alphabet = [w_to_id[a] for a in alphabet1] # 使alphabet符合Embedding输入要求：[送入样本数， 时间展开步数]。 # 此处验证效果送入了1个样本，送入样本数为1；输入4个字母出结果，循环核时间展开步数为4。 alphabet = np.reshape(alphabet, (1, 4)) result = model.predict([alphabet]) pred = tf.argmax(result, axis=1) pred = int(pred) tf.print(alphabet1 + '-\u003e' + input_word[pred]) 运行结果 ","date":"2020-06-24","objectID":"/rnn2/:2:0","tags":["循环神经网络","LSTM","GRU"],"title":"TensorFlow2.1入门学习笔记(16)——实战使用RNN，LSTM，GRU实现股票预测","uri":"/rnn2/"},{"categories":["TF2.1学习笔记"],"content":"用RNN实现股票预测 数据源 使用tushare模块下载股票数据，TuShare是一个著名的免费、开源的python财经数据接口包。其官网主页为：TuShare -财经数据接口包。该接口包如今提供了大量的金融数据，涵盖了股票、基本面、宏观、新闻的等诸多类别数据（具体请自行查看官网），并还在不断更新中。TuShare可以基本满足量化初学者的回测需求 import tushare as ts import matplotlib.pyplot as plt df1 = ts.get_k_data(‘600519’, ktype='D’, start='2010-06-22’, end='2020-06-22’) datapath1 = “./BSH600519.csv” df1.to_csv(datapath1) 代码 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dropout, Dense, SimpleRNN import matplotlib.pyplot as plt import os import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error, mean_absolute_error import math maotai = pd.read_csv('./BSH600519.csv') # 读取股票文件 training_set = maotai.iloc[0:2426 - 300, 2:3].values # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价 test_set = maotai.iloc[2426 - 300:, 2:3].values # 后300天的开盘价作为测试集 # 归一化 sc = MinMaxScaler(feature_range=(0, 1)) # 定义归一化：归一化到(0，1)之间 training_set_scaled = sc.fit_transform(training_set) # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化 test_set = sc.transform(test_set) # 利用训练集的属性对测试集进行归一化 x_train = [] y_train = [] x_test = [] y_test = [] # 测试集：csv表格中前2426-300=2126天数据 # 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。 for i in range(60, len(training_set_scaled)): x_train.append(training_set_scaled[i - 60:i, 0]) y_train.append(training_set_scaled[i, 0]) # 对训练集进行打乱 np.random.seed(7) np.random.shuffle(x_train) np.random.seed(7) np.random.shuffle(y_train) tf.random.set_seed(7) # 将训练集由list格式变为array格式 x_train, y_train = np.array(x_train), np.array(y_train) # 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。 # 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1 x_train = np.reshape(x_train, (x_train.shape[0], 60, 1)) # 测试集：csv表格中后300天数据 # 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。 for i in range(60, len(test_set)): x_test.append(test_set[i - 60:i, 0]) y_test.append(test_set[i, 0]) # 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数] x_test, y_test = np.array(x_test), np.array(y_test) x_test = np.reshape(x_test, (x_test.shape[0], 60, 1)) model = tf.keras.Sequential([ SimpleRNN(80, return_sequences=True), Dropout(0.2), SimpleRNN(100), Dropout(0.2), Dense(1) ]) model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error') # 损失函数用均方误差 # 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值 checkpoint_save_path = \"./checkpoint/rnn_stock.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True, monitor='val_loss') history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() file = open('./weights.txt', 'w') # 参数提取 for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() loss = history.history['loss'] val_loss = history.history['val_loss'] plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Validation Loss') plt.title('Training and Validation Loss') plt.legend() plt.show() ################## predict ###################### # 测试集输入模型进行预测 predicted_stock_price = model.predict(x_test) # 对预测数据还原---从（0，1）反归一化到原始范围 predicted_stock_price = sc.inverse_transform(predicted_stock_price) # 对真实数据还原---从（0，1）反归一化到原始范围 real_stock_price = sc.inverse_transform(test_set[60:]) # 画出真实数据和预测数据的对比曲线 plt.plot(real_stock_price, color='red', label='MaoTai Stock Price') plt.plot(predicted_stock_price, color='blue', label='Predicted MaoTai Stock Price') plt.title('MaoTai Stock Price Prediction') plt.xlabel('Time') plt.ylabel('MaoTai Stock Price') plt.legend() plt.show() ##########evaluate######","date":"2020-06-24","objectID":"/rnn2/:3:0","tags":["循环神经网络","LSTM","GRU"],"title":"TensorFlow2.1入门学习笔记(16)——实战使用RNN，LSTM，GRU实现股票预测","uri":"/rnn2/"},{"categories":["TF2.1学习笔记"],"content":"用LSTM实现股票预测 传统的RNN网络可以通过记忆体实现短期记忆，进行连续数据的预测，但是当连续数据过长时，会使展开的时间步过长，在反向传播更新参数时，梯度要按时间步连续相乘会导致梯度消失。 LSTM 由Hochreiter \u0026 Schmidhuber 于1997年提出，通过门控单元改善了RNN长期依赖问题。 Sepp Hochreiter,Jurgen Schmidhuber.LONG SHORT-TERM MEMORY.Neural Computation,December 1997 LSTM计算过程: 输入门：$i_t = \\sigma (W_i) \\cdot [h_{t-1},x_t] + b_i$ 遗忘门：$f_t = \\sigma (W_f) \\cdot [h_{t-1},x_t] + b_f$ 输出门：$o_t = \\sigma (W_o) \\cdot [h_{t-1},x_t] + b_o$ 细胞态（长期记忆）：$C_t = f_t \\cdot C_{t-1} + i_t\\cdot \\breve{C_t}$ 记忆体（短期记忆）：$h_t = o_t \\cdot tanh(C_t)$ 候选体（归纳出的新知识）：$\\breve{C_t} = tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$ TF描述LSTM层: tf.keras.layers.LSTM(记忆体个数，return_sequences=是否返回输出) # 参数 return_sequences=True 各时间步输出ht return_sequences=False 仅最后时间步输出ht（默认） # 例 model = tf.keras.Sequential([ LSTM(80, return_sequences=True), Dropout(0.2), LSTM(100), Dropout(0.2), Dense(1) ]) 代码 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dropout, Dense, LSTM import matplotlib.pyplot as plt import os import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error, mean_absolute_error import math maotai = pd.read_csv('./BSH600519.csv') # 读取股票文件 training_set = maotai.iloc[0:2426 - 300, 2:3].values # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价 test_set = maotai.iloc[2426 - 300:, 2:3].values # 后300天的开盘价作为测试集 # 归一化 sc = MinMaxScaler(feature_range=(0, 1)) # 定义归一化：归一化到(0，1)之间 training_set_scaled = sc.fit_transform(training_set) # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化 test_set = sc.transform(test_set) # 利用训练集的属性对测试集进行归一化 x_train = [] y_train = [] x_test = [] y_test = [] # 测试集：csv表格中前2426-300=2126天数据 # 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。 for i in range(60, len(training_set_scaled)): x_train.append(training_set_scaled[i - 60:i, 0]) y_train.append(training_set_scaled[i, 0]) # 对训练集进行打乱 np.random.seed(7) np.random.shuffle(x_train) np.random.seed(7) np.random.shuffle(y_train) tf.random.set_seed(7) # 将训练集由list格式变为array格式 x_train, y_train = np.array(x_train), np.array(y_train) # 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。 # 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1 x_train = np.reshape(x_train, (x_train.shape[0], 60, 1)) # 测试集：csv表格中后300天数据 # 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。 for i in range(60, len(test_set)): x_test.append(test_set[i - 60:i, 0]) y_test.append(test_set[i, 0]) # 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数] x_test, y_test = np.array(x_test), np.array(y_test) x_test = np.reshape(x_test, (x_test.shape[0], 60, 1)) model = tf.keras.Sequential([ LSTM(80, return_sequences=True), Dropout(0.2), LSTM(100), Dropout(0.2), Dense(1) ]) model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error') # 损失函数用均方误差 # 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值 checkpoint_save_path = \"./checkpoint/LSTM_stock.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True, monitor='val_loss') history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() file = open('./weights.txt', 'w') # 参数提取 for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() loss = history.history['loss'] val_loss = history.history['val_loss'] plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Validation Loss') plt.title('Training and Validation Loss') plt.legend() plt.show() ################## predict ###################### # 测试集输入模型进行预测 predicted_stock_price = model.predict(x_test) # 对预测数据还原---从（0，1）反归一化到原始范围 predicted","date":"2020-06-24","objectID":"/rnn2/:4:0","tags":["循环神经网络","LSTM","GRU"],"title":"TensorFlow2.1入门学习笔记(16)——实战使用RNN，LSTM，GRU实现股票预测","uri":"/rnn2/"},{"categories":["TF2.1学习笔记"],"content":"用GRU实现股票预测 GRU是由LSTM简化得到的 GRU由Cho等人于2014年提出，优化LSTM结构。 Kyunghyun Cho,Bart van Merrienboer,Caglar Gulcehre,Dzmitry Bahdanau,Fethi Bougares,HolgerSchwenk,Yoshua Bengio.Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.Computer ence, 2014. GRU计算过程： 更新门：$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$ 重置门：$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$ 记忆体：$h_t = (1-z)\\cdot h_{t-1} + z_t \\cdot \\breve{h_t}$ 候选隐藏层：$\\breve{h_t} = tanh(W \\cdot [r_t \\cdot h_{t-1}, x_t])$ TF描述GRU层: tf.keras.layers.GRU(记忆体个数，return_sequences=是否返回输出) # 参数 return_sequences=True 各时间步输出ht return_sequences=False 仅最后时间步输出ht（默认） # 例 model = tf.keras.Sequential([ GRU(80, return_sequences=True), Dropout(0.2), GRU(100), Dropout(0.2), Dense(1) ]) 代码 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dropout, Dense, GRU import matplotlib.pyplot as plt import os import pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error, mean_absolute_error import math maotai = pd.read_csv('./BSH600519.csv') # 读取股票文件 training_set = maotai.iloc[0:2426 - 300, 2:3].values # 前(2426-300=2126)天的开盘价作为训练集,表格从0开始计数，2:3 是提取[2:3)列，前闭后开,故提取出C列开盘价 test_set = maotai.iloc[2426 - 300:, 2:3].values # 后300天的开盘价作为测试集 # 归一化 sc = MinMaxScaler(feature_range=(0, 1)) # 定义归一化：归一化到(0，1)之间 training_set_scaled = sc.fit_transform(training_set) # 求得训练集的最大值，最小值这些训练集固有的属性，并在训练集上进行归一化 test_set = sc.transform(test_set) # 利用训练集的属性对测试集进行归一化 x_train = [] y_train = [] x_test = [] y_test = [] # 测试集：csv表格中前2426-300=2126天数据 # 利用for循环，遍历整个训练集，提取训练集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建2426-300-60=2066组数据。 for i in range(60, len(training_set_scaled)): x_train.append(training_set_scaled[i - 60:i, 0]) y_train.append(training_set_scaled[i, 0]) # 对训练集进行打乱 np.random.seed(7) np.random.shuffle(x_train) np.random.seed(7) np.random.shuffle(y_train) tf.random.set_seed(7) # 将训练集由list格式变为array格式 x_train, y_train = np.array(x_train), np.array(y_train) # 使x_train符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。 # 此处整个数据集送入，送入样本数为x_train.shape[0]即2066组数据；输入60个开盘价，预测出第61天的开盘价，循环核时间展开步数为60; 每个时间步送入的特征是某一天的开盘价，只有1个数据，故每个时间步输入特征个数为1 x_train = np.reshape(x_train, (x_train.shape[0], 60, 1)) # 测试集：csv表格中后300天数据 # 利用for循环，遍历整个测试集，提取测试集中连续60天的开盘价作为输入特征x_train，第61天的数据作为标签，for循环共构建300-60=240组数据。 for i in range(60, len(test_set)): x_test.append(test_set[i - 60:i, 0]) y_test.append(test_set[i, 0]) # 测试集变array并reshape为符合RNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数] x_test, y_test = np.array(x_test), np.array(y_test) x_test = np.reshape(x_test, (x_test.shape[0], 60, 1)) model = tf.keras.Sequential([ GRU(80, return_sequences=True), Dropout(0.2), GRU(100), Dropout(0.2), Dense(1) ]) model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='mean_squared_error') # 损失函数用均方误差 # 该应用只观测loss数值，不观测准确率，所以删去metrics选项，一会在每个epoch迭代显示时只显示loss值 checkpoint_save_path = \"./checkpoint/stock.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True, monitor='val_loss') history = model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() file = open('./weights.txt', 'w') # 参数提取 for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() loss = history.history['loss'] val_loss = history.history['val_loss'] plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Validation Loss') plt.title('Training and Validation Loss') plt.legend() plt.show() ################## predict ###################### # 测试集输入模型进行预测 predicted_stock_price = model.predict(x_test) # 对预测数据还原---从（0，1）反归一化到原始范围 predicted_stock_price = sc.inverse_transform(predicted_stock_price) # 对真实数据还原---从（0，1）反归一化到原始范围 real","date":"2020-06-24","objectID":"/rnn2/:5:0","tags":["循环神经网络","LSTM","GRU"],"title":"TensorFlow2.1入门学习笔记(16)——实战使用RNN，LSTM，GRU实现股票预测","uri":"/rnn2/"},{"categories":["TF2.1学习笔记"],"content":"卷积就是特征提取器，通过卷积计算层提取空间信息，例如我们可以用卷积和提取一张图片的空间特征，再把提取到的空间特征送入全连接网络，实现离散数据的分类。但是一些与时间相关的，只可以根据上文预测书下文来预测。 例如： 看到这几个字会下意识地想到“水”，这是因为脑具有记忆。记忆体记住了上文中提到的“鱼离不开”这几个字。下意识的预测出了可能性最大的“水”字，这种预测就是通过提取历史数据的特征，预测出接下来最可能发生的情况。 ","date":"2020-06-23","objectID":"/rnn1/:0:0","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"循环核 通过不同时刻的参数共享，实现了对时间序列的信息提取。 ","date":"2020-06-23","objectID":"/rnn1/:1:0","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"具体模型： $$y_t = softmax(h_t w_{hy} + b_y)$$ $$h_t = tanh(x_t w_{xh} + h_{t-1}w_{hh})$$ 输入特征：$x$ 当前记忆体输出特征：$y_t$ 当前记忆体存储状态信息：$h_t$ 上一时刻记忆体存储状态信息：$h_{t-1}$ 参数矩阵：$w_{xh}, w_{hh}, w_{hy}$ 偏置项：$b_h$ 前向传播时：记忆体内存储的状态信息$h_t$ ，在每个时刻都被刷新，三个参数矩阵$w_{xh} , w_{hh}, w_{hy}$自始至终都是固定不变的。 反向传播时：三个参数矩阵$w_{xh} , w_{hh}, w_{hy}$被梯度下降法更新。 ","date":"2020-06-23","objectID":"/rnn1/:1:1","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"循环核按照时间步展开 循环核按时间轴方向展开 每个时刻$h_t$被刷新，所训练优化的就是三个参数矩阵$w_{xh} , w_{hh}, w_{hy}$，训练完成后使用效果最好的参数矩阵，执行前向传播，输出预测结果。类比人脑的记忆体，每个时刻都根据当前的输入而更新，当前的预测推理是根据以往的知识积累，用固化下来的参数矩阵进行的推理判断。**循环神经网络：借助循环核提取时间特征后，送入全连接网络。** ","date":"2020-06-23","objectID":"/rnn1/:2:0","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"循环计算层 每个循环核构成一层循环计算层循环计算的层数是向着输出方向增长的，每个循环核中的记忆体个数是根据寻求来指定的 TF描述循环计算层 tf.keras.layers.SimpleRNN(记忆体个数，activation=‘激活函数’ ，return_sequences=是否每个时刻输出ht到下一层) # 参数 activation=‘激活函数’ （不写，默认使用tanh） return_sequences=True 各时间步输出ht return_sequences=False 仅最后时间步输出ht（默认） # 例： SimpleRNN(3, return_sequences=True) 注意： RNN要求输入数据（x_train）的维度是三维的[送入样本数，循环核时间展开步数，每个时间步输入特征个数] ","date":"2020-06-23","objectID":"/rnn1/:3:0","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"循环计算过程 循环网络的输入数据都是数字，因此需要先将数据转换为数字 例如字母预测：输入a预测出b，输入b预测出c，输入c预测出d，输入d预测出e，输入e预测出a 使用独热码将字母编码： 独热码 字母 10000 a 01000 b 00100 c 00010 d 00001 e ","date":"2020-06-23","objectID":"/rnn1/:4:0","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"用RNN实现输入一个字母，预测下一个字母 import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, SimpleRNN import matplotlib.pyplot as plt import os input_word = \"abcde\" w_to_id = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4} # 单词映射到数值id的词典 id_to_onehot = {0: [1., 0., 0., 0., 0.], 1: [0., 1., 0., 0., 0.], 2: [0., 0., 1., 0., 0.], 3: [0., 0., 0., 1., 0.], 4: [0., 0., 0., 0., 1.]} # id编码为one-hot x_train = [id_to_onehot[w_to_id['a']], id_to_onehot[w_to_id['b']], id_to_onehot[w_to_id['c']], id_to_onehot[w_to_id['d']], id_to_onehot[w_to_id['e']]] y_train = [w_to_id['b'], w_to_id['c'], w_to_id['d'], w_to_id['e'], w_to_id['a']] np.random.seed(7) np.random.shuffle(x_train) np.random.seed(7) np.random.shuffle(y_train) tf.random.set_seed(7) # 使x_train符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。 # 此处整个数据集送入，送入样本数为len(x_train)；输入1个字母出结果，循环核时间展开步数为1; 表示为独热码有5个输入特征，每个时间步输入特征个数为5 x_train = np.reshape(x_train, (len(x_train), 1, 5)) y_train = np.array(y_train) model = tf.keras.Sequential([ SimpleRNN(3), Dense(5, activation='softmax') ]) model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/rnn_onehot_1pre1.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True, monitor='loss') # 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型 history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback]) model.summary() # print(model.trainable_variables) file = open('./weights.txt', 'w') # 参数提取 for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和验证集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] loss = history.history['loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.title('Training Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.title('Training Loss') plt.legend() plt.show() ############### predict ############# preNum = int(input(\"input the number of test alphabet:\")) for i in range(preNum): alphabet1 = input(\"input test alphabet:\") alphabet = [id_to_onehot[w_to_id[alphabet1]]] # 使alphabet符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。此处验证效果送入了1个样本，送入样本数为1；输入1个字母出结果，所以循环核时间展开步数为1; 表示为独热码有5个输入特征，每个时间步输入特征个数为5 alphabet = np.reshape(alphabet, (1, 1, 5)) result = model.predict([alphabet]) pred = tf.argmax(result, axis=1) pred = int(pred) tf.print(alphabet1 + '-\u003e' + input_word[pred]) 运行结果 ","date":"2020-06-23","objectID":"/rnn1/:4:1","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"用RNN实现输入连续四个字母，预测下一个字母 即： 输入abcd输出e 输入bcde输出a 输入cdea输出b 输入deab输出c 输入eabc输出d import numpy as np import tensorflow as tf from tensorflow.keras.layers import Dense, SimpleRNN import matplotlib.pyplot as plt import os input_word = \"abcde\" w_to_id = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4} # 单词映射到数值id的词典 id_to_onehot = {0: [1., 0., 0., 0., 0.], 1: [0., 1., 0., 0., 0.], 2: [0., 0., 1., 0., 0.], 3: [0., 0., 0., 1., 0.], 4: [0., 0., 0., 0., 1.]} # id编码为one-hot x_train = [ [id_to_onehot[w_to_id['a']], id_to_onehot[w_to_id['b']], id_to_onehot[w_to_id['c']], id_to_onehot[w_to_id['d']]], [id_to_onehot[w_to_id['b']], id_to_onehot[w_to_id['c']], id_to_onehot[w_to_id['d']], id_to_onehot[w_to_id['e']]], [id_to_onehot[w_to_id['c']], id_to_onehot[w_to_id['d']], id_to_onehot[w_to_id['e']], id_to_onehot[w_to_id['a']]], [id_to_onehot[w_to_id['d']], id_to_onehot[w_to_id['e']], id_to_onehot[w_to_id['a']], id_to_onehot[w_to_id['b']]], [id_to_onehot[w_to_id['e']], id_to_onehot[w_to_id['a']], id_to_onehot[w_to_id['b']], id_to_onehot[w_to_id['c']]], ] y_train = [w_to_id['e'], w_to_id['a'], w_to_id['b'], w_to_id['c'], w_to_id['d']] np.random.seed(7) np.random.shuffle(x_train) np.random.seed(7) np.random.shuffle(y_train) tf.random.set_seed(7) # 使x_train符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。 # 此处整个数据集送入，送入样本数为len(x_train)；输入4个字母出结果，循环核时间展开步数为4; 表示为独热码有5个输入特征，每个时间步输入特征个数为5 x_train = np.reshape(x_train, (len(x_train), 4, 5)) y_train = np.array(y_train) model = tf.keras.Sequential([ SimpleRNN(3), Dense(5, activation='softmax') ]) model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/rnn_onehot_4pre1.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True, monitor='loss') # 由于fit没有给出测试集，不计算测试集准确率，根据loss，保存最优模型 history = model.fit(x_train, y_train, batch_size=32, epochs=100, callbacks=[cp_callback]) model.summary() # print(model.trainable_variables) file = open('./weights.txt', 'w') # 参数提取 for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和验证集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] loss = history.history['loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.title('Training Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.title('Training Loss') plt.legend() plt.show() ############### predict ############# preNum = int(input(\"input the number of test alphabet:\")) for i in range(preNum): alphabet1 = input(\"input test alphabet:\") alphabet = [id_to_onehot[w_to_id[a]] for a in alphabet1] # 使alphabet符合SimpleRNN输入要求：[送入样本数， 循环核时间展开步数， 每个时间步输入特征个数]。此处验证效果送入了1个样本，送入样本数为1；输入4个字母出结果，所以循环核时间展开步数为4; 表示为独热码有5个输入特征，每个时间步输入特征个数为5 alphabet = np.reshape(alphabet, (1, 4, 5)) result = model.predict([alphabet]) pred = tf.argmax(result, axis=1) pred = int(pred) tf.print(alphabet1 + '-\u003e' + input_word[pred]) 运行结果 ","date":"2020-06-23","objectID":"/rnn1/:4:2","tags":["循环神经网络","RNN"],"title":"TensorFlow2.1入门学习笔记(15)——循环神经网络，顺序字母预测","uri":"/rnn1/"},{"categories":["TF2.1学习笔记"],"content":"InceptionNet InceptionNet诞生于2014年，当年ImageNet竞赛冠军，Top5错误率为6.67% InceptionNet引入了Inception结构块，在同一层网络内使用不同尺寸的卷积核，提升了模型感知力使用了批标准化缓解了梯度消失 Inception V1（GoogleNet）——构建了1x1、3x3、5x5的 conv 和3x3的 pooling 的分支网络module，同时使用MLPConv和全局平均池化，扩宽卷积层网络宽度，增加了网络对尺度的适应性； Inception V2——提出了Batch Normalization，代替Dropout和LRN，其正则化的效果让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅提高，同时借鉴VGGNet使用两个3x3的卷积核代替5x5的卷积核，在降低参数量同时提高网络学习能力； Inception V3——引入了 Factorization，将一个较大的二维卷积拆成两个较小的一维卷积，比如将3x3卷积拆成1x3卷积和3x1卷积，一方面节约了大量参数，加速运算并减轻了过拟合，同时增加了一层非线性扩展模型表达能力，除了在 Inception Module 中使用分支，还在分支中使用了分支（Network In Network In Network）； Inception V4——研究了 Inception Module 结合 Residual Connection，结合 ResNet 可以极大地加速训练，同时极大提升性能，在构建 Inception-ResNet 网络同时，还设计了一个更深更优化的 Inception v4 模型，能达到相媲美的性能。 网络结构 InceptionNet的基本单位是Inception结构块，在同一层网络中使用了不同尺寸的卷积核，可以提取不同尺寸的特征信息 通过1x1卷积核作用到输入特征图的每个像素点，通过设定少于输入特征图的深度达到降维减少了参数量和计算量 Inception结构块设计 class ConvBNAct(Model): def __init__(self, ch, kernel_size=3, strides=1, padding='same'): super(ConvBNAct, self).__init__() self.model = tf.keras.models.Sequential([ Conv2D(ch, kernel_size, strides=strides, padding='same'), BatchNormalization(), Activation('relu') ]) def call(self, x): x = self.model(x, training=False) return x class InceptionBlk(Model): def __init__(self, ch, strides=1): super(InceptionBlk, self).__init__() self.ch = ch self.strides = strides self.c1 = ConvBNAct(ch, kernel_size=1, strides=strides) self.c2_1 = ConvBNAct(ch, kernel_size=1, strides=strides) self.c2_2 = ConvBNAct(ch, kernel_size=3, strides=1) self.c3_1 = ConvBNAct(ch, kernel_size=1, strides=strides) self.c3_2 = ConvBNAct(ch, kernel_size=5, strides=1) self.p4_1 = MaxPool2D(3, strides=1, padding='same') self.p4_2 = ConvBNAct(ch, kernel_size=1, strides=strides) def call(self, x): x1 = self.c1(x) x2_1 = self.c2_1(x) x2_2 = self.c2_2(x2_1) x3_1 = self.c3_1(x) x3_2 = self.c3_2(x3_1) x4_1 = self.p4_1(x) x4_2 = self.p4_2(x4_1) x = tf.concat([x1, x2_2, x3_2, x4_2], axis=3) return x 精简InceptionNet 包含四个Inception结构快，每两个结构块组成一个block，每个block的第一个结构块步长是2，使输出特征数据减半，第二个结构块步长是1，因此将输出特征图深度加深（self.out_channels *= 2），尽可能保证特征抽取信息的承载量一致 网络搭建示例 import tensorflow as tf import numpy as np import os import matplotlib.pyplot as plt from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPool2D, Activation, BatchNormalization, Dropout from tensorflow.keras import Model np.set_printoptions(threshold=np.inf) cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class ConvBNAct(Model): def __init__(self, ch, kernel_size=3, strides=1, padding='same'): super(ConvBNAct, self).__init__() self.model = tf.keras.models.Sequential([ Conv2D(ch, kernel_size, strides=strides, padding='same'), BatchNormalization(), Activation('relu') ]) def call(self, x): x = self.model(x, training=False) return x class InceptionBlk(Model): def __init__(self, ch, strides=1): super(InceptionBlk, self).__init__() self.ch = ch self.strides = strides self.c1 = ConvBNAct(ch, kernel_size=1, strides=strides) self.c2_1 = ConvBNAct(ch, kernel_size=1, strides=strides) self.c2_2 = ConvBNAct(ch, kernel_size=3, strides=1) self.c3_1 = ConvBNAct(ch, kernel_size=1, strides=strides) self.c3_2 = ConvBNAct(ch, kernel_size=5, strides=1) self.p4_1 = MaxPool2D(3, strides=1, padding='same') self.p4_2 = ConvBNAct(ch, kernel_size=1, strides=strides) def call(self, x): x1 = self.c1(x) x2_1 = self.c2_1(x) x2_2 = self.c2_2(x2_1) x3_1 = self.c3_1(x) x3_2 = self.c3_2(x3_1) x4_1 = self.p4_1(x) x4_2 = self.p4_2(x4_1) x = tf.concat([x1, x2_2, x3_2, x4_2], axis=3) return x class Inception10(Model): def __init__(self, num_blocks, num_classes, init_ch=16, **kwargs): super(Inception10, self).__init__(**kwargs) self.in_channels = init_ch self.out_channels = init_ch self.num_blocks = num_blocks self.init_ch = init_ch self.c1 = ConvBNAct(init_ch) self.blocks = tf.keras.models.Sequential() for block_id in range(num_blocks): for layer_id in range(2): if layer_id == 0: block = Inceptio","date":"2020-06-19","objectID":"/convolutional3/:1:0","tags":["InceptionNet","ResNet","CNN"],"title":"TensorFlow2.1入门学习笔记(14)——卷积神经网络InceptionNet, ResNet示例","uri":"/convolutional3/"},{"categories":["TF2.1学习笔记"],"content":"ResNet ResNet诞生于2015年，当年ImageNet竞赛冠军，Top5错误率为3.57% 网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果，从前面的网络可以看出网络越深而效果越好的一个实践证据。但是实验发现深度网络出现了退化问题（Degradation problem）：网络深度增加时，网络准确度出现饱和，甚至出现下降。这个现象可以在下图中直观看出来：56层的网络比20层网络效果还要差。这不会是过拟合问题，因为56层网络的训练误差同样高。我们知道深层网络存在着梯度消失或者爆炸的问题，这使得深度学习模型很难训练。但是现在已经存在一些技术手段如BatchNorm来缓解这个问题。 假设现在有一个浅层网络，想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。因此不得不承认肯定是目前的训练方法有问题，才使得深层网络很难去找到一个好的参数。 何凯明由此提出了残差学习来解决退化问题。对于一个堆积层结构（几层堆积而成）当输入为$x$时其学习到的特征记为 $H(x)$ ，现在我们希望其可以学习到残差 $F(x) = H(x) - x$ ，这样其实原始的学习特征是 $F(x) + x$ 。之所以这样是因为残差学习相比原始特征直接学习更容易。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构如图4所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。 ResNet18的网络结构 网络搭建示例 import tensorflow as tf import numpy as np import os from tensorflow.keras.layers import Flatten, Conv2D, Dense, Activation, MaxPool2D, Dropout, BatchNormalization from tensorflow.keras import Model from matplotlib import pyplot as plt np.set_printoptions(threshold=np.inf) cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class ResBlock(Model): def __init__(self, filters, strides=1, residual_path=False): super(ResBlock, self).__init__() self.filters = filters self.strides = strides self.residual_path = residual_path self.c1 = Conv2D(filters, (3, 3), strides=strides, padding='same', use_bias=False) self.b1 = BatchNormalization() self.a1 = Activation('relu') self.c2 = Conv2D(filters, (3, 3), strides=1, padding='same', use_bias=False) self.b2 = BatchNormalization() if residual_path: self.down_c1 = Conv2D(filters, (1, 1), strides=strides, padding='same', use_bias=False) self.down_b1 = BatchNormalization() self.a2 = Activation('relu') def call(self, inputs): residual = inputs x = self.c1(inputs) x = self.b1(x) x = self.a1(x) x = self.c2(x) y = self.b2(x) if self.residual_path: residual = self.down_c1(inputs) residual = self.down_b1(residual) out = self.a2(y + residual) return out class ResNet(Model): def __init__(self, block_list, initial_filters=64): super(ResNet, self).__init__() self.num_blocks = len(block_list) self.block_list = block_list self.out_filters = initial_filters self.c1 = Conv2D(self.out_filters, (3, 3), strides=1, padding='same', use_bias=False) self.b1 = BatchNormalization() self.a1 = Activation('relu') self.blocks = tf.keras.Sequential() for block_id in range(len(block_list)): for layers_id in range(block_list[block_id]): if block_list != 0 and layers_id == 0: block = ResBlock(self.out_filters, strides=2, residual_path=True) else: block = ResBlock(self.out_filters, residual_path=False) self.blocks.add(block) self.out_filters *= 2 self.p1 = tf.keras.layers.AveragePooling2D() self.f1 = Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2()) def call(self, inputs): x = self.c1(inputs) x = self.b1(x) x = self.a1(x) x = self.blocks(x) x = self.p1(x) y = self.f1(x) return y def Resnet18(): return ResNet([2, 2, 2, 2]) def Resnet34(): return ResNet([3, 4, 6, 3]) model = Resnet18() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = './checkpoint/ResNet18.ckpt' if os.path.exists(checkpoint_save_path + '.index'): print('--------------- load the model -----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() acc = history.history['sparse_categori","date":"2020-06-19","objectID":"/convolutional3/:2:0","tags":["InceptionNet","ResNet","CNN"],"title":"TensorFlow2.1入门学习笔记(14)——卷积神经网络InceptionNet, ResNet示例","uri":"/convolutional3/"},{"categories":["TF2.1学习笔记"],"content":"Cifar10数据集 提供 5万张 32*32 像素点的十分类彩色图片和标签，用于训练。 提供 1万张 32*32 像素点的十分类彩色图片和标签，用于测试。 导入cifar10数据集： cifar10 = tf.keras.datasets.cifar10 (x_train, y_train),(x_test, y_test) = cifar10.load_data() 可视化训练集输入特征的第一个元素 plt.imshow(x_train[0]) #绘制图片 plt.show() 打印出训练集输入特征的第一个元素 print(\"x_train[0]:\\n\", x_train[0]) 打印出训练集标签的第一个元素 print(\"y_train[0]:\\n\", y_train[0]) 打印出整个训练集输入特征形状 print(\"x_train.shape:\\n\", x_train.shape) 打印出整个训练集标签的形状 print(\"y_train.shape:\\n\", y_train.shape) 打印出整个测试集输入特征的形状 print(\"x_test.shape:\\n\", x_test.shape) 打印出整个测试集标签的形状 print(\"y_test.shape:\\n\", y_test.shape) ","date":"2020-06-15","objectID":"/convolutional2/:1:0","tags":["LeNet","AlexNet","VGGNet","CNN"],"title":"TensorFlow2.1入门学习笔记(13)——卷积神经网络LeNet, AlexNet, VGGNet示例","uri":"/convolutional2/"},{"categories":["TF2.1学习笔记"],"content":"搭建网络 利用cifar10数据集搭建一个网络，训练模型 网络设计： 卷积层：6个5x5，步长为1，使用全零填充的卷积核；2个2x2，步长为2，使用全零填充的最大值池化核；20%的神经元休眠（暂时舍弃）。 全连接层：先将卷积训练的数据拉直；送入128个神经元，激活函数为“relu”，20%休眠的全连接；再将数据送入10个神经元，符合概率分布的全连接。 源码 import tensorflow as tf import os import numpy as np from matplotlib import pyplot as plt from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense from tensorflow.keras import Model np.set_printoptions(threshold=np.inf) cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class Baseline(Model): def __init__(self): super(Baseline, self).__init__() self.c1 = Conv2D(filters=6, kernel_size=(5, 5), padding='same') # 卷积层 self.b1 = BatchNormalization() # BN层 self.a1 = Activation('relu') # 激活层 self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same') # 池化层 self.d1 = Dropout(0.2) # dropout层 self.flatten = Flatten() self.f1 = Dense(128, activation='relu') self.d2 = Dropout(0.2) self.f2 = Dense(10, activation='softmax') def call(self, x): x = self.c1(x) x = self.b1(x) x = self.a1(x) x = self.p1(x) x = self.d1(x) x = self.flatten(x) x = self.f1(x) x = self.d2(x) y = self.f2(x) return y model = Baseline() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/Baseline.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() # print(model.trainable_variables) file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和验证集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] val_acc = history.history['val_sparse_categorical_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.plot(val_acc, label='Validation Accuracy') plt.title('Training and Validation Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Validation Loss') plt.title('Training and Validation Loss') plt.legend() plt.show() 运行结果 ","date":"2020-06-15","objectID":"/convolutional2/:2:0","tags":["LeNet","AlexNet","VGGNet","CNN"],"title":"TensorFlow2.1入门学习笔记(13)——卷积神经网络LeNet, AlexNet, VGGNet示例","uri":"/convolutional2/"},{"categories":["TF2.1学习笔记"],"content":"LeNet 是由Yann LeCun于1998年提出，是卷积网络的开篇之作 网络结构 网络搭建 class LeNet5(Model): def __init__(self): super(LeNet5, self).__init__() self.c1=Conv2D(fliters=6, kernel_size=(5, 5), activation='sigmoid') self.p1=MaxPool2D(pool_size=(2, 2), strides=2) self.c2=Conv2D(filters=16, kernel_size=(5, 5), activation='sigmoid') self.p2=MaxPool2D(pool_size=(2, 2), strides=2) self.flatten=Flatten() self.d1=Dense(128, activation='sigmoid') self.d2=Dense(84, activation='sigmoid') self.d3=Dense(10, activation='softmax') def call(self, x): x=self.c1(x) x=self.p1(x) x=self.c2(x) x=self.p2(x) x=self.flatten(x) x=self.d1(x) x=self.d2(x) y=self.d3(x) return y 示例： import tensorflow as tf from matplotlib import pyplot as plt import os import numpy as np from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense from tensorflow.keras import Model np.set_printoptions(threshold=np.inf) cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train, x_test = x_train/255.0, x_test/255.0 class LeNet5(Model): def __init__(self): super(LeNet5, self).__init__() self.c1 = Conv2D(filters=6, kernel_size=(5, 5), activation='sigmoid') self.p1 = MaxPool2D(pool_size=(2, 2), strides=2) self.c2 = Conv2D(filters=16, kernel_size=(5, 5), activation='sigmoid') self.p2 = MaxPool2D(pool_size=(2, 2), strides=2) self.flatten = Flatten() self.d1 = Dense(128, activation='sigmoid') self.d2 = Dense(84, activation='sigmoid') self.d3 = Dense(10, activation='softmax') def call(self, x): x = self.c1(x) x = self.p1(x) x = self.c2(x) x = self.p2(x) x = self.flatten(x) x = self.d1(x) x = self.d2(x) y = self.d3(x) return y model = LeNet5() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/LeNet5.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_save_path, save_weights_only = True, save_best_only = True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() # print(model.trainable_variables) file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和测试集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] val_acc = history.history['val_sparse_categorical_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.plot(val_acc, label='Validation Accuracy') plt.title('Training and Validation Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Valiation Loss') plt.title('Train and Validation Loss') plt.legend() plt.show() 运行结果 ","date":"2020-06-15","objectID":"/convolutional2/:3:0","tags":["LeNet","AlexNet","VGGNet","CNN"],"title":"TensorFlow2.1入门学习笔记(13)——卷积神经网络LeNet, AlexNet, VGGNet示例","uri":"/convolutional2/"},{"categories":["TF2.1学习笔记"],"content":"AlexNet AlexNet网络诞生于2012年，当年ImageNet竞赛的冠军，Top5错误率为16.4% 使用“relu”激活函数，提升了训练速度，使用Dropout缓解过拟合 网络结构 网络搭建示例 import tensorflow as tf import os import numpy as np from matplotlib import pyplot as plt from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dropout, Flatten, Dense from tensorflow.keras import Model np.set_printoptions(threshold=np.inf) cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class AlexNet8(Model): def __init__(self): super(AlexNet8, self).__init__() self.c1 = Conv2D(filters=96, kernel_size=(3, 3)) self.b1 = BatchNormalization() self.a1 = Activation('relu') self.p1 = MaxPool2D(pool_size=(3, 3), strides=2) self.c2 = Conv2D(filters=256, kernel_size=(3, 3)) self.b2 = BatchNormalization() self.a2 = Activation('relu') self.p2 = MaxPool2D(pool_size=(3, 3), strides=2) self.c3 = Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu') self.c4 = Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu') self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu') self.p3 = MaxPool2D(pool_size=(3, 3), strides=2) self.flatten = Flatten() self.f1 = Dense(2048, activation='relu') self.d1 = Dropout(0.5) self.f2 = Dense(2048, activation='relu') self.d2 = Dropout(0.5) self.f3 = Dense(10, activation='softmax') def call(self, x): x = self.c1(x) x = self.b1(x) x = self.a1(x) x = self.p1(x) x = self.c2(x) x = self.b2(x) x = self.a2(x) x = self.p2(x) x = self.c3(x) x = self.c4(x) x = self.c5(x) x = self.p3(x) x = self.flatten(x) x = self.f1(x) x = self.d1(x) x = self.f2(x) x = self.d2(x) y = self.f3(x) return y model = AlexNet8() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/AlexNet8.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() # print(model.trainable_variables) file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和验证集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] val_acc = history.history['val_sparse_categorical_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.plot(val_acc, label='Validation Accuracy') plt.title('Training and Validation Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Validation Loss') plt.title('Training and Validation Loss') plt.legend() plt.show() 运行结果 ","date":"2020-06-15","objectID":"/convolutional2/:4:0","tags":["LeNet","AlexNet","VGGNet","CNN"],"title":"TensorFlow2.1入门学习笔记(13)——卷积神经网络LeNet, AlexNet, VGGNet示例","uri":"/convolutional2/"},{"categories":["TF2.1学习笔记"],"content":"VGGNet VGGNet诞生于2014年，当年ImageNet竞赛的亚军，Top5错误率减小到7.3% 使用小尺寸卷积核，在减少参数的同时提高了识别的准确率，网络规整适合硬件加速 网络结构 网络搭建示例 import tensorflow as tf import numpy as np import os from tensorflow.keras.layers import Flatten, Conv2D, Dense, Activation, MaxPool2D, Dropout, BatchNormalization from tensorflow.keras import Model from matplotlib import pyplot as plt np.set_printoptions(threshold=np.inf) cifar10 = tf.keras.datasets.cifar10 (x_train, y_train), (x_test, y_test) = cifar10.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 class VGGNet4(Model): def __init__(self): super(VGGNet4, self).__init__() self.c1 = Conv2D(filters=64, kernel_size=(3, 3), padding='same') self.b1 = BatchNormalization() self.a1 = Activation('relu') self.c2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same') self.b2 = BatchNormalization() self.a2 = Activation('relu') self.p1 = MaxPool2D(pool_size=(2, 2), strides=2) self.d1 = Dropout(0.2) self.c3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same') self.b3 = BatchNormalization() self.a3 = Activation('relu') self.c4 = Conv2D(filters=128, kernel_size=(3, 3), padding='same') self.b4 = BatchNormalization() self.a4 = Activation('relu') self.p2 = MaxPool2D(pool_size=(2, 2), strides=2) self.d2 = Dropout(0.2) self.c5 = Conv2D(filters=256, kernel_size=(3, 3), padding='same') self.b5 = BatchNormalization() self.a5 = Activation('relu') self.c6 = Conv2D(filters=256, kernel_size=(3, 3), padding='same') self.b6 = BatchNormalization() self.a6 = Activation('relu') self.c7 = Conv2D(filters=256, kernel_size=(3, 3), padding='same') self.b7 = BatchNormalization() self.a7 = Activation('relu') self.p3 = MaxPool2D(pool_size=(2, 2), strides=2) self.d3 = Dropout(0.2) self.c8 = Conv2D(filters=512, kernel_size=(3, 3), padding='same') self.b8 = BatchNormalization() self.a8 = Activation('relu') self.c9 = Conv2D(filters=512, kernel_size=(3, 3), padding='same') self.b9 = BatchNormalization() self.a9 = Activation('relu') self.c10 = Conv2D(filters=512, kernel_size=(3, 3), padding='same') self.b10 = BatchNormalization() self.a10 = Activation('relu') self.p4 = MaxPool2D(pool_size=(2, 2), strides=2) self.d4 = Dropout(0.2) self.c11 = Conv2D(filters=512, kernel_size=(3, 3), padding='same') self.b11 = BatchNormalization() self.a11 = Activation('relu') self.c12 = Conv2D(filters=512, kernel_size=(3, 3), padding='same') self.b12 = BatchNormalization() self.a12 = Activation('relu') self.c13 = Conv2D(filters=512, kernel_size=(3, 3), padding='same') self.b13 = BatchNormalization() self.a13 = Activation('relu') self.p5 = MaxPool2D(pool_size=(2, 2), strides=2) self.d5 = Dropout(0.2) self.flatten = Flatten() self.f1 = Dense(512, activation='relu') self.d6 = Dropout(0.2) self.f2 = Dense(512, activation='relu') self.d7 = Dropout(0.2) self.f3 = Dense(10, activation='softmax') def call(self, x): x = self.c1(x) x = self.b1(x) x = self.a1(x) x = self.c2(x) x = self.b2(x) x = self.a2(x) x = self.p1(x) x = self.d1(x) x = self.c3(x) x = self.b3(x) x = self.a3(x) x = self.c4(x) x = self.b4(x) x = self.a4(x) x = self.p2(x) x = self.d2(x) x = self.c5(x) x = self.b5(x) x = self.a5(x) x = self.c6(x) x = self.b6(x) x = self.a6(x) x = self.c7(x) x = self.b7(x) x = self.a7(x) x = self.p3(x) x = self.d3(x) x = self.c8(x) x = self.b8(x) x = self.a8(x) x = self.c9(x) x = self.b9(x) x = self.a9(x) x = self.c10(x) x = self.b10(x) x = self.a10(x) x = self.p4(x) x = self.d4(x) x = self.c11(x) x = self.b11(x) x = self.a11(x) x = self.c12(x) x = self.b12(x) x = self.a12(x) x = self.c13(x) x = self.b13(x) x = self.a13(x) x = self.p5(x) x = self.d5(x) x = self.flatten(x) x = self.f1(x) x = self.d6(x) x = self.f2(x) x = self.d7(x) y = self.f3(x) return y model = VGGNet4() model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = './checkpoint/AGGNet4.ckpt' if os.path.exists(checkpoint_save_path + '.index'): print('--------------- load the model -----------------') model.load","date":"2020-06-15","objectID":"/convolutional2/:5:0","tags":["LeNet","AlexNet","VGGNet","CNN"],"title":"TensorFlow2.1入门学习笔记(13)——卷积神经网络LeNet, AlexNet, VGGNet示例","uri":"/convolutional2/"},{"categories":["TF2.1学习笔记"],"content":"每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测的结果。随着隐藏层增多，网络规模的增大，待优化参数过多容易导致模型过拟合 ","date":"2020-06-13","objectID":"/convolutional1/:0:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"卷积计算过程： 全连接NN：每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出为预测的结果。 实际项目中的图片多是高分辨率彩色图 随着隐藏层增多，网络规模的增大，待优化参数过多容易导致模型过拟合 实际应用时会先对原始图像进行特征提取再把提取到的特征送给全连接网络 ","date":"2020-06-13","objectID":"/convolutional1/:1:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"卷积（Convolutional） 卷积计算可是一种有效提取图像特征的方法 一般会用一个正方形的卷积核，按指定步长，在输入特征图上滑动，遍历输入特征图中的每个像素点。每一个步长，卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输出特征的一个像素点 输入特征图的深度（channel数），决定了当前层卷积核的深度；当前层卷积核的个数，决定了当前层输出特征图的深度。 卷积核 卷积核的计算过程 ","date":"2020-06-13","objectID":"/convolutional1/:1:1","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"感受野（Receptive Field） 卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小。 例如：5x5x1的输入特征，经过2次3x3x1的卷积过程感受野是5；经过1次5x5x1的卷积过程感受野也是5，感受野相同，则特征提取能力相同。 感受野的选择 当输入特征图边长大于10像素点时，两层3x3的卷积核比一层5x5的卷积性能要好，因此在神经网络卷积计算过程中常采用两层3x3的卷积代替已成5x5的卷积。 ","date":"2020-06-13","objectID":"/convolutional1/:2:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"全零填充（Padding） 当需要卷积计算保持输入特征图的尺寸不变则使用全零填充，在输入特征的周围用零填充 在5x5x1的输入特征图经过全零填充后，在经过3x3x1的卷积核，进行步长为1的卷积计算，输出特征图仍是5x5x1 输出特征图维度的计算公式 $$ padding = \\left\\{ \\begin{array}{lr} SAME(全0填充)\u0026\\frac{入长}{步长} (向上取整)\\\\ VALID(不全零填充)\u0026\\frac{入长-核长+1}{步长} (向上取整) \\end{array} \\right. $$ TenaorFlow描述全零填充 用参数padding = ‘SAME’ 或 padding = ‘VALID’表示 ","date":"2020-06-13","objectID":"/convolutional1/:3:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"TF描述卷积层 tf.keras.layers.Conv2D ( filters = 卷积核个数, kernel_size = 卷积核尺寸, #正方形写核长整数，或（核高h，核宽w） strides = 滑动步长, #横纵向相同写步长整数，或(纵向步长h，横向步长w)，默认1 padding = “same” or “valid”, #使用全零填充是“same”，不使用是“valid”（默认） activation = “ relu ” or “ sigmoid ” or “ tanh ” or “ softmax”等 , #如有BN此处不写 input_shape = (高, 宽 , 通道数) #输入特征图维度，可省略 ) ","date":"2020-06-13","objectID":"/convolutional1/:4:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"批标准化（BN） 神经网络对0附近的数据更敏感，单随网络层数的增加特征数据会出现偏离0均值的情况 标准化：使数据符合0均值，1为标准差的分布。 批标准化：对一小批数据（batch），做标准化处理。 标准化可以是数据重新回归到标准正态分布常用在卷积操作和激活操作之间 批标准化操作将原本偏移的特征数据重新拉回到0均值，使进入到激活函数的数据分布在激活函数线性区使得输入数据的微小变化更明显的提现到激活函数的输出，提升了激活函数对输入数据的区分力。但是这种简单的特征数据标准化使特征数据完全满足标准正态分布。集中在激活函数中心的线性区域，使激活函数丧失了非线性特性。因此在BN操作中为每个卷积核引入了两个可训练参数，缩放因子$\\gamma$和偏移因子$\\beta$。反向传播时缩放因子$\\gamma$和偏移因子$\\beta$会与其他带训练参数一同被训练优化，使标准状态分布后的特征数据。通过缩放因子和偏移因子优化了特征数据分布的宽窄和偏移量。保证了网络的非线性表的力。 BN位于卷积层之后，激活层之前 TensorFlow描述批标准化 tf.keras.layers.BatchNormalization() ","date":"2020-06-13","objectID":"/convolutional1/:5:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"池化（Pooling） 池化用于减少特征数据量。最大值池化可提取图片纹理，均值池化可保留背景特征。 TensorFlow描述池化 tf.keras.layers.MaxPool2D( pool_size=池化核尺寸， #正方形写核长整数，或（核高h，核宽w） strides=池化步长， #步长整数， 或(纵向步长h，横向步长w)，默认为pool_size padding=‘valid’or‘same’ #使用全零填充是“same”，不使用是“valid”（默认） ) tf.keras.layers.AveragePooling2D( pool_size=池化核尺寸， #正方形写核长整数，或（核高h，核宽w） strides=池化步长， #步长整数， 或(纵向步长h，横向步长w)，默认为pool_size padding=‘valid’or‘same’ #使用全零填充是“same”，不使用是“valid”（默认） ) ","date":"2020-06-13","objectID":"/convolutional1/:6:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"舍弃(Dropout) 为了缓解神经网络过拟合，在神经网络训练时，将隐藏层的部分神经元按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元恢复链接。 TensorFlow描述舍弃 tf.keras.layers.Dropout(舍弃的概率) ","date":"2020-06-13","objectID":"/convolutional1/:7:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"卷积神经网络 借助卷积核提取特征后，送入全连接网络。 卷积神经网络的主要模块： 卷积（Convolutional） 批标准化（BN） 激活（Activation） 池化（Pooling） 舍弃（Dropout） 全连接（FC） model = tf.keras.models.Sequential([ Conv2D(filters=6, kernel_size=(5, 5), padding='same'), #卷积层 BatchNormalization(), #BN层 Activation('relu'), #激活层 MaxPool2D(pool_size=(2, 2), strides=2, padding='same'), #池化层 Dropout(0.2), #dropout层 ]) ","date":"2020-06-13","objectID":"/convolutional1/:8:0","tags":["CNN","神经网络"],"title":"TensorFlow2.1入门学习笔记(12)——卷积神经网络","uri":"/convolutional1/"},{"categories":["TF2.1学习笔记"],"content":"以MNIST的sequential模型为base-line，通过读取自己的数据，训练模型并存储模型，最后达到绘图实物的运用。 ","date":"2020-06-04","objectID":"/selfdatatrain/:0:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"自制数据集，解决本领域应用 ","date":"2020-06-04","objectID":"/selfdatatrain/:1:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"观察数据结构 给x_train、y_train、x_test、y_test赋值 ","date":"2020-06-04","objectID":"/selfdatatrain/:1:1","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"def generateds(图片路径,标签文件)： def generateds(path, txt): f = open(txt, 'r') # 以只读形式打开txt文件 contents = f.readlines() # 读取文件中所有行 f.close() # 关闭txt文件 x, y_ = [], [] # 建立空列表 for content in contents: # 逐行取出 value = content.split() # 以空格分开，图片路径为value[0] , 标签为value[1] , 存入列表 img_path = path + value[0] # 拼出图片路径和文件名 img = Image.open(img_path) # 读入图片 img = np.array(img.convert('L')) # 图片变为8位宽灰度值的np.array格式 img = img / 255. # 数据归一化 （实现预处理） x.append(img) # 归一化后的数据，贴到列表x y_.append(value[1]) # 标签贴到列表y_ print('loading : ' + content) # 打印状态提示 x = np.array(x) # 变为np.array格式 y_ = np.array(y_) # 变为np.array格式 y_ = y_.astype(np.int64) # 变为64位整型 return x, y_ # 返回输入特征x，返回标签y_ ","date":"2020-06-04","objectID":"/selfdatatrain/:1:2","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"数据增强，扩充数据集 数据增强（增大数据量），可以简单的扩展数据集，对图像的数据增强就是对图像的简单形变。 tensorflow2中的数据增强函数 image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator( rescale = 所有数据将乘以该数值 rotation_range = 随机旋转角度数范围 width_shift_range = 随机宽度偏移量 height_shift_range = 随机高度偏移量 水平翻转：horizontal_flip = 是否随机水平翻转 随机缩放：zoom_range = 随机缩放的范围 [1-n，1+n] ) image_gen_train.fit(x_train) ### 例 ### image_gen_train = ImageDataGenerator( rescale=1. / 1., # 如为图像，分母为255时，可归至0～1 rotation_range=45, # 随机45度旋转 width_shift_range=.15, # 宽度偏移 height_shift_range=.15, # 高度偏移 horizontal_flip=False, # 水平翻转 zoom_range=0.5 # 将图像随机缩放阈量50％) image_gen_train.fit(x_train) 其中image_gen_train.fit(x_train)中的fit需要一个四维数组 即： x_train = x_train.reshape(x_train[0], 28, 28, 1) (60000, 28, 28) $\\Rightarrow$ (60000, 28, 28, 1) 将60000张28行28列的数据转换成60000张28行28列单通道的数据集，其中“1”是灰度值 model.fit()同步更新为model.fit(image_gen_train.flow(x_train, y_train,batch_size=32), ……) model.fit(x_train, y_train,batch_size=32, ……) $$\\Downarrow$$ model.fit(image_gen_train.flow(x_train, y_train,batch_size=32), ……) 加入数据增强的的代码训练后 import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # 给数据增加一个维度,从(60000, 28, 28)reshape为(60000, 28, 28, 1) image_gen_train = ImageDataGenerator( rescale=1. / 1., # 如为图像，分母为255时，可归至0～1 rotation_range=45, # 随机45度旋转 width_shift_range=.15, # 宽度偏移 height_shift_range=.15, # 高度偏移 horizontal_flip=False, # 水平翻转 zoom_range=0.5 # 将图像随机缩放阈量50％ ) image_gen_train.fit(x_train) model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) model.fit(image_gen_train.flow(x_train, y_train, batch_size=32), epochs=5, validation_data=(x_test, y_test), validation_freq=1) model.summary() 随着模型迭代轮数的增加，模型的准确率不断提高 数据在小数据量上可以增加模型的泛化性 ","date":"2020-06-04","objectID":"/selfdatatrain/:2:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"断点续训，存取模型 ","date":"2020-06-04","objectID":"/selfdatatrain/:3:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"读取保存模型 load_weights(路径文件名) checkpoint_save_path = \"./checkpoint/fashion.ckpt\" #先定义出存放模型的路径和文件名，“.ckpt”文件在生成时会同步生成索引表 if os.path.exists(checkpoint_save_path + '.index'): #判断是否有索引表，就可以知道是否报存过模型，如果有索引表，就会调用load_weights()即模型 print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) ","date":"2020-06-04","objectID":"/selfdatatrain/:3:1","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"保存模型 使用tensorflow给出的回调函数直接保存训练的参数： tf.keras.callbacks.ModelCheckpoint(filepath=路径文件名,save_weights_only=True/False,save_best_only=True/False) history = model.fit（ callbacks=[cp_callback] ） cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, # 文件存储路径 save_weights_only=True, # 是否只保留模型参数 save_best_only=True) # 是否只保留模型最优参数 history = model.fit(x_train, y_train, batch_size=32, epochs=5, # 加入callbacks选项，记录到history中 validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) 加入断点续训的完整代码： import tensorflow as tf import os # 引入os模块，（文件处理） mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/fashion.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() 第一次运行： 第二次运行： 加载了第一次保存的参数，准确率在第一次的基础上提高 ","date":"2020-06-04","objectID":"/selfdatatrain/:3:2","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"参数提取，把参数存入文本 提取可训练参数 model.trainable_variables 返回模型中可训练的参数 设置print输出格式 np.set_printoptions(threshold=超过多少省略显示) np.set_printoptions(threshold=np.inf) # np.inf表示无限大 将可训练参数存入文本 print(model.trainable_variables) file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() 在断点续训的基础上加入参数提取 import tensorflow as tf import os import numpy as np np.set_printoptions(threshold=np.inf) mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/fashion.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() print(model.trainable_variables) file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() 运行得到weights.txt文件 ","date":"2020-06-04","objectID":"/selfdatatrain/:4:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"acc/loss可视化，查看训练效果 将训练过程可视化出来 在history中同步记录了训练集loss、测试机loss、训练集准确率和测试集准确率 history=model.fit(训练集数据, 训练集标签, batch_size=, epochs=,validation_split=用作测试数据的比例,validation_data=测试集,validation_freq=测试频率) history 训练集loss： loss 测试集loss： val_loss 训练集准确率： sparse_categorical_accuracy 测试集准确率： val_sparse_categorical_accuracy acc = history.history['sparse_categorical_accuracy'] val_acc = history.history['val_sparse_categorical_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] #使用history.histor[]提取 加入绘制图像的代码： import tensorflow as tf import os import numpy as np from matplotlib import pyplot as plt # 导入绘图模块 np.set_printoptions(threshold=np.inf) mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['sparse_categorical_accuracy']) checkpoint_save_path = \"./checkpoint/fashion.ckpt\" if os.path.exists(checkpoint_save_path + '.index'): print('-------------load the model-----------------') model.load_weights(checkpoint_save_path) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path, save_weights_only=True, save_best_only=True) history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1, callbacks=[cp_callback]) model.summary() print(model.trainable_variables) file = open('./weights.txt', 'w') for v in model.trainable_variables: file.write(str(v.name) + '\\n') file.write(str(v.shape) + '\\n') file.write(str(v.numpy()) + '\\n') file.close() ############################################### show ############################################### # 显示训练集和验证集的acc和loss曲线 acc = history.history['sparse_categorical_accuracy'] val_acc = history.history['val_sparse_categorical_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] plt.subplot(1, 2, 1) plt.plot(acc, label='Training Accuracy') plt.plot(val_acc, label='Validation Accuracy') plt.title('Training and Validation Accuracy') plt.legend() plt.subplot(1, 2, 2) plt.plot(loss, label='Training Loss') plt.plot(val_loss, label='Validation Loss') plt.title('Training and Validation Loss') plt.legend() plt.show() 可视化结果： ","date":"2020-06-04","objectID":"/selfdatatrain/:5:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"应用程序，给图识物 前面已经将模型训练好了，下面将编写一套运用程序实现给图识物 predict(输入特征，batch_size=整数) 返回前向传播计算结果 前向传播执行应用： 复现模型（前向传播） 加载参数：model.load_weights(model_save_path) 预测结果：result = model.predict(x_predict) 源码： from PIL import Image import numpy as np import tensorflow as tf model_save_path = './checkpoint/mnist.ckpt' model = tf.keras.models.Sequential([ # 复现网络 tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(10, activation='softmax')]) model.load_weights(model_save_path) # 加载参数 preNum = int(input(\"input the number of test pictures:\")) # 准备预测多少个数 for i in range(preNum): # 读入待识别的图片 image_path = input(\"the path of test picture:\") img = Image.open(image_path) img = img.resize((28, 28), Image.ANTIALIAS) # 转换成（28，28）的类型，与训练数据类型匹配 img_arr = np.array(img.convert('L')) # 转换成灰度图 img_arr = 255 - img_arr # 将“白底黑字”反转成“黑底白字” #####or##### # for i in range(28): # 转换成高对比度的图，过滤噪声 # for j in range(28): # if img_arr[i][j] \u003c 200: # img_arr[i][j] = 255 # else: # img_arr[i][j] = 0 img_arr = img_arr / 255.0 # 归一化 print(\"img_arr:\",img_arr.shape) x_predict = img_arr[tf.newaxis, ...] # 由于是按每个batch送入网络，故添加一个维度 print(\"x_predict:\",x_predict.shape) result = model.predict(x_predict) #预测结果 pred = tf.argmax(result, axis=1) print('\\n') tf.print(pred) ","date":"2020-06-04","objectID":"/selfdatatrain/:6:0","tags":["datasets","mnist"],"title":"TensorFlow2.1入门学习笔记(11)——自制数据集，并记录训练模型","uri":"/selfdatatrain/"},{"categories":["TF2.1学习笔记"],"content":"前面已经使用TensorFlow2的原生代码搭建神经网络，接下来将使用keras搭建神经网络，并改写鸢尾花分类问题的代码，将原本100多行的代码用不到20行代码实现。 ","date":"2020-05-31","objectID":"/tf_keras_mnist/:0:0","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"用TensorFlow API：tf.keras搭建网络 ","date":"2020-05-31","objectID":"/tf_keras_mnist/:1:0","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"使用Sequential 六步法： import，相关模块 train, test，指定训练集的输入特征，和训练集的标签 model = tf.keras.models.Sequential，搭建网络结构,（顺序神经网络） model.compile，配置训练方法 model.fit，执行训练 model.summary，打印出网络结构和参数统计 model = tf.keras.models.Sequential([网络结构]) 描述各层网络： 拉直层：tf.keras.layers.Flatten()，将输入特征拉直 全连接层：tf.keras.layers.Dense(神经元个数，activation=“激活函数”，kernel_regularizer=哪种正则化) activation（字符串给出）可选: relu、 softmax、 sigmoid 、 tanh kernel_regularizer可选: tf.keras.regularizers.l1()、tf.keras.regularizers.l2() 卷积层： tf.keras.layers.Conv2D(filters = 卷积核个数, kernel_size = 卷积核尺寸, strides = 卷积步长， padding = \" valid” or “same”) LSTM层： tf.keras.layers.LSTM() model.compile(optimizer = 优化器, loss = 损失函数, metrics = [“准确率”] ) Optimizer可选: ‘sgd’ or tf.keras.optimizers.SGD (lr=学习率,momentum=动量参数) ‘adagrad’ or tf.keras.optimizers.Adagrad (lr=学习率) ‘adadelta’ or tf.keras.optimizers.Adadelta (lr=学习率) ‘adam’ or tf.keras.optimizers.Adam (lr=学习率, beta_1=0.9, beta_2=0.999) loss可选: ‘mse’ or tf.keras.losses.MeanSquaredError() ‘sparse_categorical_crossentropy’ or tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) Metrics可选: ‘accuracy’ ：y_和y都是数值，如y_=[1] y=[1] ‘categorical_accuracy’ ：y_和y都是独热码(概率分布)，如y_=[0,1,0] y=[0.256,0.695,0.048] ‘sparse_categorical_accuracy’ ：y_是数值，y是独热码(概率分布),如y_=[1] y=[0.256,0.695,0.048] model.fit ()执行训练过程 model.fit (训练集的输入特征, 训练集的标签, batch_size= , epochs= , validation_data=(测试集的输入特征，测试集的标签), validation_split=从训练集划分多少比例给测试集, validation_freq = 多少次epoch测试一次) model.summary（） 打印网络的结构和参数统计 例如鸢尾花分类问题 鸢尾花问题使用六步法复现 # 1.import\r import tensorflow as tf\rfrom sklearn import datasets\rimport numpy as np\r# train,test\r x_train = datasets.load_iris().data\ry_train = datasets.load_iris().target\rnp.random.seed(116)\rnp.random.shuffle(x_train)\rnp.random.seed(116)\rnp.random.shuffle(y_train)\rtf.random.set_seed(116)\r# 3.model.Sequential\r model = tf.keras.models.Sequential([\rtf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())\r])\r# 4.model.compile\r model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\rloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\rmetrics=['sparse_categorical_accuracy'])\r# 5.model.fit\r model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)\r# 6.model.summary\r model.summary()\r ","date":"2020-05-31","objectID":"/tf_keras_mnist/:1:1","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"使用class类 六步法： import，相关模块 train, test，指定训练集的输入特征，和训练集的标签 class MyModel(Model) model=MyModel,（Sequential无法写出带有跳连的非顺序神经网络） model.compile，配置训练方法 model.fit，执行训练 model.summary，打印出网络结构和参数统计 使用class类封装一个神经网络结构 _init_( ) 定义所需网络结构块 call( ) 写出前向传播 ###############################\r class MyModel(Model):\rdef __init__(self):\rsuper(MyModel, self).__init__()\r定义网络结构块\rdef call(self, x):\r调用网络结构块，实现前向传播\rreturn y\rmodel = MyModel()\r###############################\r class IrisModel(Model):\rdef __init__(self):\rsuper(IrisModel, self).__init__()\rself.d1 = Dense(3)\rdef call(self, x):\ry = self.d1(x)\rreturn y\rmodel = IrisModel()\r 鸢尾花问题使用六步法复现 # 1.import\r import tensorflow as tf\rfrom sklearn import datasets\r######\r from tensorflow.keras.layers import Dense\rfrom tensorflow.keras import Model\r######\r import numpy as np\r# train,test\r x_train = datasets.load_iris().data\ry_train = datasets.load_iris().target\rnp.random.seed(116)\rnp.random.shuffle(x_train)\rnp.random.seed(116)\rnp.random.shuffle(y_train)\rtf.random.set_seed(116)\r###### 3.class MyModel ######\r class IrisModel(Model):\rdef __init__(self):\rsuper(IrisModel, self).__init__()\rself.d1 = Dense(3, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2())\rdef call(self, x):\ry = self.d1(x)\rreturn y\rmodel = IrisModel()\r# 4.model.compile\r model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\rloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\rmetrics=['sparse_categorical_accuracy'])\r# 5.model.fit\r model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)\r# 6.model.summary\r model.summary()\r 打印结果： ","date":"2020-05-31","objectID":"/tf_keras_mnist/:1:2","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"MNIST数据集： ","date":"2020-05-31","objectID":"/tf_keras_mnist/:2:0","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"介绍 Yann LeCun 提供 6万张 28*28 像素点的0~9手写数字图片和标签，用于训练。 提供 1万张 28*28 像素点的0~9手写数字图片和标签，用于测试。 导入MNIST数据集： mnist = tf.keras.datasets.mnist\r(x_train, y_train) , (x_test, y_test) = mnist.load_data()\r 数据处理 作为输入特征，输入神经网络时，将数据拉伸为一维数组： tf.keras.layers.Flatten( ) [0 0 0 48 238 252 252 …… …… …… 253 186 12 0 0 0 0 0] 查看数据集 plt.imshow(x_train[0], cmap='gray')#绘制灰度图\r plt.show()\r print(\"x_train[0]:\\n\", x_train[0])\r print(\"y_train[0]:\", y_train[0])\r # 打印出整个训练集输入特征形状\r print(\"x_train.shape:\\n\", x_train.shape)\r# 打印出整个训练集标签的形状\r print(\"y_train.shape:\\n\", y_train.shape)\r# 打印出整个测试集输入特征的形状\r print(\"x_test.shape:\\n\", x_test.shape)\r# 打印出整个测试集标签的形状\r print(\"y_test.shape:\\n\", y_test.shape)\r ","date":"2020-05-31","objectID":"/tf_keras_mnist/:2:1","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"使用Sequential实现手写字体识别 import tensorflow as tf\rmnist = tf.keras.datasets.mnist\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_train, x_test = x_train / 255.0, x_test / 255.0\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Flatten(),\rtf.keras.layers.Dense(128, activation='relu'),\rtf.keras.layers.Dense(10, activation='softmax')\r])\rmodel.compile(optimizer='adam',\rloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\rmetrics=['sparse_categorical_accuracy'])\rmodel.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)\rmodel.summary()\r ","date":"2020-05-31","objectID":"/tf_keras_mnist/:2:2","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"使用class MyModel实现手写字体识别 import tensorflow as tf\rfrom tensorflow.keras.layers import Dense, Flatten\rfrom tensorflow.keras import Model\rmnist = tf.keras.datasets.mnist\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_train, x_test = x_train / 255.0, x_test / 255.0\rclass MnistModel(Model):\rdef __init__(self):\rsuper(MnistModel, self).__init__()\rself.flatten = Flatten()\rself.d1 = Dense(128, activation='relu')\rself.d2 = Dense(10, activation='softmax')\rdef call(self, x):\rx = self.flatten(x)\rx = self.d1(x)\ry = self.d2(x)\rreturn y\rmodel = MnistModel()\rmodel.compile(optimizer='adam',\rloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\rmetrics=['sparse_categorical_accuracy'])\rmodel.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test), validation_freq=1)\rmodel.summary()\r 更多分享： ","date":"2020-05-31","objectID":"/tf_keras_mnist/:2:3","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"FASHINO数据集 提供 6万张 28*28 像素点的衣裤等图片和标签，用于训练。 提供 1万张 28*28 像素点的衣裤等图片和标签，用于测试。 导入数据集 fashion = tf.keras.datasets.fashion_mnist\r(x_train, y_train),(x_test, y_test) = fashion.load_data()\r ","date":"2020-05-31","objectID":"/tf_keras_mnist/:3:0","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"使用Sequential实现手写字体识别 import tensorflow as tf\rfashion = tf.keras.datasets.fashion_mnist\r(x_train, y_train),(x_test, y_test) = fashion.load_data()\rx_train, x_test = x_train/255.0, x_test/255.0\rmodel=tf.keras.models.Sequential([\rtf.keras.layers.Flatten(),\rtf.keras.layers.Dense(128,activation=\"relu\"),\rtf.keras.layers.Dense(10,activation=\"softmax\")\r])\rmodel.compile(optimizer=\"adam\",\rloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\rmetrics = ['sparse_categorical_accuracy'])\rmodel.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test,y_test), validation_freq=1)\rmodel.summary()\r ","date":"2020-05-31","objectID":"/tf_keras_mnist/:3:1","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["TF2.1学习笔记"],"content":"使用class MyModel实现手写字体识别 import tensorflow as tf\rfrom tensorflow.keras.layers import Dense,Flatten\rfrom tensorflow.keras import Model\rfashion=tf.keras.datasets.fashion_mnist\r(x_train,y_train),(x_test, y_test)=fashion.load_data()\rx_train, x_test=x_train/255.0,x_test/255.0\rclass FashionModel(Model):\rdef __init__(self):\rsuper(FashionModel, self).__init__()\rself.flatten=Flatten()\rself.d1=Dense(128,activation=\"relu\")\rself.d2=Dense(10,activation=\"softmax\")\rdef call(self,x):\rx=self.flatten(x)\rx=self.d1(x)\ry=self.d2(x)\rreturn y\rmodel = FashionModel()\rmodel.compile(optimizer=\"adam\",\rloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\rmetrics=[\"sparse_categorical_accuracy\"])\rmodel.fit(x_train,y_train,batch_size=32,epochs=5,validation_data=(x_test,y_test),validation_freq=1)\rmodel.summary()\r ","date":"2020-05-31","objectID":"/tf_keras_mnist/:3:2","tags":["keras","MNIST","FASHION","神经网络"],"title":"TensorFlow2.1入门学习笔记(10)——使用keras搭建神经网络(Mnist,Fashion)","uri":"/tf_keras_mnist/"},{"categories":["热力学"],"content":"热力学第一定律 ","date":"2020-05-30","objectID":"/thermodynamic1/:1:0","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"内能，功和热量 实际气体内能：所有热分子热运动的动能和分子势能的总和 内能是状态量: $E=E(T,V)$ 理想气体内能: $E={\\frac{M}{M_{mol}}{\\frac{i}{2}}RT}$ 是状态参量T的单值函数 系统内能改变的两种方式 做工可以改变系统的状态：摩擦升温（机械功），电加热（电功） 热量的传递可以改变系统的内能：热量是过程量 ","date":"2020-05-30","objectID":"/thermodynamic1/:1:1","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"准静态过程 $$ 热力学过程 = \\left\\{ \\begin{array}{lr} 准静态过程\\\\ 非静态过程 \\end{array} \\right. $$ 准静态过程：系统从一个平衡态到另一个平衡态，如果过程中所有的中间态都可以近似的看作平衡态法过程 准静态过程是理想化过程 弛豫时间$\\tau$: 系统从一个平衡态变道相邻平衡态所经过的时间 当$\\Delta t_{过程}»\\tau$: 过程就可以视为准静态过程，故 **无限缓慢** 只是一个相对的概念。 非静态过程: 系统从一平衡态到另一平衡态，过程中所有中间态为非静态的过程 准静态过程曲线 p-V图上，一个点代表一个平衡态，一条连续的曲线代表一个准静态过程 ","date":"2020-05-30","objectID":"/thermodynamic1/:1:2","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"准静态过程的功与热 体积功： 当活塞移动微小位移$dl$时，系统外界所做的元功为： $$dA = Fdl = pSdl = pdV$$ $$A=\\begin{aligned}\\int_{V_{1}}^{V_{2}} p \\mathrm{d} V\\end{aligned}$$ $dV\u003e0,dA\u003e0$系统对外界做正功 $dV\u003c0,dA\u003c0$系统对外界做负功 $dV=0,dA=0$系统不做功 功是过程量 做功改变系统热力学状态的微观实质 -功是系统与外界交换的能量的量度 准静态过程中的热量计算 $$C = \\frac{dQ}{dT}$$ C（热容量）：系统在某一无限小过程中吸收热量$dQ$与温度变化$dT$的比值 单位：$J\\cdot K^{-1}$ 热容量与比热的关系为：$C = Mc_{比}$ $C_m$（摩尔热容量）： $$C = {\\frac{M}{M_{mol}}}{C_{m}}$$ $$dQ = {\\frac{M}{M_{mol}}}{C_m}{dT}$$ $$Q = {\\frac{M}{M_{mol}}}{C_m}(T_2-T_1)$$ 传热的微观本质： 热量也是能量变化的量度 ","date":"2020-05-30","objectID":"/thermodynamic1/:1:3","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"热力学第一定律 对于任一过程，系统与外界可能同时有功和热量的转换，且系统能量改变仅为内能时，根据能量守恒： $$\\Delta E = Q + (-A)$$ 或$$Q = \\Delta E + A$$ $Q\u003e0$系统吸热，$Q\u003c0$系统放热 $A\u003e0$系统对外做功，$A\u003c0$外界对系统做功 $\\Delta E\u003e 0$系统内能增加，$\\Delta E\u003c0$系统内能减少 如果系统经历一些微小变化过程，则$dQ=dE+dA$； 对准静态过程： $$dQ=dE+pdV$$ $$Q=\\Delta E + {\\begin{aligned}{\\int_{V_{1}}^{V_{2}}}p{\\mathrm{d} V}\\end{aligned}}$$ ","date":"2020-05-30","objectID":"/thermodynamic1/:1:4","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"理想气体等值过程 ","date":"2020-05-30","objectID":"/thermodynamic1/:2:0","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"等容过程，定容摩尔热容 $$\\because dV=0,dA= pdV = 0$$ $$\\therefore dQ=dE={\\frac{M}{M_{mol}}}{\\frac{i}{2}}RdT$$ $$Q_V=E_2-E_1={\\frac{M}{M_{mol}}}{\\frac{i}{2}}Rd(T_2-T_1)$$ 定容摩尔热容量 $$dQ_V=dE={\\frac{i}{2}}RdT$$ $$C_V=({\\frac {dQ}{dT}})_V$$ $$C_{V,m}={\\frac{i}{2}}R$$ 单原子理想气体：$C_{V,m}={\\frac{3}{2}}R$ 双原子理想气体：$C_{V,m}={\\frac{5}{2}}R$ 多原子理想气体：$C_{V,m}=3R$ 理想气体内能 $$E={\\frac{M}{M_{mol}}}{C_{V,m}}T$$ 理想气体的任一$T_1\\rightarrow T_2$过程 $$dE=\\nu C_{V,m}dT$$ $$\\Delta E=E_2-E_1={\\nu}{\\begin{aligned}{\\int_{T_{1}}^{T_{2}}}{C_{V,m}}{\\mathrm{d} T}\\end{aligned}}$$ 若$C_{V,m}$近似为常数，则有$\\Delta E = \\nu C_{V,m}\\Delta T$ ","date":"2020-05-30","objectID":"/thermodynamic1/:2:1","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"等压过程，定压摩尔热容 $$dA=pdV$$ $$dQ_p=dE+pdV$$ $$A_p={\\begin{aligned}{\\int_{V_{1}}^{V_{2}}}p{\\mathrm{d} V}\\end{aligned}}=p(V_2-V_1)$$ $$Q_p={\\frac{M}{M_{mol}}}{\\frac{i}{2}}R(T_2-T_1)+{\\frac{M}{M_{mol}}}R(T_2-T_1)$$ 定压摩尔热容量 $$dQ_p=dE+dA_p=C_{V,m}dT+pdV$$ $$pV=RT微分得pdV=RdT$$ $$dQ_p={\\frac{i}{2}}R\\cdot dT+R\\cdot dT$$ $$C_{p,m}=(\\frac{dQ}{dT})_p$$ $$C_{p,m}={\\frac{i}{2}}R+R$$ $$C_{p,m}=C_{V,m}+R$$ $$Q_{p,m}={\\frac{M}{M_{mol}}}{C_{p,m}}(T_2-T_1)$$ 比热容比： $\\gamma =\\frac{C_{p,m}}{C_{V,m}}$为绝热系数 理想气体：$\\gamma =\\frac{C_{p,m}}{C_{V,m}}=\\frac{\\frac{i}{2}R+R}{\\frac{i}{2}R}=\\frac{i+2}{i}$ 对单原子分子：$i=3,\\gamma=1.67$ 对刚性双原子分子：$i=5,\\gamma=1.40$ 对刚性多原子分子：$i=6,\\gamma=1.33$ ","date":"2020-05-30","objectID":"/thermodynamic1/:2:2","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"等温过程 $dT=0,dE=0$ $dQ_T=dA_T$ $dQ_T=pdV,p=\\nu RT\\cdot \\frac{1}{V}$ $Q_T=A_T={\\begin{aligned}{\\int_{V_{1}}^{V_{2}}}\\nu RT{\\frac{dV}{V}}\\end{aligned}}=\\nu RTln{\\frac{V_2}{V_1}}=p_1 V_1 ln{\\frac{V_2}{V_1}}$ $\\Rightarrow Q_T = \\left\\{\\begin{array}{lr}p_1 V_1 ln{\\frac{p_1}{p_2}}=p_2 V_2 ln{\\frac{p_1}{p_2}}\\\\\\frac{M}{M_{mol}}RTln{\\frac{p_1}{p_2}}\\end{array}\\right.$ ","date":"2020-05-30","objectID":"/thermodynamic1/:2:3","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":["热力学"],"content":"绝热过程 系统变化过程中，系统与外界没有热交换 特征：$dQ=0,dE+dA=0$ 绝热方程 对于准静态方程 $\\nu C_{V,m}dT+pdV=0$ $pV=\\nu RT$ 取微分得 $pdV+Vdp=\\nu RdT$ 消去$\\nu dT$得 $pdV+Vdp=-R{\\frac{pdV}{C_{V,m}}}$ ${C_{V,m}}pdV+{C_{V,m}}Vdp=-RpdV$ ${C_{p,m}}pdV+{C_{V,m}}Vdp=0$ ${\\frac{dp}{p}}+\\gamma {\\frac{dV}{V}}=0$ 积分得 ${\\begin{aligned}\\int \\frac{dp}{p}\\end{aligned}}+{\\begin{aligned}\\int \\gamma \\frac{dV}{V}\\end{aligned}}=0$ 得 $lnp+\\gamma lnV=C$ $lnpV^\\gamma=C$ $pV^\\gamma=C_1$ $pV^{\\gamma-1}=C_2$ $p^{\\gamma-1}T^{-\\gamma}=C_3$,即松柏方程 绝热线与等温线 $pV=C_1,等温线$ $pV^\\gamma=C_2,绝热线$ 对于等温过程 $pV=C_1=p_A V_A$ $p=\\frac{C_1}{V}$ $\\frac{dp}{dV}|_{AT}=-\\frac{C_1}{V^2}|_A=-\\frac{C_1}{V_A ^2}=-\\frac{p_AV_A}{V_A ^2}=-\\frac{p_A}{V_A}$ 对于绝热过程 $pV^\\gamma=C_2=p_AV_A ^\\gamma$ $p=\\frac{C_2}{V^\\gamma}$ $\\frac{dp}{dV}|_{A\\gamma}=-\\gamma \\frac{C_2}{V^{\\gamma+1}}|_A=-\\gamma \\frac{p_AV_A ^\\gamma}{V_A ^{\\gamma+1}}=-\\gamma \\frac{p_A}{V_A}$ $\\because \\gamma \u003e 1$ $\\therefore |\\frac{dp}{dV}|_{A\\gamma}=\\gamma \\frac{p_A}{V_A}\u003e|\\frac{dp}{dV}|_{AT}=\\frac{p_A}{V_A}$ 即绝热线要陡一些 $p=nkT$ 绝热过程中功值计算 ","date":"2020-05-30","objectID":"/thermodynamic1/:2:4","tags":["等容","等温","等压","体积功"],"title":"热力学第一定律 等值过程 绝热过程","uri":"/thermodynamic1/"},{"categories":null,"content":" -- -- 站名: 一颗小木 首页: xmoon.info 描述: 努力的把不会的变成「会的」 主题: LoveIt 框架: Hugo 小工具: 陈YF的工具箱 ","date":"2020-05-22","objectID":"/music/:0:0","tags":null,"title":"每日歌单","uri":"/music/"},{"categories":["TF2.1学习笔记"],"content":"神经网络是基于链接的人工智能，当网络结构固定后，不同参数的选取对模型的表达力影响很大，优化器就是引导更新模型参数的工具 ","date":"2020-05-21","objectID":"/optimizer/:0:0","tags":["optimizer"],"title":"TensorFlow2.1入门学习笔记(9)——神经网络参数优化器(优化器性能比较)","uri":"/optimizer/"},{"categories":["TF2.1学习笔记"],"content":"常用符号 待优化参数w 损失函数loss 学习率lr 每次迭代一个batch（以batch为单位批量喂入神经网络，batch常为$2^n$） t表示当前batch迭代的总次数 ","date":"2020-05-21","objectID":"/optimizer/:0:1","tags":["optimizer"],"title":"TensorFlow2.1入门学习笔记(9)——神经网络参数优化器(优化器性能比较)","uri":"/optimizer/"},{"categories":["TF2.1学习笔记"],"content":"更新参数的过程 计算t时刻损失函数关于当前参数的梯度$g_t=\\nabla loss=\\frac{\\partial loss}{\\partial(w_t)}$ 计算t时刻一阶动量$m_t$和二阶动量$V_t$ 计算t时刻下降梯度：$\\eta_t=lr*{\\frac {m_t}{\\sqrt V_t}}$ 计算t+1时刻参数：$w_{t+1}=w_t-\\eta_t=w_t-lr*{\\frac {m_t}{\\sqrt V_t}}$ 一阶动量：与梯度相关的函数 二阶动量：与梯度平方相关的函数 不同的优化器实质上是定义了不同的一阶动量和二阶动量公式 ","date":"2020-05-21","objectID":"/optimizer/:0:2","tags":["optimizer"],"title":"TensorFlow2.1入门学习笔记(9)——神经网络参数优化器(优化器性能比较)","uri":"/optimizer/"},{"categories":["TF2.1学习笔记"],"content":"五种常见优化器 使用鸢尾花分类问题代码检测五种优化器性能。 SGD（无moment）：随机梯度下降 $m_t=g_t$ $V_t=1$ $\\eta_t=lr*{\\frac {m_t}{\\sqrt V_t}}=lr*g_t$ $w_{t+1}=w_t-\\eta_t=w_t-lr*{\\frac {m_t}{\\sqrt V_t}}=w_t-lr*g_t$ ${\\Rightarrow \\boxed{w_{t+1}=w_t-lr*{\\frac {\\partial loss}{\\partial w_t}}}}$ 代码实现： # sgd w1.assign_sub(lr*grads[0]) # 参数w1自更新 b1.assign_sub(lr*grads[1]) # 参数b1自更新 例： # 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线 # 导入所需模块 import tensorflow as tf from sklearn import datasets from matplotlib import pyplot as plt import numpy as np import time ##1## # 导入数据，分别为输入特征和标签 x_data = datasets.load_iris().data y_data = datasets.load_iris().target # 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率） # seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致） np.random.seed(116) # 使用相同的seed，保证输入特征和标签一一对应 np.random.shuffle(x_data) np.random.seed(116) np.random.shuffle(y_data) tf.random.set_seed(116) # 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行 x_train = x_data[:-30] y_train = y_data[:-30] x_test = x_data[-30:] y_test = y_data[-30:] # 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错 x_train = tf.cast(x_train, tf.float32) x_test = tf.cast(x_test, tf.float32) # from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据） train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) # 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元 # 用tf.Variable()标记参数可训练 # 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed） w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1)) b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1)) lr = 0.1 # 学习率为0.1 train_loss_results = [] # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据 test_acc = [] # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据 epoch = 500 # 循环500轮 loss_all = 0 # 每轮分4个step，loss_all记录四个step生成的4个loss的和 # 训练部分 now_time = time.time() ##2## for epoch in range(epoch): # 数据集级别的循环，每个epoch循环一次数据集 for step, (x_train, y_train) in enumerate(train_db): # batch级别的循环 ，每个step循环一个batch with tf.GradientTape() as tape: # with结构记录梯度信息 y = tf.matmul(x_train, w1) + b1 # 神经网络乘加运算 y = tf.nn.softmax(y) # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss） y_ = tf.one_hot(y_train, depth=3) # 将标签值转换为独热码格式，方便计算loss和accuracy loss = tf.reduce_mean(tf.square(y_ - y)) # 采用均方误差损失函数mse = mean(sum(y-out)^2) loss_all += loss.numpy() # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确 # 计算loss对各个参数的梯度 grads = tape.gradient(loss, [w1, b1]) # 实现梯度更新 w1 = w1 - lr * w1_grad b = b - lr * b_grad w1.assign_sub(lr * grads[0]) # 参数w1自更新 b1.assign_sub(lr * grads[1]) # 参数b自更新 # 每个epoch，打印loss信息 print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4)) train_loss_results.append(loss_all / 4) # 将4个step的loss求平均记录在此变量中 loss_all = 0 # loss_all归零，为记录下一个epoch的loss做准备 # 测试部分 # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0 total_correct, total_number = 0, 0 for x_test, y_test in test_db: # 使用更新后的参数进行预测 y = tf.matmul(x_test, w1) + b1 y = tf.nn.softmax(y) pred = tf.argmax(y, axis=1) # 返回y中最大值的索引，即预测的分类 # 将pred转换为y_test的数据类型 pred = tf.cast(pred, dtype=y_test.dtype) # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型 correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32) # 将每个batch的correct数加起来 correct = tf.reduce_sum(correct) # 将所有batch中的correct数加起来 total_correct += int(correct) # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数 total_number += x_test.shape[0] # 总的准确率等于total_correct/total_number acc = total_correct / total_number test_acc.append(acc) print(\"Test_acc:\", acc) print(\"--------------------------\") total_time = time.time() - now_time ##3## print(\"total_time\", total_time) ##4## # 绘制 loss 曲线 plt.title('Loss Function Curve') # 图片标题 plt.xlabel('Epoch') # x轴变量名称 plt.ylabel('Loss') # y轴变量名称 plt.plot(train_loss_results, label=\"$Loss$\") # 逐点画出trian_loss_results值并连线，连线图标是Loss plt.legend() # 画出曲线图标 plt.show() # 画出图像 # 绘制 Accuracy 曲线 plt.title('Acc Curve') # 图片标题 plt.xlabel('Epoch') # x轴变量名称 plt.ylabel('Acc') # y轴变量名称 plt.plot(test_acc, label=\"$Accuracy$\") # 逐点画出test_acc值并连线，连线图标是Accuracy plt.legend() plt.show() SGDM（含moment的SGD），在SGD基础上增加了一阶动量 $m_{t}$表示各个时刻梯度方向的指数滑动平均值 $m_{t-1}$表示上一时刻的一阶动量 $\\beta$是","date":"2020-05-21","objectID":"/optimizer/:0:3","tags":["optimizer"],"title":"TensorFlow2.1入门学习笔记(9)——神经网络参数优化器(优化器性能比较)","uri":"/optimizer/"},{"categories":["TF2.1学习笔记"],"content":"欠拟合与过拟合 欠拟合：模型不能有效拟合数据集对现有数据集学习的不够彻底 过拟合：模型对训练集拟合的太好，而缺失了泛化力 欠拟合的解决方法： 增加输入特征项 增加网络参数 减少正则化参数 过拟合的解决方法： 数据清洗 增大训练集 采用正则化 增大正则化参数 正则化缓解过拟合 正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练 数据的噪声（一般不正则化b） 常见的正则化： $loss_{l_1}(w)=\\sum_i |w_i|$ $loss_{l_1}(w)=\\sum_i |w_i ^2|$ 正则化的选择： L1正则化大概率会使很多参数变为零，因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度。 L2正则化会使参数很接近零但不为零，因此该方法可通过减小参数值的大小降低复杂度。 例如：给出一个dot.csv数据集，判断y_是1的可能性大还是0的可能性大。 思路：可以将x1、x2分别作为横纵坐标，y_为1的点标为红色，为0的点标为蓝色，在坐标系中展示出来，通过神经网络离合出分界线模型。 采用l2正则化的源码： # 导入所需模块 import tensorflow as tf from matplotlib import pyplot as plt import numpy as np import pandas as pd # 读入数据/标签 生成x_train y_train df = pd.read_csv('dot.csv') x_data = np.array(df[['x1', 'x2']]) y_data = np.array(df['y_c']) x_train = x_data y_train = y_data.reshape(-1, 1) Y_c = [['red' if y else 'blue'] for y in y_train] # 转换x的数据类型，否则后面矩阵相乘时会因数据类型问题报错 x_train = tf.cast(x_train, tf.float32) y_train = tf.cast(y_train, tf.float32) # from_tensor_slices函数切分传入的张量的第一个维度，生成相应的数据集，使输入特征和标签值一一对应 train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) # 生成神经网络的参数，输入层为4个神经元，隐藏层为32个神经元，2层隐藏层，输出层为3个神经元 # 用tf.Variable()保证参数可训练 w1 = tf.Variable(tf.random.normal([2, 11]), dtype=tf.float32) b1 = tf.Variable(tf.constant(0.01, shape=[11])) w2 = tf.Variable(tf.random.normal([11, 1]), dtype=tf.float32) b2 = tf.Variable(tf.constant(0.01, shape=[1])) lr = 0.005 # 学习率为 epoch = 800 # 循环轮数 # 训练部分 for epoch in range(epoch): for step, (x_train, y_train) in enumerate(train_db): with tf.GradientTape() as tape: # 记录梯度信息 h1 = tf.matmul(x_train, w1) + b1 # 记录神经网络乘加运算 h1 = tf.nn.relu(h1) y = tf.matmul(h1, w2) + b2 # 采用均方误差损失函数mse = mean(sum(y-out)^2) loss_mse = tf.reduce_mean(tf.square(y_train - y)) # 添加l2正则化 loss_regularization = [] # tf.nn.l2_loss(w)=sum(w ** 2) / 2 loss_regularization.append(tf.nn.l2_loss(w1)) #对w1使用l2正则化处理 loss_regularization.append(tf.nn.l2_loss(w2)) # 求和 # 例：x=tf.constant(([1,1,1],[1,1,1])) # tf.reduce_sum(x) # \u003e\u003e\u003e6 loss_regularization = tf.reduce_sum(loss_regularization) loss = loss_mse + 0.03 * loss_regularization # REGULARIZER = 0.03 # 计算loss对各个参数的梯度 variables = [w1, b1, w2, b2] grads = tape.gradient(loss, variables) # 实现梯度更新 # w1 = w1 - lr * w1_grad w1.assign_sub(lr * grads[0]) b1.assign_sub(lr * grads[1]) w2.assign_sub(lr * grads[2]) b2.assign_sub(lr * grads[3]) # 每200个epoch，打印loss信息 if epoch % 20 == 0: print('epoch:', epoch, 'loss:', float(loss)) # 预测部分 print(\"*******predict*******\") # xx在-3到3之间以步长为0.01，yy在-3到3之间以步长0.01,生成间隔数值点 xx, yy = np.mgrid[-3:3:.1, -3:3:.1] # 将xx, yy拉直，并合并配对为二维张量，生成二维坐标点 grid = np.c_[xx.ravel(), yy.ravel()] grid = tf.cast(grid, tf.float32) # 将网格坐标点喂入神经网络，进行预测，probs为输出 probs = [] for x_predict in grid: # 使用训练好的参数进行预测 h1 = tf.matmul([x_predict], w1) + b1 h1 = tf.nn.relu(h1) y = tf.matmul(h1, w2) + b2 # y为预测结果 probs.append(y) # 取第0列给x1，取第1列给x2 x1 = x_data[:, 0] x2 = x_data[:, 1] # probs的shape调整成xx的样子 probs = np.array(probs).reshape(xx.shape) plt.scatter(x1, x2, color=np.squeeze(Y_c)) # 把坐标xx yy和对应的值probs放入contour函数，给probs值为0.5的所有点上色 plt.show()后 显示的是红蓝点的分界线 plt.contour(xx, yy, probs, levels=[.5]) plt.show() # 读入红蓝点，画出分割线，包含正则化 学习结果 相比于不使用正则化，分界线更平滑有效缓解了过拟合，泛化性更好 主要学习的资料，西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记 ","date":"2020-05-21","objectID":"/regularization/:0:1","tags":["regularization"],"title":"TensorFlow2.1入门学习笔记(8)——欠拟合与过拟合(正则化)","uri":"/regularization/"},{"categories":["TF2.1学习笔记"],"content":"损失函数（loss）： 预测值（y）与已知答案（y_）的差距 神经网络的优化目标： loss最小: $\\Rightarrow\\left\\{\\begin{array}{lr}{mse(Mean Aquared Error)}\\\\{自定义}\\\\{ce(Cross Entropy)}\\end{array}\\right.$ 均方误差mse：loss_mse = tf.reduce_mean(tf.square(y_-y)) $$MSE(y_,y)=\\frac{\\sum_{i=1}^{n}(y-y_)^2}{n}$$ 例 预测酸奶日销量y，x1、x2是影响日销量的因素。 建模前，应预先采集的数据有：每日x1、x2和销量y_(即已知答案，最佳的情况：产量=销量) 拟造数据集X，Y：y_=x1+x2 噪声：-0.05~+0.05 拟合可以预算销量的函数 import tensorflow as tf import numpy as np SEED = 23455 rdm = np.random.RandomState(seed=SEED) # 生成[0,1)之间的随机数 x = rdm.rand(32, 2) y_ = [[x1 + x2 + (rdm.rand() / 10.0 - 0.05)] for (x1, x2) in x] # 生成噪声[0,1)/10=[0,0.1); [0,0.1)-0.05=[-0.05,0.05) x = tf.cast(x, dtype=tf.float32) w1 = tf.Variable(tf.random.normal([2, 1], stddev=1, seed=1)) epoch = 15000 lr = 0.002 for epoch in range(epoch): with tf.GradientTape() as tape: y = tf.matmul(x, w1) loss_mse = tf.reduce_mean(tf.square(y_ - y)) grads = tape.gradient(loss_mse, w1) w1.assign_sub(lr * grads) if epoch % 500 == 0: print(\"After %dtraining steps,w1 is \" % (epoch)) print(w1.numpy(), \"\\n\") print(\"Final w1 is: \", w1.numpy()) 运行结果 自定义损失函数： 如预测产品销量，预测多了，损失成本；预测少了，损失利润。若利润$\\neq$成本，则mse产生的loss无法利益最大化。 自定义损失函数： $lossy_,y=\\sum_{n}f(y_,y)$ $$ f(x) = \\left\\{ \\begin{array}{lr} PROFIT*(y\\_-y) \u0026 : y loss_zdy=tf.reduce_sum(tf.where(tf.greater(y,y_),COST(y-y_),PROFIT(y_-y))) 如：预测酸奶销量，酸奶成本（COST）1元，酸奶利润（PROFIT）99元 预测少了损失利润99元，预测多了损失1元，希望生成的预测函数往多了预测 import tensorflow as tf import numpy as np SEED = 23455 COST = 1 PROFIT = 99 rdm = np.random.RandomState(SEED) x = rdm.rand(32, 2) y_ = [[x1 + x2 + (rdm.rand() / 10.0 - 0.05)] for (x1, x2) in x] # 生成噪声[0,1)/10=[0,0.1); [0,0.1)-0.05=[-0.05,0.05) x = tf.cast(x, dtype=tf.float32) w1 = tf.Variable(tf.random.normal([2, 1], stddev=1, seed=1)) epoch = 10000 lr = 0.002 for epoch in range(epoch): with tf.GradientTape() as tape: y = tf.matmul(x, w1) loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * COST, (y_ - y) * PROFIT)) grads = tape.gradient(loss, w1) w1.assign_sub(lr * grads) if epoch % 500 == 0: print(\"After %dtraining steps,w1 is \" % (epoch)) print(w1.numpy(), \"\\n\") print(\"Final w1 is: \", w1.numpy()) 运行结果 交叉熵损失函数：tf.losses.categorical_crossentropy(y_,y) $$H(y_,y)=-\\sum y_*lny$$ CE(Cross Entropy):表示两个概率分布之间的距离 eg.二分类 已知答案y_=(1,0) 预测$y_1$=(0.6,0.4) $y_2$=(0.8,0.2)哪个更接近标准答案 $H_1((1,0),(0.6,0.4))=-(1 * ln0.6 + 0 * ln0.4) \\approx -(-0.511 + 0) = 0.511$ $H_2((1,0),(0.8,0.2))=-(1 * ln0.8 + 0 * ln0.2) \\approx -(-0.223 + 0) = 0.223$ 因为$H_1\u003eH_2$,所以$y_2$预测更准确 import tensorflow as tf loss_ce1 = tf.losses.categorical_crossentropy([1, 0], [0.6, 0.4]) loss_ce2 = tf.losses.categorical_crossentropy([1, 0], [0.8, 0.2]) print(\"loss_ce1:\", loss_ce1) print(\"loss_ce2:\", loss_ce2) 运行结果 softmax与交叉熵解和：tf.nn.softmax_cross_entropy_with_logits(y_,y) 输出先过softmax函数，再计算y与y_的交叉熵损失函数 # softmax与交叉熵损失函数的结合 import tensorflow as tf import numpy as np y_ = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]) y = np.array([[12, 3, 2], [3, 10, 1], [1, 2, 5], [4, 6.5, 1.2], [3, 6, 1]]) y_pro = tf.nn.softmax(y) loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro) loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y) print('分步计算的结果:\\n', loss_ce1) print('结合计算的结果:\\n', loss_ce2) 运行结果： 主要学习的资料，西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记 ","date":"2020-05-20","objectID":"/loss/:0:1","tags":["loss"],"title":"TensorFlow2.1入门学习笔记(7)——损失函数","uri":"/loss/"},{"categories":["TF2.1学习笔记"],"content":"常见函数 tf.where(条件语句，真返回A，假返回B) import tensorflow as tf a = tf.constant([1, 2, 3, 1, 1]) b = tf.constant([0, 1, 3, 4, 5]) # 若a\u003eb，返回a对应位置的元素，否则返回b对应位置的元素 c = tf.where(tf.greater(a, b), a, b) print(\"c：\", c) # c： tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32) np.random.RandomState.rand(维度) 返回一个[0, 1)之间的随机数 import numpy as np rdm = np.random.RandomState a = rdm.rand() # 返回一个随机标量 b = rdm.rand(2,3) # 返回一个维度为2行3列的随机数矩阵 print(\"a:\",a) # a: 0.417022004702574 print(\"b:\",b) # b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01][1.46755891e-01 9.23385948e-02 1.86260211e-01]] np.vstack(数组1，数组2) 将两个数组按垂直方向叠加 import numpy as np a = np.array([1,2,3]) b = np.array([4,5,6]) c = np.vstack(a,b) print(\"c:\",c) # c:[[1 2 3][4 5 6]] np.mgrid[] np.mgrid[起始值 : 结束值 : 步长，起始值 : 结束值 : 步长, ……] 包含起始值，不包含结束值 x.ravel() x.ravel() 将x变为一维数组，将变量拉直 np.c_[] np.c_[数组1，数组2，……] 使返回的间隔数值点配对 例： import numpy as np x, y = np.mgrid[1:3:1,2:4:0.5] grid = np.c_[x.ravel(),y.ravel()] print(\"x:\",x) print(\"y:\",y) print(\"grid:\\n\",grid) 运行结果： ","date":"2020-05-20","objectID":"/activation/:0:1","tags":["activation","function"],"title":"TensorFlow2.1入门学习笔记(6)——激活函数","uri":"/activation/"},{"categories":["TF2.1学习笔记"],"content":"神经网络（NN）复杂度 NN复杂度：用NN层数和NN参数的个数表示 空间复杂度：层数 = 隐藏层的层数 + 1个输出层 总参数：总w数 + 总b数 时间复杂度：乘加运算次数 ","date":"2020-05-20","objectID":"/activation/:0:2","tags":["activation","function"],"title":"TensorFlow2.1入门学习笔记(6)——激活函数","uri":"/activation/"},{"categories":["TF2.1学习笔记"],"content":"学习率 选择合适的学习率来更新参数 指数衰减学习率 $指数衰减学习率 = 初始学习率*学习率衰减率^\\frac{当前层数}{多少轮衰减一次}$ import tensorflow as tf w = tf.Variable(tf.constant(5, dtype=tf.float32)) epoch = 40 LR_BASE = 0.2 # 最初学习率 LR_DECAY = 0.99 # 学习率衰减率 LR_STEP = 1 # 喂入多少轮BATCH_SIZE后，更新一次学习率 for epoch in range(epoch): # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环100次迭代。 lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP) with tf.GradientTape() as tape: # with结构到grads框起了梯度的计算过程。 loss = tf.square(w + 1) grads = tape.gradient(loss, w) # .gradient函数告知谁对谁求导 w.assign_sub(lr * grads) # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads print(\"After %sepoch,w is %f,loss is %f,lr is %f\" % (epoch, w.numpy(), loss, lr)) 运行结果： ","date":"2020-05-20","objectID":"/activation/:0:3","tags":["activation","function"],"title":"TensorFlow2.1入门学习笔记(6)——激活函数","uri":"/activation/"},{"categories":["TF2.1学习笔记"],"content":"激活函数 简化模型始终是线性函数，影响模型的表达力 MP模型多了一个非线性函数（激活函数），使得多层神经网络不再是线性，提高层数来提高模型表达力 好的激活函数的特点： 非线性：激活函数非线性时，多层神经网络可以逼近所有函数 可微性：优化器大多用梯度下降更新参数 单调性 ：当激活函数是单调的，能保证单层神经网络的损失函数是凸函数（更容易收敛） 近似恒等性：$f(x)\\approx x$当参数初始化为随机小值时，神经网路更稳定 激活函数输出值的范围： 激活函数为有限值时，基于梯度下降的优化方法更稳定 激活函数输出为无限值时，可调小学习率 常用的激活函数 Sigmoid函数：tf.nn.sigmoid(x) $f(x)=\\frac{1}{1+e^{-x}}$ 特点: 易造成梯度消失 输出非0均值，收敛慢 幂运算复杂，训练时间长 函数图像 Tanh函数：tf.math.tanh(x) $f(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}$ 特点 输出是0均值 易造成梯度消失 幂运算复杂，训练时间长 函数图像 Relu函数：tf.nn.relu(x) $f(x)=max(x,0)$ 优点 解决了梯度消失问题（在正区间内） 只需判断输入是否大于0，计算速度快 收敛速度远快于sigmoid和tanh 缺点 输出非0均值，收敛慢 Dead Relu问题：某些神经元可能永远不会被激活，导致相应的参数不能被更新 函数图像 Leaky Relu函数：tf.nn.leaky_relu(x) $f(x)=max(\\alpha x,x)$ 特点：理论上来说，Leaky Relu有Relu的所有优点，也不会出现Dead Relu问题，但是在实际使用过程中，并没有完全证明比Relu好用 函数图像 SUMMARIZE 首选relu函数 学习率设置较小值 输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布 初始化中心化，即让随机数生成的参数满足以0为均值，$\\sqrt{\\frac{2}{当前输入特征个数}}$为正态分布 主要学习的资料，西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记 ","date":"2020-05-20","objectID":"/activation/:0:4","tags":["activation","function"],"title":"TensorFlow2.1入门学习笔记(6)——激活函数","uri":"/activation/"},{"categories":["TF2.1学习笔记"],"content":"根据前面的基础知识，可以开始第一个神经网络的搭建，主要学习的资料西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记 ","date":"2020-05-12","objectID":"/firstnn_iris/:0:0","tags":["NN","神经网络"],"title":"TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）","uri":"/firstnn_iris/"},{"categories":["TF2.1学习笔记"],"content":"1.问题背景 问题描述 人们通过经验总结出的规律：通过测量鸢尾花的花萼长、花萼宽、花瓣长、花瓣宽，可以得出鸢尾花的类别。（如：花萼长\u003e花萼宽 且 花瓣长/花瓣宽\u003e2 则为杂色鸢尾花）。大量依靠人工分类工作量巨大，不同的人员分类，标准，准确率都会有所差距。可以借助深度学习来学习其中的特征并对新数据进行预测。 流程设计 大量的[花萼长、花萼宽、花瓣长、花瓣宽（输入特征），对应的类别（标签）]数据对构成数据集 把数据集喂入搭建好的神经网络结构 网络优化参数得到模型 模型读入新输入特征，输出识别结果 模型设计 搭建网络模型 转换为数学模型 所有输入特征x与相应特征权重w相乘加上偏置项b输出结果y。 x：一行四列矩阵，对应四个特征 w：四行三列矩阵 b：3个偏置项 y：一行三列矩阵，对应三种类别的可信度 搭建网络 每个神经元$y_0,y_1,y_2与输入节点x_0,x_1,x_2,x_3$都有联系，称为全连接神经网络权重w与偏置项b会随机初始化一组参数 前向传播 神经网络执行y = x * w + b的过程称为前向传播 损失函数 损失函数：预测值(y)与标准答案($y_i$)的差距，可以定量判断w，b的优劣，当损失函数输出最小时会出现最优解。（有多种损失函数，这里用均方误差） 均方误差：$MSE(y,y_i)=\\frac{\\sum_{k=0}^n(y-y_i)^2}{n}$ 梯度下降 目的：找到一组参数w和b，使得损失函数最小。 梯度：函数对个参数求偏导后的向量，梯度下降的方向是函数减小的方向。 梯度下降：延损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数。 学习率(learning rate, lr)：当学习率设置过小时，收敛过程将变得十分缓慢。当学习率设置过大时，梯度可能会在最小值附近震荡，甚至无法收敛 反向传播 $w_{t+1}=w_t-lr*\\frac{\\partial loss}{\\partial w_t}$ 从前向后，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数。 ","date":"2020-05-12","objectID":"/firstnn_iris/:0:1","tags":["NN","神经网络"],"title":"TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）","uri":"/firstnn_iris/"},{"categories":["TF2.1学习笔记"],"content":"2.数据读入 数据集介绍： 该数据集已集成在sklearn包中，可直接调入使用，数据共有150组，每组包括花萼长、花萼宽、花瓣长、花瓣宽共四个输入特征。同时给出了这一组特征的的对应鸢尾花类别。类别包括Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Viginaica Iris（弗吉尼亚鸢尾）三类，分别用数字0，1，2表示 从sklearn包datasets读入数据集 from sklearn.datasets import load_iris x_data = datasets.load_iris().data #读入iris数据集的所有输入特征 y_data = datasets.load_iris().target #读入iris数据集所有标签 数据预处理 数据集乱序：随机打乱数据 # seed: 随机数种子，是一个整数，当设置之后，每次生征和标签一一对应 np.random.seed(116) np.random.shuffle(x_data) np.random.seed(116) np.random.shuffle(y_data) tf.random.set_seed(116) 将数据集分成训练集和测试集 # 训练集为前120行，测试集为后30行 x_train = x_data[:-30] y_train = y_data[:-30] x_test = x_data[-30:] y_test = y_data[-30:] 输入特征和标签值一一对应，把数据集分批次，每个批次batch(32)组数据 train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) 数据训练 定义神经网络中所有参数可训练 w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1)) b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1)) 嵌套循环迭代，with结构更新参数，显示当前loss for epoch in range(epoch): # 数据集级别迭代 for step, (x_train, y_train) in enumerate(train_db): # batch级别的迭代 with tf.GradientTape() as tape: # 记录梯度信息 # 前向传播过程计算y # 计算总loss grads = tape.gradient(loss, [w1, b1]) # 求导 w1.assign_sub(lr * grads[0]) # 参数w1自更新 b1.assign_sub(lr * grads[1]) # 参数b自更新 # 每个epoch，打印loss信息 print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4)) 计算当前参数前向传播后的准确率，显示当前acc（accuracy） for x_test, y_test in test_db: y = tf.matmul(x_test, w1) + b1 # y为预测结果 y = tf.nn.softmax(y) # y符合概率分布 pred = tf.argmax(y, axis=1) # 返回y中最大值的索引，即预测的分类 pred = tf.cast(pred, dtype=y_test.dtype) # 调整参数类型与标签一致 correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32) correct = tf.reduce_sum(correct) # 将所有batch中的correct数加起来 total_correct += int(correct) # 将所有batch中的correct数加起来 total_number += x_test.shape[0] acc = total_correct / total_number test_acc.append(acc) print(\"Test_acc:\", acc) 数据可视化 loss可视化 plt.title('Loss Function Curve') # 图片标题 plt.xlabel('Epoch') # x轴变量名称 plt.ylabel('Loss') # y轴变量名称 plt.plot(train_loss_results, label=\"$Loss$\") # 逐点画出trian_loss_results值并连线，连线图标是Loss plt.legend() # 画出曲线图标 plt.show() # 画出图像 acc可视化 plt.title('Acc Curve') # 图片标题 plt.xlabel('Epoch') # x轴变量名称 plt.ylabel('Acc') # y轴变量名称 plt.plot(test_acc, label=\"$Accuracy$\") # 逐点画出test_acc值并连线，连线图标是Accuracy plt.legend() plt.show() ","date":"2020-05-12","objectID":"/firstnn_iris/:0:2","tags":["NN","神经网络"],"title":"TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）","uri":"/firstnn_iris/"},{"categories":["TF2.1学习笔记"],"content":"3.完整源码 # 导入所需模块 import tensorflow as tf from sklearn import datasets from matplotlib import pyplot as plt import numpy as np # 导入数据，分别为输入特征和标签 x_data = datasets.load_iris().data y_data = datasets.load_iris().target # 随机打乱数据（因为原始数据是顺序的，顺序不打乱会成的随机数都一样 np.random.seed(116) # 使用相同的seed，保证输入特影响准确率 # seed: 随机数种子，是一个整数，当设置之后，每次生征和标签一一对应 np.random.shuffle(x_data) np.random.seed(116) np.random.shuffle(y_data) tf.random.set_seed(116) # 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行 x_train = x_data[:-30] y_train = y_data[:-30] x_test = x_data[-30:] y_test = y_data[-30:] # 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错 x_train = tf.cast(x_train, tf.float32) x_test = tf.cast(x_test, tf.float32) # from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据） train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32) test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) # 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元 # 用tf.Variable()标记参数可训练 # 使用seed使每次生成的随机数相同 w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1)) b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1)) lr = 0.1 # 学习率为0.1 train_loss_results = [] # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据 test_acc = [] # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据 epoch = 500 # 循环500轮 loss_all = 0 # 每轮分4个step，loss_all记录四个step生成的4个loss的和 # 训练部分 for epoch in range(epoch): #数据集级别的循环，每个epoch循环一次数据集 for step, (x_train, y_train) in enumerate(train_db): #batch级别的循环 ，每个step循环一个batch with tf.GradientTape() as tape: # with结构记录梯度信息 y = tf.matmul(x_train, w1) + b1 # 神经网络乘加运算 y = tf.nn.softmax(y) # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss） y_ = tf.one_hot(y_train, depth=3) # 将标签值转换为独热码格式，方便计算loss和accuracy loss = tf.reduce_mean(tf.square(y_ - y)) # 采用均方误差损失函数mse = mean(sum(y-out)^2) loss_all += loss.numpy() # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确 # 计算loss对各个参数的梯度 grads = tape.gradient(loss, [w1, b1]) # 实现梯度更新 w1 = w1 - lr * w1_grad b = b - lr * b_grad w1.assign_sub(lr * grads[0]) # 参数w1自更新 b1.assign_sub(lr * grads[1]) # 参数b自更新 # 每个epoch，打印loss信息 print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4)) train_loss_results.append(loss_all / 4) # 将4个step的loss求平均记录在此变量中 loss_all = 0 # loss_all归零，为记录下一个epoch的loss做准备 # 测试部分 # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0 total_correct, total_number = 0, 0 for x_test, y_test in test_db: # 使用更新后的参数进行预测 y = tf.matmul(x_test, w1) + b1 y = tf.nn.softmax(y) pred = tf.argmax(y, axis=1) # 返回y中最大值的索引，即预测的分类 # 将pred转换为y_test的数据类型 pred = tf.cast(pred, dtype=y_test.dtype) # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型 correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32) # 将每个batch的correct数加起来 correct = tf.reduce_sum(correct) # 将所有batch中的correct数加起来 total_correct += int(correct) # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数 total_number += x_test.shape[0] # 总的准确率等于total_correct/total_number acc = total_correct / total_number test_acc.append(acc) print(\"Test_acc:\", acc) print(\"--------------------------\") # 绘制 loss 曲线 plt.title('Loss Function Curve') # 图片标题 plt.xlabel('Epoch') # x轴变量名称 plt.ylabel('Loss') # y轴变量名称 plt.plot(train_loss_results, label=\"$Loss$\") # 逐点画出trian_loss_results值并连线，连线图标是Loss plt.legend() # 画出曲线图标 plt.show() # 画出图像 # 绘制 Accuracy 曲线 plt.title('Acc Curve') # 图片标题 plt.xlabel('Epoch') # x轴变量名称 plt.ylabel('Acc') # y轴变量名称 plt.plot(test_acc, label=\"$Accuracy$\") # 逐点画出test_acc值并连线，连线图标是Accuracy plt.legend() plt.show() 博客园链接 ","date":"2020-05-12","objectID":"/firstnn_iris/:0:3","tags":["NN","神经网络"],"title":"TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）","uri":"/firstnn_iris/"},{"categories":["TF2.1学习笔记"],"content":"前面已经学习了有关TensorFlow的一些常用库，以及相关数据的处理方式，下面就是有关神经网络计算的学习笔记。主要学习的资料西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记 ","date":"2020-05-11","objectID":"/calculate/:0:0","tags":["calculate","神经网络"],"title":"TensorFlow2.1入门学习笔记(4)——神经网络计算","uri":"/calculate/"},{"categories":["TF2.1学习笔记"],"content":"1.张量(Tensor)的生成 张量可以表示0阶到n阶数组（列表） 张量：多维数组、多维列表 阶：张量的维数 维数 阶 名字 例子 0-D 0 标量 scalar s=1 2 3 1-D 0 向量 vector v=[1, 2, 3] 2-D 0 矩阵 matrix m=[[1, 2, 3],[4 ,5 ,6]] n-D 0 张量 tensor t=[[[(n个“[”) 数据类型 tf.int,tf.float…… tf.int32, tf.float32, tf.float64 tf.bool tf.constant([True,False]) tf.string tf.constant(“Hello,world!\") 创建一个张量 # tf.constant(张量内容，dtype=数据类型(可选)) import tensorflow as tf a = tf.constant([1,5],dtype=tf.int64) print(a) print(a.dtype) print(a.shape) 运行结果： 将numpy的数据类型转换为Tensor数据类型 tf.convert_to_tensor(数据名, dtype=数据类型(可选)) import tensorflow as tf import numpy as np a = np.arange(0,5) b = tf.convert_to_tensor(a, dtype=int64) print(a) print(b) 运行结果： 创建特殊张量 创建全为0的张量 tf.zeros([个数]维度) 创建全为1的张量 tf.ones([行, 列]维度) chuangjian全为指定值的张量 tf.fill([n,m,j,k……]维度，指定值) import tensorflow as tf a = tf.zeros([2, 3]) b = tf.ones([2,3]) c = tf.fill([2, 3], 5) print(\"a:\", a) print(\"b:\", b) print(\"c:\", c) 运行结果： 正态分布随机数 生成正态分布的随机数，默认均值为0，标准差为1 tf.random.normal(维度，mean=均值，stddev=标准差) 生成截断式正态分布的随机数 tf.random.truncated_normal(维度, mean=均值, stddev=标准差) 保证了生成的随机数在$(\\mu-2\\sigma,\\mu+2\\sigma)$之内 $\\mu:均值, \\sigma:标准差$ 标准差计算公式: $\\sigma = \\sqrt[][\\frac{\\sum_{i=1}^n{(x_i-\\overline{x})^2}}{n}]$ import tensorflow as tf d = tf.random.normal([2, 2], mean=0.5, stddev=1) print(\"d:\", d) e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1) print(\"e:\", e) 运行结果: 生成均匀分布随机数 [minval,maxval) tf.random.uniform(维度, minval=最小值, maxval=最大值) import tensorflow as tf f = tf.random.uniform([2, 2], minval=0, maxval=1) print(\"f:\", f) 运行结果: ","date":"2020-05-11","objectID":"/calculate/:0:1","tags":["calculate","神经网络"],"title":"TensorFlow2.1入门学习笔记(4)——神经网络计算","uri":"/calculate/"},{"categories":["TF2.1学习笔记"],"content":"2.常用函数 强制tensor转换为该数据类型 tf.cast(张量名,dtype=数据类型) 计算张量维度上的最小值 tf.reduce_min(张量名) 计算张量维度上的最大值 tf.reduce_min(张量名) import tensorflow as tf x1 = tf.constant([1., 2., 3.], dtype=tf.float64) print(\"x1:\", x1) x2 = tf.cast(x1, tf.int32) print(\"x2\", x2) print(\"minimum of x2：\", tf.reduce_min(x2)) print(\"maxmum of x2:\", tf.reduce_max(x2)) 运行结果: 理解axis 在一个二维张量或数组中,可以通过调整axis等于1或者1来控制执行维度 axis=0代表跨行(经度,down),而axis=1代表跨列(维度,across) 如果不指定axis,则所有元素参与运算 计算张量沿指定维度的平均值 tf.reduce_mean(张量名, axis=操作轴) 计算张量沿指定维度的和 tf.reduce_sum(张量名, axis=操作轴) import tensorflow as tf x = tf.constant([[1, 2, 3],[2, 2, 3]]) print(\"x:\",x) print(\"mean of axis=0:\",tf.reduce_mean(x,axis=0)) #计算每一列的均值 print(\"sum of axis=1:\",tf.reduce_sum(x,axis=1)) #计算每行的和 运行结果: tf.Variable(初始值) 将变量标记为\"可训练\"的,被标记的变量会在反向传播中记录梯度信息.神经网络训练中,常用该函数标记待训练参数 例如:w = tf.Variable(tf.random.noaml([2,2],mean=2,stddev=1)) 就可以在反向传播过程中通过梯度下降更新参数w TensorFlow中的数学运算 PS: 只有维度相同的张量才可以做四则运算 对应元素的四则运算: tf.add(张量1,张量2,张量3……) tf.subtract(张量1,张量2,张量3……) tf.multiply(张量1,张量2,张量3……) tf.divide(张量1,张量2,张量3……) import tensorflow as tf a = tf.ones([1, 3]) b = tf.fill([1, 3], 3.) print(\"a:\", a) print(\"b:\", b) print(\"a+b:\", tf.add(a, b)) print(\"a-b:\", tf.subtract(a, b)) print(\"a*b:\", tf.multiply(a, b)) print(\"b/a:\", tf.divide(b, a)) 运算结果: 平方,次方与开方: tf.aquare(张量1,张量2,张量3……) tf.pow(张量1,张量2,张量3……) tf.sqrt(张量1,张量2,张量3……) import tensorflow as tf a = tf.fill([1, 2], 3.) print(\"a:\", a) print(\"a的3次方:\", tf.pow(a, 3)) print(\"a的平方:\", tf.square(a)) print(\"a的开方:\", tf.sqrt(a)) 矩阵乘: tf.matmul(张量1,张量2,张量3……) import tensorflow as tf a = tf.ones([3, 2]) b = tf.fill([2, 3], 3.) print(\"a:\", a) print(\"b:\", b) print(\"a*b:\", tf.matmul(a, b)) 运行结果: 将输入特征/标签配对,构建数据集 tf.data.Dataset.from_tensor_slices((输入特征,标签)) (Numpy和Tensor格式都可以用该语句读入数据) import tensorflow as tf features=tf.constant([12,23,10,17]) labels=tf.constant([0,1,1,2]) dataset=tf.data.Dataset.from_tensor_slices((features,labels)) print(dataset) for i in dataset: print(i) 运行结果: 求导运算:tf.GradientTape() with结构记录计算过程,gradient求出张量的梯度 例如: $$\\frac{\\partial\\omega^2}{\\partial\\omega}=2\\omega=2\\ast3.0=6.0$$ import tensorflow as tf with tf.GradientTape() as tape: x = tf.Variable(tf.constant(3.0)) y = tf.pow(x, 2) grad = tape.gradient(y, x) print(grad) 运行结果: enumerate(列表名) enumerate是python的内建函数,可以遍历每个元素(如列表,元组或字符串),组合为:索引 元素,常在for循环中使用 seq = ['one', 'two', 'three'] for i, element in enumerate(seq): print(i, element) 运行结果: 独热编码:tf.one_hot() tf.one_hot(待转换数据,depth=分几类) 在分类问题中,常用独热码做标签 标记类别:1表示是;0表示非 import tensorflow as tf classes = 3 labels = tf.constant([1, 0, 2]) # 输入的元素值最小为0，最大为2 output = tf.one_hot(labels, depth=classes) print(\"result of labels1:\", output) 运行结果： 将输出结果转换为概率分布：tf.nn.softmax() 数学表达式：$Softmax(y_i)=\\frac{e^y_i}{\\sum_{j=0}^ne^y_i}$ 可以使n个分类的n个输出（$y_0,y_1,……y_{n-1}$）符合概率分布 $\\forall x P(X=x)\\in[0,1]且\\sum_xP(X=x)=1$ import tensorflow as tf y = tf.constant([1.01, 2.01, -0.66]) y_pro = tf.nn.softmax(y) print(\"After softmax, y_pro is:\", y_pro) # y_pro 符合概率分布 print(\"The sum of y_pro:\", tf.reduce_sum(y_pro)) # 通过softmax后，所有概率加起来和为1 运行结果： 参数自更新assign_sub() 复制操作，更新参数的值并返回。 调用assign_sub前，先用tf.Variable定义为变量w为可训练（可自更新） w.assign_sub(w要自减的内容) $w-=1$ import tensorflow as tf w=tf.Variable(4) w.assign_sub(1) print(\"w:\",w) 返回指定维度的最大值tf.argmax() 返回张量沿指定维度最大值的索引 tf.argmax(张量名，axis=操作轴) import numpy as np import tensorflow as tf test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]]) print(\"test:\\n\", test) print(\"每一列的最大值的索引：\", tf.argmax(test, axis=0)) # 返回每一列最大值的索引 print(\"每一行的最大值的索引\", tf.argmax(test, axis=1)) # 返回每一行最大值的索引 运行结果： 通过前面的基础知识，下面可以构建一个简单的神经网络——鸢尾花分类问题 博客园链接 ","date":"2020-05-11","objectID":"/calculate/:0:2","tags":["calculate","神经网络"],"title":"TensorFlow2.1入门学习笔记(4)——神经网络计算","uri":"/calculate/"},{"categories":null,"content":" 留言板 ","date":"2020-05-09","objectID":"/comments/:0:0","tags":null,"title":"","uri":"/comments/"},{"categories":["TF2.1学习笔记"],"content":"在正式学习tensorflow2.0之前需要有一定的python基础，对numpy，matplotlib等库有基本的了解，笔者还是AI小白，通过写博客来记录自己的学习过程，同时对所学的东西进行总结。主要学习的资料西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记博客从tf常用的库开始，需要学习python基础的朋友推荐菜鸟教程 ","date":"2020-05-07","objectID":"/pillow/:0:0","tags":["pillow","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(3)——Pillow数字图像处理","uri":"/pillow/"},{"categories":["TF2.1学习笔记"],"content":"1.数字图像的基本概念 在处理数据过程中绝大多数的数据来自图像，图像数据处理是人工智能的重要组成 图像的离散化 连续图像：人眼能直接感受到的图像 数字图像：把连续图像数字化、离散化之后的图像，他是对连续图像的一种近似 **像素（Pixel）：**图像中的最小单位 位图（bitmap）：通过记录每一个像素值来存储和表达的图像 色彩深度/位深度：位图中的每个像素点要用多少个二进制来表示 例如：位深度为24表示每个像素点用24个二进制位来表示（通常RGB三个字节，每个字节8位） EMP：Windows系统的标准位图格式 二值图像（Binary Image） 每个像素只有2种可能的取值，用1位二进制来表示，位深度为1 黑白图像：只有黑色和白色两种颜色，在图像处理和分析时，通常先对图像二值化处理 **PS：**只要仅有两种颜色的图像，都可以被称为二值图像，区分于灰度图 灰度图像（Gray Image） 每个像素使用一个字节表示，位深度为8，可以表示256种级别的灰度，0表示黑色，255表示白色 例：存储512x512的灰度图像，512x512x8bit=256KB 彩色图像 每个像素都有红(R),绿(G),蓝(B)三个分量；一个像素使用三个字节，位深度为24位；可以表示256^3种颜色 RGB为24位真彩色 RGBA图像——32位真彩色 RGB图像+8位透明度信息Alpha，1一个像素使用4个字节，位深度为32位 256色彩色图像 对每个像素使用8位二进制表示，是彩色调色板中的索引值；对于不同的图像，所对应的256种颜色的集合是不一样的；在保存和加载这种类型的位图时，需要将调色板和图像一同保存和加载 图像的压缩 适当降低图像的质量来减它所占有的空间；不同的压缩算法对应不同的图像格式 图像格式 BMP格式：占用空间大，不支持文件压缩，不适用于网页 JPEG格式：有损压缩，压缩效率高，所占空间小 适合于色彩丰富，细节清晰细腻的大图 不适合所含颜色较少，具有大块颜色相近的区域或亮度差异十分明显的简单照片 PNG格式：无损压缩；适合有规律的渐变色彩的图像，广泛运用于网络 GIF格式：支持静态格式和动态格式；动态图片由多幅图片保存为一个图片，循环显示，形成动画效果；只支持256色，适用于色彩简单。颜色较少的小图像 色彩模式 二值图像、灰度图像、RGB图像、RGBA图像 CMYK——印刷四分色：C（cyan=青色）、M（magenta=洋红色）、Y（yellow=黄色）、K（black=黑色） YCbCr——Y（亮度）、Cb（蓝色色度）、Cr（红色色度） HSI——H（色调）、S（饱和度）、I（亮度） 图像类型 序列图像：时间上有一定顺序和间隔、内容上相关的一组图像，其中每幅图像称为帧图像，帧图像之间的时间间隔是固定的 深度图像：是一种三维场景信息的表达式；每个像素的取值代表这个点离相机的距离；采用灰度图表示，每个像素点由一个字节表示；像素点的取值不代表距离，颜色的深浅只代表相对距离的远近 ","date":"2020-05-07","objectID":"/pillow/:0:1","tags":["pillow","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(3)——Pillow数字图像处理","uri":"/pillow/"},{"categories":["TF2.1学习笔记"],"content":"2.Pillow图像处理库 安装和导入包/模块 Pillow的安装 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pillow Pillow.image的导入 from PIL import Image 常用函数 Image.open()函数：打开图像 Image.open(路径)：返回值为image对象 Image.save()函数：保存图像 图像对象.save(文件名)：保存图像，改变文件名后缀名，可转换图像格式 图像对象的主要属性 图像对象.format 图像格式 图像对象.size 图像尺寸 图像对象.mode 色彩模式 from PIL import Image #导入库 img = Image.open(\"TF.jpg\") #当前目录下图片名称（路径） print(img.format) #JPEG 图像格式 print(img.size) #(473, 349) 图像尺寸 print(img.mode) #RGB 色彩模式 imshow()显示图像 需要使用matplotlib库 plt.imshow(image对象/Numpy数组) from PIL import Image import matplotlib.pyplot as plt img = Image.open(\"TF.jpg\") plt.figure(figsize=(5,5)) #创建画布 plt.imshow(img) #画图 plt.title(img.format) #在标题显示图片格式 plt.show() #显示 convert()函数——转换图像的色彩模式 图像对象.convert(色彩模式) img_gray=img.convert(\"L\") print(img_gray.mode) plt.figure(figsize=(5,5)) plt.imshow(img) plt.show() 运行结果 颜色通道的分离与合并 通道分离：图像对象.split() 图像合并：Image.merge(色彩模式,图像列表) from PIL import Image import matplotlib.pyplot as plt #导入库 img = Image.open(\"TF.jpg\") #打开文件 img_r,img_g,img_b = img.split() #通道分离 plt.figure(figsize=(10,10)) #创建画布 plt.subplot(2,2,1) #创建2x2的子图 plt.axis(\"off\") #不显示坐标轴 plt.imshow(img_r,cmap=\"gray\") #以灰度图显示r通道 plt.title(\"R\",fontsize=20) #创建子图标题 plt.subplot(2,2,2) plt.axis(\"off\") plt.imshow(img_g,cmap=\"gray\") plt.title(\"G\",fontsize=20) plt.subplot(2,2,3) plt.axis(\"off\") plt.imshow(img_b,cmap=\"gray\") plt.title(\"B\",fontsize=20) plt.subplot(2,2,4) img_rgb=Image.merge(\"RGB\",[img_r,img_g,img_b]) #通道合并 plt.axis(\"off\") plt.imshow(img_rgb) plt.title(\"RGB\",fontsize=20) plt.show() 运行结果： 转为数组 np.array(图像对象) from PIL import Image import numpy as np import matplotlib.pyplot as plt img = Image.open(\"TF.jpg\") arr_img=np.array(img) print(arr_img.shape) #(349, 473, 3) print(arr_img) 部分结果 对图像的颜色反向，缩放，旋转和镜像 颜色反向：255-图像数组（arr_ima_new=255-arr_img） 缩放图像： 图像对象.resize((width,heigth))，不对原图进行修改 图像对象.thumbnail((width,heigth))，直接对图像对象本身进行缩放 旋转，镜像：图像对象.transpose(旋转方式) 旋转方式 实例： from PIL import Image import numpy as np import matplotlib.pyplot as plt #导入库 plt.rcParams['font.sans-serif']=\"SimHei\" img = Image.open(\"TF.jpg\") #打开文件 plt.figure(figsize=(10,10)) #创建画布 plt.subplot(3,2,1) plt.axis(\"off\") plt.imshow(img) plt.title(\"原图\",fontsize=20) plt.subplot(3,2,2) plt.axis(\"off\") img_arr=np.array(img) img_arr_new=255-img_arr #颜色反向处理 plt.imshow(img_arr_new) plt.title(\"颜色反向\",fontsize=20) plt.subplot(3,2,3) plt.axis(\"off\") plt.imshow(img) plt.title(\"原图\",fontsize=20) plt.subplot(3,2,4) plt.axis(\"off\") img_flr=img.transpose(Image.FLIP_LEFT_RIGHT) plt.imshow(img_flr) plt.title(\"左右翻转\",fontsize=20) plt.subplot(3,2,5) plt.axis(\"off\") img_r90=img.transpose(Image.ROTATE_90) plt.imshow(img_r90) plt.title(\"逆时针旋转90度\",fontsize=20) plt.subplot(3,2,6) plt.axis(\"off\") img_tp=img.transpose(Image.TRANSPOSE) plt.imshow(img_tp) plt.title(\"转置\",fontsize=20) plt.show() 运行结果： 裁剪图像 在图像上的指定位置裁剪出一个矩形区域 图像对象.crop((x0,y0,x1,y1)) ，返回图像对象，(x0,y0)是左上角的像素位置，(x1,y1)是右下角的像素位置 from PIL import Image import matplotlib.pyplot as plt img = Image.open(\"TF.jpg\") plt.figure(figsize=(10,10)) plt.subplot(1,2,1) plt.imshow(img) plt.subplot(1,2,2) img_region=img.crop((100,100,300,300)) plt.imshow(img_region) plt.show() 运行结果： SUMMARIZE: 博客园链接 ","date":"2020-05-07","objectID":"/pillow/:0:2","tags":["pillow","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(3)——Pillow数字图像处理","uri":"/pillow/"},{"categories":["TF2.1学习笔记"],"content":"在正式学习tensorflow2.0之前需要有一定的python基础，对numpy，matplotlib等库有基本的了解，笔者还是AI小白，通过写博客来记录自己的学习过程，同时对所学的东西进行总结。主要学习的资料西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记博客从tf常用的库开始，需要学习python基础的朋友推荐菜鸟教程 数据可视化： 数据分析阶段：理解和洞察数据之间的关系 算法调试阶段：发现问题，优化算法 项目总结阶段：展示项目成果 Matplotlib：第三方库，可以快速生成高质量图表 安装Matplotlib库 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib 导入Matplotlib库 import matplotlib as plt #可用pyplot库绘制平面图，常用import matplotlib.pyplot as plt导入 ","date":"2020-05-04","objectID":"/matplotlib/:0:0","tags":["matplotlib","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(2)——Matplotlib数据可视化","uri":"/matplotlib/"},{"categories":["TF2.1学习笔记"],"content":"1.常用函数 figure对象：创建画布 pit.figure(figsize=(3,2),facecolor=\"green\") #创建画布 plt.plot() #绘制空白图形 plt.show() #显示绘图 subplot()函数——划分子图 subplot(行数,列数,子图序号) 例：将画布划分为2x2的子图区域，并绘制3个子图 fig = plt.figure() plt.subplot(2,2,1) plt.subplot(2,2,2) plt.subplot(2,2,3) plt.show() 运行结果： plt.rcParams[]——设置中文字体 matplotlib库中的文字为英文，如果需要中文时容易乱码，例如：使用plt.rcParams[“font.sans-serif”] = “SimHei\"使默认字体改为中文。 无法正常显示时： 添加标题 添加全局标题：suptitle(标题文字) 添加子标题：title(标题文字) suptitle()函数的主要参数： title()函数的主要参数： tight_layout()函数 检查坐标轴标签、刻度标签和子图标题，自动调整子图，使之填充整个绘图区域，并消除子图之间的重叠 SUMMARIZE: import matplotlib.pyplot as plt plt.rcParams[\"font.family\"] = \"SimHei\" fig = plt.figure(facecolor = \"lightgrey\") plt.subplot(2,2,1) plt.title('子图标题1') plt.subplot(2,2,2) plt.title('子图标题2',loc=\"left\",color=\"b\") plt.subplot(2,2,3) myfontdict = {\"fontsize\":12,\"color\":\"g\",\"rotation\":30} plt.title('子图标题3',fontdict=myfontdict) plt.subplot(2,2,4) plt.title('子图标题4',color = \"white\",backgroundcolor=\"black\") plt.suptitle(\"全局标题\",fontsize=20,color=\"red\",backgroundcolor=\"yellow\") plt.tight_layout(rect=[0,0,1,0.9]) plt.show() 运行结果： ","date":"2020-05-04","objectID":"/matplotlib/:0:1","tags":["matplotlib","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(2)——Matplotlib数据可视化","uri":"/matplotlib/"},{"categories":["TF2.1学习笔记"],"content":"2.散点图（Scatter） 数据点在直角坐标系中的分布图 scatter()函数 text()函数 添加文字 xlabel(),ylabel()函数 设置坐标轴 增加图例 绘制标准正态分布，均匀分布的散点图 import matplotlib.pyplot as plt import numpy as np plt.rcParams[\"font.sans-serif\"]=\"SimHei\" #设置中文字体为默认字体 plt.rcParams[\"axes.unicode_minus\"]=False #使“-”正常显示 n = 1024 #随机点个数：1024 x = np.random.normal(0,1,n) #生成数据点x坐标 y = np.random.normal(0,1,n) #生成数据点y坐标 plt.scatter(x,y,color=\"blue\",marker=\".\") #绘制数据点 plt.title(\"标准正态分布\",fontsize=20) #设置标题 plt.text(2.5,2.5,\"均 值：0\\n标准差：1\") #显示文本 plt.xlim(-4,4) #x轴范围 plt.ylim(-4,4) #y轴范围 plt.xlabel('横坐标x',fontsize=14) #x轴名称 plt.ylabel('纵坐标y',fontsize=14) #y轴名称 plt.show() #显示 运行结果： ","date":"2020-05-04","objectID":"/matplotlib/:0:2","tags":["matplotlib","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(2)——Matplotlib数据可视化","uri":"/matplotlib/"},{"categories":["TF2.1学习笔记"],"content":"3.折线图（Line Chart） 在散点图的基础上，将相邻的点用线段连接，描述变量变化的趋势 plot()函数 绘制温度和湿度数据折线图 import matplotlib.pyplot as plt import numpy as np plt.rcParams[\"font.sans-serif\"]=\"SimHei\" n = 24 y1 = np.random.randint(27,37,n) #生成随机数据 y2 = np.random.randint(40,60,n) plt.plot(y1, label=\"温度\") #绘制 plt.plot(y2, label=\"湿度\") plt.xlabel(\"小时\",fontsize=12) plt.ylabel(\"测量值\",fontsize=12) plt.title(\"24小时温度湿度统计\",fontsize=16) plt.legend() plt.show() 运行结果 ","date":"2020-05-04","objectID":"/matplotlib/:0:3","tags":["matplotlib","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(2)——Matplotlib数据可视化","uri":"/matplotlib/"},{"categories":["TF2.1学习笔记"],"content":"4.柱形图（Bar Chart） 由一系列高度不等的柱形图条纹表示数据分布的情况 bar()函数 绘制柱形图 import matplotlib.pyplot as plt import numpy as np plt.rcParams[\"font.sans-serif\"]=\"SimHei\" #设置中文字体为默认字体 plt.rcParams[\"axes.unicode_minus\"]=False #使“-”正常显示 y1 = [32,25,32,35,45,33,17,24,20,10,32,5] y2 = [-14,-25,-18,-35,-46,-22,-18,-3,-24,-13,-25,-28] plt.bar(range(len(y1)),y1,width=0.8,facecolor=\"g\",edgecolor=\"w\",label=\"统计量1\") plt.bar(range(len(y2)),y2,width=0.8,facecolor='r',edgecolor='w',label=\"统计量2\") plt.title(\"柱状图\",fontsize=20) plt.legend() plt.show() 运行结果 ","date":"2020-05-04","objectID":"/matplotlib/:0:4","tags":["matplotlib","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(2)——Matplotlib数据可视化","uri":"/matplotlib/"},{"categories":["TF2.1学习笔记"],"content":"Matplotlib官网 http://matplotlib.org https://matplotlib.org/genindex.html Gallery页面 博客园链接 ","date":"2020-05-04","objectID":"/matplotlib/:0:5","tags":["matplotlib","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(2)——Matplotlib数据可视化","uri":"/matplotlib/"},{"categories":["TF2.1学习笔记"],"content":"在正式学习tensorflow2.0之前需要有一定的python基础，对numpy，matplotlib等库有基本的了解，笔者还是AI小白，通过写博客来记录自己的学习过程，同时对所学的东西进行总结。主要学习资料西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学：人工智能实践Tensorflow笔记。博客从tf常用的库开始，需要学习python基础的朋友推荐菜鸟教程 ","date":"2020-04-29","objectID":"/numpy/:0:0","tags":["numpy","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(1)——Numpy科学计算库","uri":"/numpy/"},{"categories":["TF2.1学习笔记"],"content":"1.多为数组的形状(Shape) 描述数组的维度，以及各维度内部元素个数 一维数组 shape:(5,) 描述某位同学5门课程的成绩： 二维数组 shape:(30,5) 描述某个班30位同学5门课成绩： 三维数组 shape:(10,30,5) 描述某个学校10个班30位同学5门课成绩： 四维数组 shape:(5,10,30,5) 描述某个地区5所学校10个班30位同学5门课成绩： 五维数组 shape:(4,5,10,30,5) 描述某个某个国家4个地区5所学校10个班30位同学5门课成绩： 更高维以此类推 ","date":"2020-04-29","objectID":"/numpy/:0:1","tags":["numpy","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(1)——Numpy科学计算库","uri":"/numpy/"},{"categories":["TF2.1学习笔记"],"content":"2.创建Nump 安装Numpy库 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy 导入Numpy库 import numpy as np import numpy import * #可直接调用库，但不推荐，容易和其他包冲突 创建数组 m = np.array([[[4, 5, 8, 3],[3, 6, 9, 0],[8, 4, 5, 6]], [[4, 5, 8, 3],[3, 6, 9, 0],[8, 4, 2, 1]]]) # 数组属性 m.ndim #3 维度 m.shape #(2,3,4) 形状 m.size #24 元素的总个数 m.dtype #int32 数据类型 m.itemsize #4 每个元素的字节数 创建特殊的数组 # np.arrange(start=0,stop,num=1,dtype) 前闭后开，不包含结束值 n=np.arange(4) #array([0, 1, 2, 3]) a=np.arange(0,2,0.3) #array([0., 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) np.ones((3,2),dtype =np.int64) #array([[1,1],[1, 1],[1, 1]],dtype = int64) np.eye(2, 3) #array([[1., 0., 0.], [0., 1., 0.]])创建一个单位矩阵 # np.logspace(stat,stop,num,base,dtype)参数：起始指数，结束指数，基，元素数据类型，包含结束值 np.logspace(1, 5, 5, base=2) #array([2., 4., 8, 16, 32]) ","date":"2020-04-29","objectID":"/numpy/:0:2","tags":["numpy","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(1)——Numpy科学计算库","uri":"/numpy/"},{"categories":["TF2.1学习笔记"],"content":"3.数组计算 需要了解几个常见的数组数据处理函数 # 数组元素切片 a = np.array([0,1,2,3]) #一维数组 print(a[:3]) #array([0,1,2]) 输出前三个数 b = np.array([[0,1,2,3],[3,4,5,6],[6,7,8,9]]) #二维数组 print(b[:2]) #array([[0,1,2,3],[3,4,5,6]]) 输出前两行 # 改变数组的形状 c = np.arange(12) d = c.reshape(3,4) print(d) #array([[0,1,2,3],[4,5,6,7],[8,9,10,11]]) 不改变当前数组，按照shape创建新的数组 c.reshape(-1,1) print(c) #array([[0],[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11]]) c.resize(3,4) print(c) #array([[0,1,2,3],[4,5,6,7],[8,9,10,11]]) 改变当前数组，按照shape创建新的数组 数组间的运算 1.数组间的元素运算 a = np.arange(4) print(np.sum(a)) #6 print(np.sqrt(a)) #array([0. ,1. ,1.41421356, 1.73205081]) 数组的轴和秩 数组的堆叠运算 x = np.array([1,2,3]) y = np.array([4,5,6]) print(np.stack((x,y),axis = 0)) #array([1,2,3],[4,5,6]) print(np.stack((x,y),axis = 1)) #array([1,4],[2,5],[3,6]) a = np.arrange(12).reshape(3,4) #a = ([0,1,2,3],[4,5,6,7],[8,9,10,11]) print(np.sum(b,axis=0)) #array([12,15,18,21]) print(np.sum(b,axis=1)) #array([6,22,38]) 2.数组加减法，对应元素相加减（进行运算的数组长度要一致） a = np.ones([3,3]) b = np.eye(3,3) print(a+b) #array([[2，1,1],[1,2,1],[1,1,2]]) 3.一维数组可以和多维数组相加，相加时将会将一维数组扩展至多维 a = np.array([1,2,3]) b = np.array([1,1,1],[2,2,2]) print(a+b) #array([2,3,4],[3,4,5]) print(b**2) #array([1,1,1],[4,4,4]) SUMMARIZE:数组间的四则运算，是对应元素加减乘除； 当数组中元素的数据类型不同时，精度低的数据类型会转换成精度高的数据类型，然后再运算 矩阵运算 1.矩阵乘法，按矩阵相乘的规则运算 A = np.array([[1,2],[2,3]]) B = np.array([[4,2,1],[1,5,2]]) print(np.matmul(A,B)) #array([6,12,5],[11,19,8]) 2.转置和求逆 #转置 print(np.transpose(A)) #array([1,2],[2,3]) #求逆 print(np.linalg.inv(A)) #array([-3,2],[2,-1]) 博客园链接 ","date":"2020-04-29","objectID":"/numpy/:0:3","tags":["numpy","神经网络","基本库"],"title":"TensorFlow2.1入门学习笔记(1)——Numpy科学计算库","uri":"/numpy/"},{"categories":["Uncategorized"],"content":"Introduction This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I’ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won’t cover using CSS to style your theme. We’ll start with creating a new site with a very basic template. Then we’ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites. In this tutorial, commands that you enter will start with the “$” prompt. The output will follow. Lines that start with “#” are comments that I’ve added to explain a point. When I show updates to a file, the “:wq” on the last line means to save the file. Here’s an example: ## this is a comment\r$ echo this is a command\rthis is a command\r## edit the file\r$vi foo.md\r+++\rdate = \"2014-09-28\"\rtitle = \"creating a new theme\"\r+++\rbah and humbug\r:wq\r## show it\r$ cat foo.md\r+++\rdate = \"2014-09-28\"\rtitle = \"creating a new theme\"\r+++\rbah and humbug\r$\r ","date":"2014-09-28","objectID":"/creating-a-new-theme/:1:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Some Definitions There are a few concepts that you need to understand before creating a theme. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:2:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Skins Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors. You have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin. Your second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it? The difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it. The rest of this tutorial will call a skin created in the themes/ directory a theme. Note that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:2:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"The Home Page The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:2:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Site Configuration File When Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it. Hugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:2:3","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Content Content is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML. Front Matter The front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON. The information in the front matter is passed into the template before the content is rendered into HTML. Markdown Content is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:2:4","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Template Files Hugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it’s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use. There are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template. Hugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site. Please note that you can use the front matter to influence Hugo’s choice of templates. Single Template A single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template. List Template A list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups. The homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site. Partial Template A partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:2:5","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Create a New Site Let’s use Hugo to create a new web site. I’m a Mac user, so I’ll create mine in my home directory, in the Sites folder. If you’re using Linux, you might have to create the folder first. The “new site” command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file. $ hugo new site ~/Sites/zafta\r$ cd ~/Sites/zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 7 quoha staff 238 Sep 29 16:49 .\rdrwxr-xr-x 3 quoha staff 102 Sep 29 16:49 ..\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$\r Take a look in the content/ directory to confirm that it is empty. The other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That’s a topic for a different tutorial, so please ignore them for now. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:3:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Generate the HTML For the New Site Running the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that’s everything that’s not content). Since we have an empty site, it won’t do much, but it will do it very quickly. $ hugo --verbose\rINFO: 2014/09/29 Using config file: config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ The “--verbose” flag gives extra information that will be helpful when we build the template. Every line of the output that starts with “INFO:” or “WARN:” is present because we used that flag. The lines that start with “WARN:” are warning messages. We’ll go over them later. We can verify that the command worked by looking at the directory again. $ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\r$\r See that new public/ directory? Hugo placed all generated content there. When you’re ready to publish your web site, that’s the place to start. For now, though, let’s just confirm that we have what we’d expect from a site with no content. $ ls -l public\rtotal 16\r-rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml\r-rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml\r$ Hugo created two XML files, which is standard, but there are no HTML files. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:3:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Test the New Site Verify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the “server” command. If it is successful, you will see output similar to the following: $ hugo server --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\r Connect to the listed URL (it’s on the line that starts with “Web Server”). If everything is working correctly, you should get a page that shows the following: index.xml\rsitemap.xml\r That’s a listing of your public/ directory. Hugo didn’t create a home page because our site has no content. When there’s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser. Let’s go back and look at those warnings again. WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\rWARN: 2014/09/29 Unable to locate layout: [404.html]\r That second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial. Now for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page. I like that the verbose flag causes Hugo to list the files that it’s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we’ll cover later that explain the names and paths. For now, just remember that Hugo couldn’t find a template for the home page and it told you so. At this point, you’ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:3:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Create a New Theme Hugo doesn’t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes. We’re going to create a new theme called “zafta.” Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional. All themes have opinions on content and layout. For example, Zafta uses “post” over “blog”. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:4:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Create a Skeleton Use the hugo “new” command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out. $ hugo new theme zafta\r$ ls -l\rtotal 8\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes\r-rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public\rdrwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes\r$ find themes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r-rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml\r$ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype. Please take a minute to fill out the theme.toml and LICENSE.md files. They’re optional, but if you’re going to be distributing your theme, it tells the world who to praise (or blame). It’s also nice to declare the license so that people will know how they can use the theme. $ vi themes/zafta/theme.toml\rauthor = \"michael d henderson\"\rdescription = \"a minimal working template\"\rlicense = \"MIT\"\rname = \"zafta\"\rsource_repo = \"\"\rtags = [\"tags\", \"categories\"]\r:wq\r## also edit themes/zafta/LICENSE.md and change\r## the bit that says \"YOUR_NAME_HERE\"\r Note that the the skeleton’s template files are empty. Don’t worry, we’ll be changing that shortly. $ find themes/zafta -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html\r$\r ","date":"2014-09-28","objectID":"/creating-a-new-theme/:4:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Update the Configuration File to Use the Theme Now that we’ve got a theme to work with, it’s a good idea to add the theme name to the configuration file. This is optional, because you can always add “-t zafta” on all your commands. I like to put it the configuration file because I like shorter command lines. If you don’t put it in the configuration file or specify it on the command line, you won’t use the template that you’re expecting to. Edit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format. $ vi config.toml\rtheme = \"zafta\"\rbaseurl = \"\"\rlanguageCode = \"en-us\"\rtitle = \"zafta - totally refreshing\"\rMetaDataFormat = \"toml\"\r:wq\r$\r ","date":"2014-09-28","objectID":"/creating-a-new-theme/:4:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Generate the Site Now that we have an empty theme, let’s generate the site again. $ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$\r Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme’s directory. Let’s check the public/ directory to see what Hugo’s created. $ ls -l public\rtotal 16\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html\r-rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js\r-rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml\r$\r Notice four things: Hugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn’t create any pages. That’s because it considers a “page” to be a file created directly from a content file. It doesn’t count things like the index.html files that it creates automatically. The Home Page Hugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo’s warning message shows that it looks for three different templates: WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html]\r If it can’t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed. When Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page. $ find . -name index.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html\r$ The Magic of Static Hugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are. Hugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo’s opinion is that you’ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don’t like that, you can change the directory names in your theme directory or even delete them completely. Hugo’s nice enough to offer its opinion, then behave nicely if you disagree. $ find themes/zafta -type d | xargs ls -ld\rdrwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta\rdrwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes\rdrwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials\rdrwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css\rdrwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js\r$ ","date":"2014-09-28","objectID":"/creating-a-new-theme/:4:3","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"The Theme Development Cycle When you’re working on a theme, you will make changes in the theme’s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy: Purge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4. I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes. Check the main Hugo site for information on using Git with Hugo. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:5:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Purge the public/ Directory When generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it. Note: If you’re building on an SSD, you should ignore this. Churning on a SSD can be costly. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:5:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Hugo’s Watch Option Hugo’s “--watch” option will monitor the content/ and your theme directories for changes and rebuild the site automatically. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:5:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Live Reload Hugo’s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, “Wow, that’s totally amazing.” ","date":"2014-09-28","objectID":"/creating-a-new-theme/:5:3","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Development Commands Use the following commands as the basis for your workflow. ## purge old files. hugo will recreate the public directory.\r##\r$ rm -rf public\r##\r## run hugo in watch mode\r##\r$ hugo server --watch --verbose\r Here’s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I’ve said this before, it’s amazing. $ rm -rf public\r$ hugo server --watch --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\rWatching for changes in /Users/quoha/Sites/zafta/content\rServing pages from /Users/quoha/Sites/zafta/public\rWeb Server is available at http://localhost:1313\rPress Ctrl+C to stop\rINFO: 2014/09/29 File System Event: [\"/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\": MODIFY|ATTRIB]\rChange detected, rebuilding site\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 1 ms\r ","date":"2014-09-28","objectID":"/creating-a-new-theme/:5:4","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Update the Home Page Template The home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme’s layout/ directory: index.html _default/list.html _default/single.html We could update one of the default templates, but a good design decision is to update the most specific template available. That’s not a hard and fast rule (in fact, we’ll break it a few times in this tutorial), but it is a good generalization. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Make a Static Home Page Right now, that page is empty because we don’t have any content and we don’t have any logic in the template. Let’s change that by adding some text to the template. $ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003cp\u003ehugo says hello!\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e :wq\r$\r Build the web site and then verify the results. $ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 0 pages created 0 tags created\r0 categories created\rin 2 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003cp\u003ehugo says hello!\u003c/p\u003e \u003c/html\u003e\r Live Reload Note: If you’re running the server with the --watch option, you’ll see different content in the file: $ cat public/index.html \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003cbody\u003e \u003cp\u003ehugo says hello!\u003c/p\u003e \u003cscript\u003edocument.write('\u003cscript src=\"http://' + (location.host || 'localhost').split(':')[0] + ':1313/livereload.js?mindelay=10\"\u003e\u003c/' + 'script\u003e')\u003c/script\u003e\u003c/body\u003e \u003c/html\u003e\r When you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Build a “Dynamic” Home Page “Dynamic home page?” Hugo’s a static web site generator, so this seems an odd thing to say. I mean let’s have the home page automatically reflect the content in the site every time Hugo builds it. We’ll use iteration in the template to do that. Create New Posts Now that we have the home page generating static content, let’s add some content to the site. We’ll display these posts as a list on the home page and on their own page, too. Hugo has a command to generate a skeleton post, just like it does for sites and themes. $ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md\rERROR: 2014/09/29 Unable to Cast \u003cnil\u003e to map[string]interface{}\r$ That wasn’t very nice, was it? The “new” command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there’s a theme. For me, the workaround was to create an archetypes file specifically for the post type. $ vi themes/zafta/archetypes/post.md\r+++\rDescription = \"\"\rTags = []\rCategories = []\r+++\r:wq\r$ find themes/zafta/archetypes -type f | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md\r-rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md\r$ hugo --verbose new post/first.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/first.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md\r/Users/quoha/Sites/zafta/content/post/first.md created\r$ hugo --verbose new post/second.md\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 attempting to create post/second.md of post\rINFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md\rINFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md\r/Users/quoha/Sites/zafta/content/post/second.md created\r$ ls -l content/post\rtotal 16\r-rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md\r-rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md\r$ cat content/post/first.md +++\rCategories = []\rDescription = \"\"\rTags = []\rdate = \"2014-09-29T21:54:53-05:00\"\rtitle = \"first\"\r+++\rmy first post\r$ cat content/post/second.md +++\rCategories = []\rDescription = \"\"\rTags = []\rdate = \"2014-09-29T21:57:09-05:00\"\rtitle = \"second\"\r+++\rmy second post\r$ Build the web site and then verify the results. $ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"category\":\"categories\", \"tag\":\"tags\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$\r The output says that it created 2 pages. Those are our new posts: $ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html\r$\r The new files are empty because because the templates used to generate the content are empty. The homepage doesn’t show the new content, either. We have to update the templates to add the posts. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"List and Single Templates In Hugo, we have three major kinds of templates. There’s the home page template that we updated previously. It is used only by the home page. We also have “single” templates which are used to generate output for a single content file. We also have “list” templates that are used to group multiple pieces of content before generating output. Generally speaking, list templates are named “list.html” and single templates are named “single.html.” There are three other types of templates: partials, content views, and terms. We will not go into much detail on these. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:3","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Add Content to the Homepage The home page will contain a list of posts. Let’s update its template to add the posts that we just created. The logic in the template will run every time we build the site. $ vi themes/zafta/layouts/index.html \u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003cbody\u003e\r{{ range first 10 .Data.Pages }}\r\u003ch1\u003e{{ .Title }}\u003c/h1\u003e\r{{ end }}\r\u003c/body\u003e\r\u003c/html\u003e\r:wq\r$\r Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between “{{” and “}}\". In our template, the commands are: range .Title end The “range” command is an iterator. We’re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created. The “.Title” command prints the value of the “title” variable. Hugo pulls it from the front matter in the Markdown file. The “end” command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds “end.” Everything between the “range” and “end” is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one. It’s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files. Build the web site and then verify the results. $ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003cbody\u003e\r\u003ch1\u003esecond\u003c/h1\u003e\r\u003ch1\u003efirst\u003c/h1\u003e\r\u003c/body\u003e\r\u003c/html\u003e\r$\r Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let’s take a moment to appreciate what we’ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you’ve learned everything you need to know to build a theme. All that’s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine. And, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don’t worry, though, that’s all to come. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:4","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Add Content to the Posts We’re working with posts, which are in the content/post/ directory. That means that their section is “post” (and if we don’t do something weird, their type is also “post”). Hugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can’t find one, then it will look in the _default/ directory. There are some twists that we’ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html. Now that we know the search rule, let’s see what we actually have available: $ find themes/zafta -name single.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html\r We could create a new template, post/single.html, or change the default. Since we don’t know of any other content types, let’s start with updating the default. Remember, any content that we haven’t created a template for will end up using this template. That can be good or bad. Bad because I know that we’re going to be adding different types of content and we’re going to end up undoing some of the changes we’ve made. It’s good because we’ll be able to see immediate results. It’s also good to start here because we can start to build the basic layout for the site. As we add more content types, we’ll refactor this file and move logic around. Hugo makes that fairly painless, so we’ll accept the cost and proceed. Please see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you’re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That’s a refreshing amount of joy right there. Update the Template File $ vi themes/zafta/layouts/_default/single.html \u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003chead\u003e\r\u003ctitle\u003e{{ .Title }}\u003c/title\u003e\r\u003c/head\u003e\r\u003cbody\u003e\r\u003ch1\u003e{{ .Title }}\u003c/h1\u003e\r{{ .Content }}\r\u003c/body\u003e\r\u003c/html\u003e\r:wq\r$\r Build the web site and verify the results. $ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html\r$ cat public/post/first/index.html \u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003chead\u003e\r\u003ctitle\u003efirst\u003c/title\u003e\r\u003c/head\u003e\r\u003cbody\u003e\r\u003ch1\u003efirst\u003c/h1\u003e\r\u003cp\u003emy first post\u003c/p\u003e\r\u003c/body\u003e\r\u003c/html\u003e\r$ cat public/post/second/index.html \u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003chead\u003e\r\u003ctitle\u003esecond\u003c/title\u003e\r\u003c/head\u003e\r\u003cbody\u003e\r\u003ch1\u003esecond\u003c/h1\u003e\r\u003cp\u003emy second post\u003c/p\u003e\r\u003c/body\u003e\r\u003c/html\u003e\r$\r Notice that the posts now have content. You can go to localhost:1313/post/first to verify. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:5","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Linking to Content The posts are on the home page. Let’s add a link from there to the post. Since this is the home page, we’ll update its template. $ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003cbody\u003e\r{{ range first 10 .Data.Pages }}\r\u003ch1\u003e\u003ca href=\"{{ .Permalink }}\"\u003e{{ .Title }}\u003c/a\u003e\u003c/h1\u003e\r{{ end }}\r\u003c/body\u003e\r\u003c/html\u003e\r Build the web site and verify the results. $ rm -rf public\r$ hugo --verbose\rINFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/\rINFO: 2014/09/29 found taxonomies: map[string]string{\"tag\":\"tags\", \"category\":\"categories\"}\rWARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html]\r0 draft content 0 future content 2 pages created 0 tags created\r0 categories created\rin 4 ms\r$ find public -type f -name '*.html' | xargs ls -l\r-rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html\r-rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html\r-rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html\r-rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html\r$ cat public/index.html \u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003cbody\u003e\r\u003ch1\u003e\u003ca href=\"/post/second/\"\u003esecond\u003c/a\u003e\u003c/h1\u003e\r\u003ch1\u003e\u003ca href=\"/post/first/\"\u003efirst\u003c/a\u003e\u003c/h1\u003e\r\u003c/body\u003e\r\u003c/html\u003e\r$\r ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:6","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Create a Post Listing We have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let’s make it show a list of all posts (not just the first ten). We need to decide which template to update. This will be a listing, so it should be a list template. Let’s take a quick look and see which list templates are available. $ find themes/zafta -name list.html | xargs ls -l\r-rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html\r As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don’t have multiple content types, so let’s stay consistent and update the default list template. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:6:7","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Creating Top Level Pages Let’s add an “about” page and display it at the top level (as opposed to a sub-level like we did with posts). The default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let’s verify that by creating an “about” page at the top level: $ vi content/about.md +++\rtitle = \"about\"\rdescription = \"about this site\"\rdate = \"2014-09-27\"\rslug = \"about time\"\r+++\r## about us\ri'm speechless\r:wq\r Generate the web site and verify the results. $ find public -name '*.html' | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html\r Notice that the page wasn’t created at the top level. It was created in a sub-directory named ‘about-time/'. That name came from our slug. Hugo will use the slug to name the generated content. It’s a reasonable default, by the way, but we can learn a few things by fighting it for this file. One other thing. Take a look at the home page. $ cat public/index.html\r\u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003cbody\u003e\r\u003ch1\u003e\u003ca href=\"http://localhost:1313/post/theme/\"\u003ecreating a new theme\u003c/a\u003e\u003c/h1\u003e\r\u003ch1\u003e\u003ca href=\"http://localhost:1313/about-time/\"\u003eabout\u003c/a\u003e\u003c/h1\u003e\r\u003ch1\u003e\u003ca href=\"http://localhost:1313/post/second-post/\"\u003esecond\u003c/a\u003e\u003c/h1\u003e\r\u003ch1\u003e\u003ca href=\"http://localhost:1313/post/first-post/\"\u003efirst\u003c/a\u003e\u003c/h1\u003e\r\u003cscript\u003edocument.write('\u003cscript src=\"http://'\r+ (location.host || 'localhost').split(':')[0]\r+ ':1313/livereload.js?mindelay=10\"\u003e\u003c/'\r+ 'script\u003e')\u003c/script\u003e\u003c/body\u003e\r\u003c/html\u003e\r Notice that the “about” link is listed with the posts? That’s not desirable, so let’s change that first. $ vi themes/zafta/layouts/index.html\r\u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003cbody\u003e\r\u003ch1\u003eposts\u003c/h1\u003e\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \"post\"}}\r\u003ch2\u003e\u003ca href=\"{{ .Permalink }}\"\u003e{{ .Title }}\u003c/a\u003e\u003c/h2\u003e\r{{ end }}\r{{ end }}\r\u003ch1\u003epages\u003c/h1\u003e\r{{ range .Data.Pages }}\r{{ if eq .Type \"page\" }}\r\u003ch2\u003e\u003ca href=\"{{ .Permalink }}\"\u003e{{ .Title }}\u003c/a\u003e\u003c/h2\u003e\r{{ end }}\r{{ end }}\r\u003c/body\u003e\r\u003c/html\u003e\r:wq\r Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it. But, that about page still renders to about-time/index.html. $ find public -name '*.html' | xargs ls -l\r-rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html\r-rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html\r-rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html\r-rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html\r-rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html\r Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let’s do it the hard way and change the permalink in the configuration file. $ vi config.toml\r[permalinks]\rpage = \"/:title/\"\rabout = \"/:filename/\"\r Generate the web site and verify that this didn’t work. Hugo lets “slug” or “URL” override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:7:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Sharing Templates If you’ve been following along, you probably noticed that posts have titles in the browser and the home page doesn’t. That’s because we didn’t put the title in the home page’s template (layouts/index.html). That’s an easy thing to do, but let’s look at a different option. We can put the common bits into a shared template that’s stored in the themes/zafta/layouts/partials/ directory. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:8:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Create the Header and Footer Partials In Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme’s presentation. $ vi themes/zafta/layouts/partials/header.html\r\u003c!DOCTYPE html\u003e\r\u003chtml\u003e\r\u003chead\u003e\r\u003ctitle\u003e{{ .Title }}\u003c/title\u003e\r\u003c/head\u003e\r\u003cbody\u003e\r:wq\r$ vi themes/zafta/layouts/partials/footer.html\r\u003c/body\u003e\r\u003c/html\u003e\r:wq\r ","date":"2014-09-28","objectID":"/creating-a-new-theme/:8:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Update the Home Page Template to Use the Partials The most noticeable difference between a template call and a partials call is the lack of path: {{ template \"theme/partials/header.html\" . }}\r versus {{ partial \"header.html\" . }}\r Both pass in the context. Let’s change the home page template to use these new partials. $ vi themes/zafta/layouts/index.html\r{{ partial \"header.html\" . }}\r\u003ch1\u003eposts\u003c/h1\u003e\r{{ range first 10 .Data.Pages }}\r{{ if eq .Type \"post\"}}\r\u003ch2\u003e\u003ca href=\"{{ .Permalink }}\"\u003e{{ .Title }}\u003c/a\u003e\u003c/h2\u003e\r{{ end }}\r{{ end }}\r\u003ch1\u003epages\u003c/h1\u003e\r{{ range .Data.Pages }}\r{{ if or (eq .Type \"page\") (eq .Type \"about\") }}\r\u003ch2\u003e\u003ca href=\"{{ .Permalink }}\"\u003e{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u003c/a\u003e\u003c/h2\u003e\r{{ end }}\r{{ end }}\r{{ partial \"footer.html\" . }}\r:wq\r Generate the web site and verify the results. The title on the home page is now “your title here”, which comes from the “title” variable in the config.toml file. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:8:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Update the Default Single Template to Use the Partials $ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r\u003ch1\u003e{{ .Title }}\u003c/h1\u003e\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\r Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:8:3","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Add “Date Published” to Posts It’s common to have posts display the date that they were written or published, so let’s add that. The front matter of our posts has a variable named “date.” It’s usually the date the content was created, but let’s pretend that’s the value we want to display. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:9:0","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Add “Date Published” to the Template We’ll start by updating the template used to render the posts. The template code will look like: {{ .Date.Format \"Mon, Jan 2, 2006\" }}\r Posts use the default single template, so we’ll change that file. $ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r\u003ch1\u003e{{ .Title }}\u003c/h1\u003e\r\u003ch2\u003e{{ .Date.Format \"Mon, Jan 2, 2006\" }}\u003c/h2\u003e\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\r Generate the web site and verify the results. The posts now have the date displayed in them. There’s a problem, though. The “about” page also has the date displayed. As usual, there are a couple of ways to make the date display only on posts. We could do an “if” statement like we did on the home page. Another way would be to create a separate template for posts. The “if” solution works for sites that have just a couple of content types. It aligns with the principle of “code for today,” too. Let’s assume, though, that we’ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we’re going to create a section template. Let’s restore the default single template before we forget. $ mkdir themes/zafta/layouts/post\r$ vi themes/zafta/layouts/_default/single.html\r{{ partial \"header.html\" . }}\r\u003ch1\u003e{{ .Title }}\u003c/h1\u003e\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\r Now we’ll update the post’s version of the single template. If you remember Hugo’s rules, the template engine will use this version over the default. $ vi themes/zafta/layouts/post/single.html\r{{ partial \"header.html\" . }}\r\u003ch1\u003e{{ .Title }}\u003c/h1\u003e\r\u003ch2\u003e{{ .Date.Format \"Mon, Jan 2, 2006\" }}\u003c/h2\u003e\r{{ .Content }}\r{{ partial \"footer.html\" . }}\r:wq\r Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn’t. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:9:1","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Don’t Repeat Yourself DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you’re figuring that out, accept that you’ll be doing some refactoring. Hugo makes that easy and fast, so it’s okay to delay splitting up a template. ","date":"2014-09-28","objectID":"/creating-a-new-theme/:9:2","tags":["Theme","Hugo"],"title":"Creating a New Theme","uri":"/creating-a-new-theme/"},{"categories":["Uncategorized"],"content":"Hugo uses the excellent go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in go templates. This document is a brief primer on using go templates. The go docs provide more details. ","date":"2014-07-28","objectID":"/goisforlovers/:0:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Introduction to Go Templates Go templates provide an extremely simple template language. It adheres to the belief that only the most basic of logic belongs in the template or view layer. One consequence of this simplicity is that go templates parse very quickly. A unique characteristic of go templates is they are content aware. Variables and content will be sanitized depending on the context of where they are used. More details can be found in the go docs. ","date":"2014-07-28","objectID":"/goisforlovers/:1:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Basic Syntax Go lang templates are html files with the addition of variables and functions. Go variables and functions are accessible within {{ }} Accessing a predefined variable “foo”: {{ foo }}\r Parameters are separated using spaces Calling the add function with input of 1, 2: {{ add 1 2 }}\r Methods and fields are accessed via dot notation Accessing the Page Parameter “bar” {{ .Params.bar }}\r Parentheses can be used to group items together {{ if or (isset .Params \"alt\") (isset .Params \"caption\") }} Caption {{ end }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:2:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Variables Each go template has a struct (object) made available to it. In hugo each template is passed either a page or a node struct depending on which type of page you are rendering. More details are available on the variables page. A variable is accessed by referencing the variable name. \u003ctitle\u003e{{ .Title }}\u003c/title\u003e\r Variables can also be defined and referenced. {{ $address := \"123 Main St.\"}}\r{{ $address }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:3:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Functions Go template ship with a few functions which provide basic functionality. The go template system also provides a mechanism for applications to extend the available functions with their own. Hugo template functions provide some additional functionality we believe are useful for building websites. Functions are called by using their name followed by the required parameters separated by spaces. Template functions cannot be added without recompiling hugo. Example: {{ add 1 2 }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:4:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Includes When including another template you will pass to it the data it will be able to access. To pass along the current context please remember to include a trailing dot. The templates location will always be starting at the /layout/ directory within Hugo. Example: {{ template \"chrome/header.html\" . }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:5:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Logic Go templates provide the most basic iteration and conditional logic. ","date":"2014-07-28","objectID":"/goisforlovers/:6:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Iteration Just like in go, the go templates make heavy use of range to iterate over a map, array or slice. The following are different examples of how to use range. Example 1: Using Context {{ range array }} {{ . }}\r{{ end }}\r Example 2: Declaring value variable name {{range $element := array}} {{ $element }} {{ end }}\r Example 2: Declaring key and value variable name {{range $index, $element := array}}\r{{ $index }} {{ $element }} {{ end }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:6:1","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Conditionals If, else, with, or, \u0026 and provide the framework for handling conditional logic in Go Templates. Like range, each statement is closed with end. Go Templates treat the following values as false: false 0 any array, slice, map, or string of length zero Example 1: If {{ if isset .Params \"title\" }}\u003ch4\u003e{{ index .Params \"title\" }}\u003c/h4\u003e{{ end }}\r Example 2: If -\u003e Else {{ if isset .Params \"alt\" }} {{ index .Params \"alt\" }}\r{{else}}\r{{ index .Params \"caption\" }}\r{{ end }}\r Example 3: And \u0026 Or {{ if and (or (isset .Params \"title\") (isset .Params \"caption\")) (isset .Params \"attr\")}}\r Example 4: With An alternative way of writing “if” and then referencing the same value is to use “with” instead. With rebinds the context . within its scope, and skips the block if the variable is absent. The first example above could be simplified as: {{ with .Params.title }}\u003ch4\u003e{{ . }}\u003c/h4\u003e{{ end }}\r Example 5: If -\u003e Else If {{ if isset .Params \"alt\" }} {{ index .Params \"alt\" }}\r{{ else if isset .Params \"caption\" }}\r{{ index .Params \"caption\" }}\r{{ end }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:6:2","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Pipes One of the most powerful components of go templates is the ability to stack actions one after another. This is done by using pipes. Borrowed from unix pipes, the concept is simple, each pipeline’s output becomes the input of the following pipe. Because of the very simple syntax of go templates, the pipe is essential to being able to chain together function calls. One limitation of the pipes is that they only can work with a single value and that value becomes the last parameter of the next pipeline. A few simple examples should help convey how to use the pipe. Example 1 : {{ if eq 1 1 }} Same {{ end }}\r is the same as {{ eq 1 1 | if }} Same {{ end }}\r It does look odd to place the if at the end, but it does provide a good illustration of how to use the pipes. Example 2 : {{ index .Params \"disqus_url\" | html }}\r Access the page parameter called “disqus_url” and escape the HTML. Example 3 : {{ if or (or (isset .Params \"title\") (isset .Params \"caption\")) (isset .Params \"attr\")}}\rStuff Here\r{{ end }}\r Could be rewritten as {{ isset .Params \"caption\" | or isset .Params \"title\" | or isset .Params \"attr\" | if }}\rStuff Here {{ end }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:7:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Context (aka. the dot) The most easily overlooked concept to understand about go templates is that {{ . }} always refers to the current context. In the top level of your template this will be the data set made available to it. Inside of a iteration it will have the value of the current item. When inside of a loop the context has changed. . will no longer refer to the data available to the entire page. If you need to access this from within the loop you will likely want to set it to a variable instead of depending on the context. Example: {{ $title := .Site.Title }}\r{{ range .Params.tags }}\r\u003cli\u003e \u003ca href=\"{{ $baseurl }}/tags/{{ . | urlize }}\"\u003e{{ . }}\u003c/a\u003e - {{ $title }} \u003c/li\u003e\r{{ end }}\r Notice how once we have entered the loop the value of {{ . }} has changed. We have defined a variable outside of the loop so we have access to it from within the loop. Hugo Parameters Hugo provides the option of passing values to the template language through the site configuration (for sitewide values), or through the meta data of each specific piece of content. You can define any values of any type (supported by your front matter/config format) and use them however you want to inside of your templates. ","date":"2014-07-28","objectID":"/goisforlovers/:8:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Using Content (page) Parameters In each piece of content you can provide variables to be used by the templates. This happens in the front matter. An example of this is used in this documentation site. Most of the pages benefit from having the table of contents provided. Sometimes the TOC just doesn’t make a lot of sense. We’ve defined a variable in our front matter of some pages to turn off the TOC from being displayed. Here is the example front matter: ---\rtitle: \"Permalinks\"\rdate: \"2013-11-18\"\raliases:\r- \"/doc/permalinks/\"\rgroups: [\"extras\"]\rgroups_weight: 30\rnotoc: true\r---\r Here is the corresponding code inside of the template: {{ if not .Params.notoc }}\r\u003cdiv id=\"toc\" class=\"well col-md-4 col-sm-6\"\u003e\r{{ .TableOfContents }}\r\u003c/div\u003e\r{{ end }}\r ","date":"2014-07-28","objectID":"/goisforlovers/:9:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Using Site (config) Parameters In your top-level configuration file (eg, config.yaml) you can define site parameters, which are values which will be available to you in chrome. For instance, you might declare: params:CopyrightHTML:\"Copyright \u0026#xA9; 2013 John Doe. All Rights Reserved.\"TwitterUser:\"spf13\"SidebarRecentLimit:5 Within a footer layout, you might then declare a \u003cfooter\u003e which is only provided if the CopyrightHTML parameter is provided, and if it is given, you would declare it to be HTML-safe, so that the HTML entity is not escaped again. This would let you easily update just your top-level config file each January 1st, instead of hunting through your templates. {{if .Site.Params.CopyrightHTML}}\u003cfooter\u003e\r\u003cdiv class=\"text-center\"\u003e{{.Site.Params.CopyrightHTML | safeHtml}}\u003c/div\u003e\r\u003c/footer\u003e{{end}}\r An alternative way of writing the “if” and then referencing the same value is to use “with” instead. With rebinds the context . within its scope, and skips the block if the variable is absent: {{with .Site.Params.TwitterUser}}\u003cspan class=\"twitter\"\u003e\r\u003ca href=\"https://twitter.com/{{.}}\" rel=\"author\"\u003e\r\u003cimg src=\"/images/twitter.png\" width=\"48\" height=\"48\" title=\"Twitter: {{.}}\"\ralt=\"Twitter\"\u003e\u003c/a\u003e\r\u003c/span\u003e{{end}}\r Finally, if you want to pull “magic constants” out of your layouts, you can do so, such as in this example: \u003cnav class=\"recent\"\u003e\r\u003ch1\u003eRecent Posts\u003c/h1\u003e\r\u003cul\u003e{{range first .Site.Params.SidebarRecentLimit .Site.Recent}}\r\u003cli\u003e\u003ca href=\"{{.RelPermalink}}\"\u003e{{.Title}}\u003c/a\u003e\u003c/li\u003e\r{{end}}\u003c/ul\u003e\r\u003c/nav\u003e\r ","date":"2014-07-28","objectID":"/goisforlovers/:10:0","tags":["go","golang","templates","themes","development"],"title":"(Hu)go Template Primer","uri":"/goisforlovers/"},{"categories":["Uncategorized"],"content":"Step 1. Install Hugo Goto hugo releases and download the appropriate version for your os and architecture. Save it somewhere specific as we will be using it in the next step. More complete instructions are available at installing hugo ","date":"2014-04-02","objectID":"/hugoisforlovers/:1:0","tags":["go","golang","hugo","development"],"title":"Getting Started with Hugo","uri":"/hugoisforlovers/"},{"categories":["Uncategorized"],"content":"Step 2. Build the Docs Hugo has its own example site which happens to also be the documentation site you are reading right now. Follow the following steps: Clone the hugo repository Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313 Corresponding pseudo commands: git clone https://github.com/spf13/hugo\rcd hugo\r/path/to/where/you/installed/hugo server --source=./docs\r\u003e 29 pages created\r\u003e 0 tags index created\r\u003e in 27 ms\r\u003e Web Server is available at http://localhost:1313\r\u003e Press ctrl+c to stop\r Once you’ve gotten here, follow along the rest of this page on your local build. ","date":"2014-04-02","objectID":"/hugoisforlovers/:2:0","tags":["go","golang","hugo","development"],"title":"Getting Started with Hugo","uri":"/hugoisforlovers/"},{"categories":["Uncategorized"],"content":"Step 3. Change the docs site Stop the Hugo process by hitting ctrl+c. Now we are going to run hugo again, but this time with hugo in watch mode. /path/to/hugo/from/step/1/hugo server --source=./docs --watch\r\u003e 29 pages created\r\u003e 0 tags index created\r\u003e in 27 ms\r\u003e Web Server is available at http://localhost:1313\r\u003e Watching for changes in /Users/spf13/Code/hugo/docs/content\r\u003e Press ctrl+c to stop\r Open your favorite editor and change one of the source content pages. How about changing this very file to fix the typo. How about changing this very file to fix the typo. Content files are found in docs/content/. Unless otherwise specified, files are located at the same relative location as the url, in our case docs/content/overview/quickstart.md. Change and save this file.. Notice what happened in your terminal. \u003e Change detected, rebuilding site\r\u003e 29 pages created\r\u003e 0 tags index created\r\u003e in 26 ms\r Refresh the browser and observe that the typo is now fixed. Notice how quick that was. Try to refresh the site before it’s finished building.. I double dare you. Having nearly instant feedback enables you to have your creativity flow without waiting for long builds. ","date":"2014-04-02","objectID":"/hugoisforlovers/:3:0","tags":["go","golang","hugo","development"],"title":"Getting Started with Hugo","uri":"/hugoisforlovers/"},{"categories":["Uncategorized"],"content":"Step 4. Have fun The best way to learn something is to play with it. ","date":"2014-04-02","objectID":"/hugoisforlovers/:4:0","tags":["go","golang","hugo","development"],"title":"Getting Started with Hugo","uri":"/hugoisforlovers/"},{"categories":["Uncategorized"],"content":"Move static content to static Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like ▾ \u003croot\u003e/\r▾ images/\rlogo.png\r should become ▾ \u003croot\u003e/\r▾ static/\r▾ images/\rlogo.png\r Additionally, you’ll want any files that should reside at the root (such as CNAME) to be moved to static. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:1:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Create your Hugo configuration file Hugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:2:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Set your configuration publish folder to _site The default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch, you’ll want to do one of two alternatives: Change your submodule to point to map gh-pages to public instead of _site (recommended). git submodule deinit _site\rgit rm _site\rgit submodule add -b gh-pages git@github.com:your-username/your-repo.git public\r Or, change the Hugo configuration to use _site instead of public. {\r..\r\"publishdir\": \"_site\",\r..\r}\r ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:3:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Convert Jekyll templates to Hugo templates That’s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll’s template documentation if you need to refresh your memory on how you built your blog and Hugo’s template to learn Hugo’s way. As a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:4:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Convert Jekyll plugins to Hugo shortcodes Jekyll has plugins; Hugo has shortcodes. It’s fairly trivial to do a port. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:5:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Implementation As an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing. Jekyll’s plugin: module Jekyll\rclass ImageTag \u003c Liquid::Tag\r@url = nil\r@caption = nil\r@class = nil\r@link = nil\r// Patterns\rIMAGE_URL_WITH_CLASS_AND_CAPTION =\rIMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\"(.*?)\"(\\s+)-\u003e((https?:\\/\\/|\\/)(\\S+))(\\s*)/i\rIMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\"(.*?)\"/i\rIMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i\rIMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i\rdef initialize(tag_name, markup, tokens)\rsuper\rif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK\r@class = $1\r@url = $3\r@caption = $7\r@link = $9\relsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION\r@class = $1\r@url = $3\r@caption = $7\relsif markup =~ IMAGE_URL_WITH_CAPTION\r@url = $1\r@caption = $5\relsif markup =~ IMAGE_URL_WITH_CLASS\r@class = $1\r@url = $3\relsif markup =~ IMAGE_URL\r@url = $1\rend\rend\rdef render(context)\rif @class\rsource = \"\u003cfigure class='#{@class}'\u003e\"\relse\rsource = \"\u003cfigure\u003e\"\rend\rif @link\rsource += \"\u003ca href=\\\"#{@link}\\\"\u003e\"\rend\rsource += \"\u003cimg src=\\\"#{@url}\\\"\u003e\"\rif @link\rsource += \"\u003c/a\u003e\"\rend\rsource += \"\u003cfigcaption\u003e#{@caption}\u003c/figcaption\u003e\" if @caption\rsource += \"\u003c/figure\u003e\"\rsource\rend\rend\rend\rLiquid::Template.register_tag('image', Jekyll::ImageTag)\r is written as this Hugo shortcode: \u003c!-- image --\u003e\r\u003cfigure {{ with .Get \"class\" }}class=\"{{.}}\"{{ end }}\u003e\r{{ with .Get \"link\"}}\u003ca href=\"{{.}}\"\u003e{{ end }}\r\u003cimg src=\"{{ .Get \"src\" }}\" {{ if or (.Get \"alt\") (.Get \"caption\") }}alt=\"{{ with .Get \"alt\"}}{{.}}{{else}}{{ .Get \"caption\" }}{{ end }}\"{{ end }} /\u003e\r{{ if .Get \"link\"}}\u003c/a\u003e{{ end }}\r{{ if or (or (.Get \"title\") (.Get \"caption\")) (.Get \"attr\")}}\r\u003cfigcaption\u003e{{ if isset .Params \"title\" }}\r{{ .Get \"title\" }}{{ end }}\r{{ if or (.Get \"caption\") (.Get \"attr\")}}\u003cp\u003e\r{{ .Get \"caption\" }}\r{{ with .Get \"attrlink\"}}\u003ca href=\"{{.}}\"\u003e {{ end }}\r{{ .Get \"attr\" }}\r{{ if .Get \"attrlink\"}}\u003c/a\u003e {{ end }}\r\u003c/p\u003e {{ end }}\r\u003c/figcaption\u003e\r{{ end }}\r\u003c/figure\u003e\r\u003c!-- image --\u003e\r ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:5:1","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Usage I simply changed: {% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \"One of my favorite touristy-type photos. I secretly waited for the good light while we were \"having fun\" and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\" -\u003ehttp://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %}\r to this (this example uses a slightly extended version named fig, different than the built-in figure): {{% fig class=\"full\" src=\"http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\" title=\"One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\" link=\"http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\" %}}\r As a bonus, the shortcode named parameters are, arguably, more readable. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:5:2","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Finishing touches ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:6:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Fix content Depending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:6:1","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"Clean up You’ll want to remove the Jekyll configuration at this point. If you have anything else that isn’t used, delete it. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:6:2","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"},{"categories":["Uncategorized"],"content":"A practical example in a diff Hey, it’s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff. ","date":"2014-03-10","objectID":"/migrate-from-jekyll/:7:0","tags":null,"title":"Migrate to Hugo from Jekyll","uri":"/migrate-from-jekyll/"}]