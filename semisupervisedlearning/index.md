# 半监督深度学习



## 半监督学习

- 在有标签数据+无标签数据**混合**成的训练数据中使用的机器学习算法。一般假设，无标签数据比有标签数据多，甚至多得多。

- **要求**：

  - 无标签数据一般是有标签数据中的某一个类别的（不要不属于的，也不要属于多个类别的）；
  - 有标签数据的标签应该都是对的；
  - 无标签数据一般是类别平衡的（即每一类的样本数差不多）；
  - 无标签数据的分布应该和有标签的相同或类似 。

- **半监督学习算法**

  - **简单自训练（simple self-training）**：用有标签数据训练一个分类器，然后用这个分类器对无标签数据进行分类，这样就会产生伪标签（pseudo label）或软标签（soft label），挑选认为分类正确的无标签样本（此处应该有一个**挑选准则**），把选出来的无标签样本用来训练分类器

    ![](https://gitee.com/xiaomoon/image/raw/master/img/1479233-20180920221248528-1319232835.png " ")

  - **协同训练（co-training）**：是 self-training 的一种，但其思想是好的。假设每个数据可以从不同的角度（view）进行分类，不同角度可以训练出不同的分类器，然后用这些从不同角度训练出来的分类器对无标签样本进行分类，再选出认为可信的无标签样本加入训练集中。由于这些分类器从不同角度训练出来的，可以形成一种互补，而提高分类精度；就如同从不同角度可以更好地理解事物一样。

  - **半监督字典学习**：是 self-training 的一种，先是用有标签数据作为字典，对无标签数据进行分类，挑选出认为分类正确的无标签样本，加入字典中（此时的字典就变成了半监督字典了）

  - **标签传播算法（Label Propagation Algorithm）**：是一种基于图的半监督算法，通过构造图结构（数据点为顶点，点之间的相似性为边）来寻找**训练数据**中有标签数据和无标签数据的关系。只是训练数据中，这是一种直推式的半监督算法，即只对训练集中的无标签数据进行分类，这其实感觉很像一个有监督分类算法...，但其实并不是，因为其标签传播的过程，会流经无标签数据，即有些无标签数据的标签的信息，是从另一些无标签数据中流过来的，这就用到了无标签数据之间的联系

  - **半监督支持向量机**：监督支持向量机是利用了结构风险最小化来分类的，半监督支持向量机还用上了无标签数据的空间分布信息，即决策超平面应该与无标签数据的分布一致（应该经过无标签数据密度低的地方）（**这其实是一种假设**，不满足的话这种无标签数据的空间分布信息会误导决策超平面，导致性能比只用有标签数据时还差）

- **半监督学习分类**

  - **纯（pure）半监督学习**：假定训练数据中的未标记样本并非待测的数据
  - **直推学习**：假定学习过程中所考虑的未标记样本恰是待预测数据，学习的目的就是在这些未标记样本上获得最优泛化性能

  ![](https://gitee.com/xiaomoon/image/raw/master/img/1479233-20180920213147835-6619680.png " ")

## 半监督深度学习

- 半监督深度学习**算法**：

  - 无标签数据预训练网络后有标签数据微调（fine-tune）；
  - 有标签数据训练网络，利用从网络中得到的深度特征来做半监督算法；
  - 让网络 work in semi-supervised fashion。

- 对于神经网络来说，一个好的初始化可以使得结果更稳定，迭代次数更少。目前我见过的初始化方式有两种：

  - **无监督预训练**：用所有训练数据训练自动编码器（[AutoEncoder](https://blog.csdn.net/sinat_27935693/article/details/53502656)），然后把自编码网络的参数作为初始参数，用有标签数据微调网络（验证集）。
  - **伪有监督预训练**：通过半监督算法或聚类算法等方式，给无标签数据附上伪标签信息，先用这些伪标签信息来预训练网络，然后再用有标签数据来微调网络（验证集）。

-  **有标签数据提取特征**的半监督学习

  1. 先用有标签数据训练网络（此时网络一般过拟合...）
  2. 通过隐藏层提取特征，以这些特征来用某种分类算法对无标签数据进行分类
  3. 挑选认为分类正确的无标签数据加入到训练集
  4. 重复上述过程

- 网络本身的半监督学习（**端到端的半监督深度模型**）

  - ICML 2013 的文章[Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks](http://pdfs.semanticscholar.org/798d/9840d2439a0e5d47bcf5d164aa46d5e7dc26.pdf)：该文章简单的说就是在伪标签学习中使用深度学习网络作为分类器，就是把网络对无标签数据的预测，作为无签数据的伪标签（Pseudo label），用来对网络进行训练。方法虽然简单，但是效果很好，比单纯用有标签数据有不少的提升

  - 其主要的贡献在于损失函数的构造：

    $$L=\sum_{m=1}^n\sum_{i=1}^C L(y_i^m, f_i^m)+\alpha(t)\sum_{m=1}^{n^\prime}\sum_{i=1}^CL({y^\prime}_i^m, {f^\prime}_i^m)$$

    > 损失函数的第一项是有标签数据的损失，第二项是无标签数据的损失
    > 在无标签数据的损失中， $y'$为无标签数据预测得到的伪标签，是直接取网络对无标签数据的预测的最大值为标签。
    > 其中 $\alpha (t)$决定着无标签数据的代价在网络更新的作用，选择合适的 $\alpha (t)$ 很重要，太大性能退化，太小提升有限。
    > 在网络初始时，网络的预测时不太准确的，因此生成的伪标签的准确性也不高。
    > 在初始训练时， $\alpha (t)$ 要设为 0，然后再慢慢增加，论文中给出其增长函数。
  
  - [Semi-Supervised Learning with Ladder Networks](http://www.jiqizhixin.com/wp-content/uploads/2015/11/2.-Semi-Supervised-Learning-with-Ladder-Network-.pdf)：
  
    ladderNet 是有监督算法和无监督算法的有机结合。前面提及到的无监督预训练+有监督微调的思想中所有监督和无监督是分开的，两个阶段的训练相互独立，并不能称之为真正的半监督学习。
  
    无监督学习是用重构样本进行训练，其编码（学习特征）的目的是尽可能地保留原始数据的信息；而有监督学习是用于分类，希望只保留其本质特征，去除不必要的特征。
  
    > 举例来说：分类任务判断一张人脸图片是单眼皮，还是双眼皮；那么有监督学习经过训练完毕后，就会尽可能的把与这个分类任务无关的信息过滤掉，过滤的越好，那么分类的精度将会更高。
    > 比如一个人的嘴巴、鼻子信息这些都是与这个分类任务无关的，那么就要尽量的过滤掉，因此，基于这个原因以至于一直以来有监督学习和无监督学习不能很好的兼容在一起。
    > ladderNet 成功的原因在于损失函数和 skip connection 。通过在每层的编码器和解码器之间添加跳跃连接（skip connection），减轻模型较高层表示细节的压力，使得无监督学习和有监督学习能结合在一起，并在最高层添加分类器。
    >$$Cost=-\sum_{n=1}^NlogP(\hat{y}(n)=y^*(n)\mid x(n))+\sum_{n=N+1}^M\lambda_l ReconsructionCost({z^{(l)}}_{(n)},{\hat{z}^{(l)}}_{(n)})$$
    > 损失函数的第一项是有标签样本数据的交叉熵损失函数，第二项是无监督各层噪声解码器重构误差欧式损失函数

## 参考

- [半监督学习](https://www.cnblogs.com/kamekin/p/9683162.html)

- [半监督深度学习小结](https://zhuanlan.zhihu.com/p/33196506)
- [Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks](http://pdfs.semanticscholar.org/798d/9840d2439a0e5d47bcf5d164aa46d5e7dc26.pdf)
- [Semi-Supervised Learning with Ladder Networks](http://www.jiqizhixin.com/wp-content/uploads/2015/11/2.-Semi-Supervised-Learning-with-Ladder-Network-.pdf)


