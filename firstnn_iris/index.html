<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1"><meta name="robots" content="noodp"/><title>TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码） | Moon&#39;s Blog</title><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>
<meta name="twitter:title" content="TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）"/>
<meta name="twitter:description" content=""/><meta name="twitter:creator" content="@MoonXiaomu"/><meta name="Description" content="KEEP KWARKING"><meta property="og:title" content="TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）" />
<meta property="og:description" content="根据前面的基础知识，可以开始第一个神经网络的搭建，主要学习的资料西安科技大学：神经网络与深度学习——TensorFlow2.0实战，北京大学" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.aimoon.top/firstnn_iris/" /><meta property="og:image" content="https://blog.aimoon.top/images/favicon.svg"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-12T22:53:27&#43;08:00" />
<meta property="article:modified_time" content="2021-03-29T11:34:14&#43;08:00" /><meta property="og:site_name" content="Moon&#39;s Blog" />

<meta name="application-name" content="MOON">
<meta name="apple-mobile-web-app-title" content="MOON"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="/images/favicon.svg" type="image/x-icon"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.aimoon.top/firstnn_iris/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.aimoon.top\/firstnn_iris\/"
        },"image": ["https:\/\/blog.aimoon.top\/images\/cover.png"],"genre": "posts","keywords": "NN, 神经网络","wordCount":  3294 ,
        "url": "https:\/\/blog.aimoon.top\/firstnn_iris\/","datePublished": "2020-05-12T22:53:27+08:00","dateModified": "2021-03-29T11:34:14+08:00",
        "publisher": {
            "@type": "Person",
            "name": "Wang Yuexin", "image": [
            {
            "@type": "ImageObject",
            "url": "https:\/\/blog.aimoon.top\/images\/avatars.png"
            }
            ]},"author": {
                "@type": "Person",
                "name": "Wang Yuexin"
            },"description": ""
    }
    </script><script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [{
            "@type": "ListItem",
            "position": 1,
            "name": "主页",
            "item": "https:\/\/blog.aimoon.top"
        },{
            "@type": "ListItem",
            "position": 2,
            "name": "TF2.1学习笔记",
            "item": "https://blog.aimoon.top/categories/tf2.1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
        },{
                "@type": "ListItem",
                "position": 3,
                "name": "TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）"
            }]
    }
</script></head>
    <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header>
    <div class="desktop header" id="header-desktop">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Moon&#39;s Blog" class="header-logo logo-svg">Moon&#39;s Blog</a>
            </div>
            <div class="menu">
                <nav>
                    <h2 class="display-hidden">Основная навигация</h2>
                    <ul class="menu-inner"><li>
                            <a class="menu-item" href="/posts/"> 目录 </a>
                        </li><li>
                            <a class="menu-item" href="/tags/"> 标签 </a>
                        </li><li>
                            <a class="menu-item" href="/categories/"> 归档 </a>
                        </li><li>
                            <a class="menu-item" href="https://aimoon.top" rel="noopener noreffer" target="_blank"> 关于 </a>
                        </li><li>
                            <a class="menu-item" href="/music/"> 歌单 </a>
                        </li><li>
                            <a class="menu-item" href="/comments/"> 留言板 </a>
                        </li></ul>
                </nav><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="search……" id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <span class="svg-icon icon-search"></span>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <span class="svg-icon icon-cancel"></span>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <span class="svg-icon icon-loading"></span>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <span class="svg-icon icon-moon"></span>
                </a>
            </div>
        </div>
    </div><div class="mobile header" id="header-mobile">
        <div class="header-container">
            <div class="header-wrapper">
                <div class="header-title">
                    <a href="/" title="Moon&#39;s Blog" class="header-logo">Moon&#39;s Blog</a>
                </div>
                <div class="menu-toggle" id="menu-toggle-mobile">
                    <span></span><span></span><span></span>
                </div>
            </div>
            <div class="menu" id="menu-mobile"><div class="search-wrapper">
                        <div class="search mobile" id="search-mobile">
                            <input type="text" placeholder="search……" id="search-input-mobile">
                            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                                <span class="svg-icon icon-search"></span>
                            </a>
                            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                                <span class="svg-icon icon-cancel"></span>
                            </a>
                            <span class="search-button search-loading" id="search-loading-mobile">
                                <span class="svg-icon icon-loading"></span>
                            </span>
                        </div>
                        <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                            取消
                        </a>
                    </div><nav>
                    <h2 class="display-hidden">Основная навигация</h2>
                    <ul><li>
                            <a class="menu-item" href="/posts/" title="">目录</a>
                        </li><li>
                            <a class="menu-item" href="/tags/" title="">标签</a>
                        </li><li>
                            <a class="menu-item" href="/categories/" title="">归档</a>
                        </li><li>
                            <a class="menu-item" href="https://aimoon.top" title="" rel="noopener noreffer" target="_blank">关于</a>
                        </li><li>
                            <a class="menu-item" href="/music/" title="">歌单</a>
                        </li><li>
                            <a class="menu-item" href="/comments/" title="">留言板</a>
                        </li></ul>
                </nav>
                <a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <span class="svg-icon icon-moon"></span>
                </a></div>
        </div>
    </div><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div></header><main class="main">
<div class="container content-article page-toc theme-classic"><div class="toc" id="toc-auto">
            <div class="toc-title">目录</div>
            <div class="toc-content" id="toc-content-auto"></div>
        </div>
    

    
    
    <article>
    

        <header class="header-post">

            

            
            <div class="post-title">

                    <div class="post-all-meta">
                        <nav class="breadcrumbs">
    <ol>
        <li><a href="/">主页 </a></li><li><a href="/categories/tf2.1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">TF2.1学习笔记 </a></li><li>TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）</li>
    </ol>
</nav>
                        <h1 class="single-title flipInX">TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）</h1><div class="post-meta summary-post-meta"><span class="post-category meta-item">
                                <a href="/categories/tf2.1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="svg-icon icon-folder"></span>TF2.1学习笔记</a>
                            </span><span class="post-meta-date meta-item">
                                <span class="svg-icon icon-clock"></span><time class="timeago" datetime="2020-05-12">2020-05-12</time>
                            </span><span class="post-meta-words meta-item">
                                <span class="svg-icon icon-pencil"></span>约 3294 字
                            </span>
                            <span class="post-meta-reading meta-item">
                                <span class="svg-icon icon-stopwatch"></span>预计阅读 7 分钟
                            </span>
                        </div>

                    </div>

                </div>

                </header>

        <div class="article-post toc-start">

            <div class="content-block content-block-first content-block-position">

                <div class="post single"><div class="details toc" id="toc-static"  data-kept="">
                        <div class="details-summary toc-title">
                            <span>目录</span>
                        </div>
                        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1问题背景">1.问题背景</a>
          <ul>
            <li><a href="#问题描述">问题描述</a></li>
            <li><a href="#流程设计">流程设计</a></li>
            <li><a href="#模型设计">模型设计</a>
              <ul>
                <li>
                  <ul>
                    <li><a href="#搭建网络模型">搭建网络模型</a></li>
                    <li><a href="#转换为数学模型">转换为数学模型</a></li>
                    <li><a href="#搭建网络">搭建网络</a></li>
                    <li><a href="#前向传播">前向传播</a></li>
                    <li><a href="#损失函数">损失函数</a></li>
                    <li><a href="#梯度下降">梯度下降</a></li>
                    <li><a href="#反向传播">反向传播</a></li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#2数据读入">2.数据读入</a>
          <ul>
            <li><a href="#数据集介绍">数据集介绍：</a></li>
            <li><a href="#数据预处理">数据预处理</a></li>
            <li><a href="#数据训练">数据训练</a></li>
            <li><a href="#数据可视化">数据可视化</a></li>
          </ul>
        </li>
        <li><a href="#3完整源码">3.完整源码</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
                    </div><p>根据前面的基础知识，可以开始第一个神经网络的搭建，主要学习的资料西安科技大学：<a href="https://www.icourse163.org/learn/XUST-1206363802#/learn/announce" target="_blank" rel="noopener noreffer">神经网络与深度学习——TensorFlow2.0实战</a>，北京大学：<a href="https://www.icourse163.org/learn/PKU-1002536002#/learn/announce" target="_blank" rel="noopener noreffer">人工智能实践Tensorflow笔记</a></p>
<!-- more -->
<h3 id="1问题背景" class="headerLink"><a href="#1%e9%97%ae%e9%a2%98%e8%83%8c%e6%99%af" class="header-mark"></a>1.问题背景</h3><h4 id="问题描述" class="headerLink"><a href="#%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0" class="header-mark"></a>问题描述</h4><p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512181537793.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512181537793.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<p>人们通过经验总结出的规律：通过测量鸢尾花的花萼长、花萼宽、花瓣长、花瓣宽，可以得出鸢尾花的类别。（如：花萼长&gt;花萼宽 且 花瓣长/花瓣宽&gt;2 则为杂色鸢尾花）。大量依靠人工分类工作量巨大，不同的人员分类，标准，准确率都会有所差距。可以借助深度学习来学习其中的特征并对新数据进行预测。</p>
<h4 id="流程设计" class="headerLink"><a href="#%e6%b5%81%e7%a8%8b%e8%ae%be%e8%ae%a1" class="header-mark"></a>流程设计</h4><ul>
<li>大量的[花萼长、花萼宽、花瓣长、花瓣宽（输入特征），对应的类别（标签）]数据对构成数据集</li>
<li>把数据集喂入搭建好的神经网络结构</li>
<li>网络优化参数得到模型</li>
<li>模型读入新输入特征，输出识别结果</li>
</ul>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512181625192.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512181625192.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h4 id="模型设计" class="headerLink"><a href="#%e6%a8%a1%e5%9e%8b%e8%ae%be%e8%ae%a1" class="header-mark"></a>模型设计</h4><h6 id="搭建网络模型" class="headerLink"><a href="#%e6%90%ad%e5%bb%ba%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b" class="header-mark"></a>搭建网络模型</h6><p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512181759189.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512181759189.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h6 id="转换为数学模型" class="headerLink"><a href="#%e8%bd%ac%e6%8d%a2%e4%b8%ba%e6%95%b0%e5%ad%a6%e6%a8%a1%e5%9e%8b" class="header-mark"></a>转换为数学模型</h6><p>所有输入特征x与相应特征权重w相乘加上偏置项b输出结果y。
x：一行四列矩阵，对应四个特征
w：四行三列矩阵
b：3个偏置项
y：一行三列矩阵，对应三种类别的可信度</p>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512181930231.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512181930231.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h6 id="搭建网络" class="headerLink"><a href="#%e6%90%ad%e5%bb%ba%e7%bd%91%e7%bb%9c" class="header-mark"></a>搭建网络</h6><p>每个神经元$y_0,y_1,y_2与输入节点x_0,x_1,x_2,x_3$都有联系，称为全连接神经网络权重w与偏置项b会随机初始化一组参数</p>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512185443354.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512185443354.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h6 id="前向传播" class="headerLink"><a href="#%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad" class="header-mark"></a>前向传播</h6><p>神经网络执行y = x * w + b的过程称为前向传播</p>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512185915493.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512185915493.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h6 id="损失函数" class="headerLink"><a href="#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0" class="header-mark"></a>损失函数</h6><p>损失函数：预测值(y)与标准答案($y_i$)的差距，可以定量判断w，b的优劣，当损失函数输出最小时会出现最优解。（有多种损失函数，这里用均方误差）</p>
<ul>
<li>均方误差：$MSE(y,y_i)=\frac{\sum_{k=0}^n(y-y_i)^2}{n}$</li>
</ul>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512215140250.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512215140250.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h6 id="梯度下降" class="headerLink"><a href="#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" class="header-mark"></a>梯度下降</h6><p>目的：找到一组参数w和b，使得损失函数最小。
梯度：函数对个参数求偏导后的向量，梯度下降的方向是函数减小的方向。
<font color=red>梯度下降：</font>延损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数。
<font color=red>学习率(learning rate, lr)：</font>当学习率设置过小时，收敛过程将变得十分缓慢。当学习率设置过大时，梯度可能会在最小值附近震荡，甚至无法收敛</p>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/2020051221584380.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/2020051221584380.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h6 id="反向传播" class="headerLink"><a href="#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad" class="header-mark"></a>反向传播</h6><p>$w_{t+1}=w_t-lr*\frac{\partial loss}{\partial w_t}$
从前向后，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数。</p>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512221012828.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512221012828.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<h3 id="2数据读入" class="headerLink"><a href="#2%e6%95%b0%e6%8d%ae%e8%af%bb%e5%85%a5" class="header-mark"></a>2.数据读入</h3><h4 id="数据集介绍" class="headerLink"><a href="#%e6%95%b0%e6%8d%ae%e9%9b%86%e4%bb%8b%e7%bb%8d" class="header-mark"></a>数据集介绍：</h4><p>该数据集已集成在sklearn包中，可直接调入使用，数据共有150组，每组包括花萼长、花萼宽、花瓣长、花瓣宽共四个输入特征。同时给出了这一组特征的的对应鸢尾花类别。类别包括Setosa Iris（狗尾草鸢尾），Versicolour Iris（杂色鸢尾），Viginaica Iris（弗吉尼亚鸢尾）三类，分别用数字0，1，2表示</p>
<p>




<figure class="render-image"><a target="_blank" href="https://img-blog.csdnimg.cn/20200512182800425.png" title=" " >
        <img loading="lazy" decoding="async"
             class="render-image"
             src="https://img-blog.csdnimg.cn/20200512182800425.png"
            alt=" "
        />
    </a><figcaption class="image-caption"> </figcaption>
</figure></p>
<ul>
<li>从sklearn包datasets读入数据集</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>		<span class="c1">#读入iris数据集的所有输入特征</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span><span class="o">.</span><span class="n">target</span>	<span class="c1">#读入iris数据集所有标签</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="数据预处理" class="headerLink"><a href="#%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86" class="header-mark"></a>数据预处理</h4><ul>
<li>数据集乱序：随机打乱数据</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># seed: 随机数种子，是一个整数，当设置之后，每次生征和标签一一对应</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">116</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">116</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">116</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li>将数据集分成训练集和测试集</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 训练集为前120行，测试集为后30行</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_data</span><span class="p">[:</span><span class="o">-</span><span class="mi">30</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_data</span><span class="p">[:</span><span class="o">-</span><span class="mi">30</span><span class="p">]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_data</span><span class="p">[</span><span class="o">-</span><span class="mi">30</span><span class="p">:]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_data</span><span class="p">[</span><span class="o">-</span><span class="mi">30</span><span class="p">:]</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li>输入特征和标签值一一对应，把数据集分批次，每个批次batch(32)组数据</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_db</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">test_db</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="数据训练" class="headerLink"><a href="#%e6%95%b0%e6%8d%ae%e8%ae%ad%e7%bb%83" class="header-mark"></a>数据训练</h4><ul>
<li>定义神经网络中所有参数可训练</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li><strong><font color=red>嵌套循环迭代，with结构更新参数，显示当前loss</font></strong></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>  <span class="c1"># 数据集级别迭代</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_db</span><span class="p">):</span>  <span class="c1"># batch级别的迭代</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>  <span class="c1"># 记录梯度信息</span>
        	<span class="c1"># 前向传播过程计算y</span>
        	<span class="c1"># 计算总loss</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">])</span>	<span class="c1"># 求导</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 参数w1自更新</span>
        <span class="n">b1</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># 参数b自更新</span>
    <span class="c1"># 每个epoch，打印loss信息</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Epoch {}, loss: {}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss_all</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li>计算当前参数前向传播后的准确率，显示当前acc（accuracy）</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python">    <span class="k">for</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="ow">in</span> <span class="n">test_db</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>	<span class="c1"># y为预测结果</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>	<span class="c1"># y符合概率分布</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 返回y中最大值的索引，即预测的分类</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y_test</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>	<span class="c1"># 调整参数类型与标签一致</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>	<span class="c1"># 将所有batch中的correct数加起来</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>	<span class="c1"># 将所有batch中的correct数加起来</span>
        <span class="n">total_number</span> <span class="o">+=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_number</span>
    <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Test_acc:&#34;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="数据可视化" class="headerLink"><a href="#%e6%95%b0%e6%8d%ae%e5%8f%af%e8%a7%86%e5%8c%96" class="header-mark"></a>数据可视化</h4><ul>
<li>loss可视化</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Function Curve&#39;</span><span class="p">)</span>  <span class="c1"># 图片标题</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>  <span class="c1"># x轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>  <span class="c1"># y轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss_results</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;$Loss$&#34;</span><span class="p">)</span>  <span class="c1"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>  <span class="c1"># 画出曲线图标</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># 画出图像</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li>acc可视化</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Acc Curve&#39;</span><span class="p">)</span>  <span class="c1"># 图片标题</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>  <span class="c1"># x轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Acc&#39;</span><span class="p">)</span>  <span class="c1"># y轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;$Accuracy$&#34;</span><span class="p">)</span>  <span class="c1"># 逐点画出test_acc值并连线，连线图标是Accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="3完整源码" class="headerLink"><a href="#3%e5%ae%8c%e6%95%b4%e6%ba%90%e7%a0%81" class="header-mark"></a>3.完整源码</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 导入所需模块</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># 导入数据，分别为输入特征和标签</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span><span class="o">.</span><span class="n">data</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># 随机打乱数据（因为原始数据是顺序的，顺序不打乱会成的随机数都一样</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">116</span><span class="p">)</span>  <span class="c1"># 使用相同的seed，保证输入特影响准确率</span>
<span class="c1"># seed: 随机数种子，是一个整数，当设置之后，每次生征和标签一一对应</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">116</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">116</span><span class="p">)</span>

<span class="c1"># 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_data</span><span class="p">[:</span><span class="o">-</span><span class="mi">30</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_data</span><span class="p">[:</span><span class="o">-</span><span class="mi">30</span><span class="p">]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_data</span><span class="p">[</span><span class="o">-</span><span class="mi">30</span><span class="p">:]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_data</span><span class="p">[</span><span class="o">-</span><span class="mi">30</span><span class="p">:]</span>

<span class="c1"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span>
<span class="n">train_db</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">test_db</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元</span>
<span class="c1"># 用tf.Variable()标记参数可训练</span>
<span class="c1"># 使用seed使每次生成的随机数相同</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># 学习率为0.1</span>
<span class="n">train_loss_results</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># 循环500轮</span>
<span class="n">loss_all</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span>

<span class="c1"># 训练部分</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>  <span class="c1">#数据集级别的循环，每个epoch循环一次数据集</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_db</span><span class="p">):</span>  <span class="c1">#batch级别的循环 ，每个step循环一个batch</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>  <span class="c1"># with结构记录梯度信息</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># 神经网络乘加运算</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 将标签值转换为独热码格式，方便计算loss和accuracy</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>  <span class="c1"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span>
            <span class="n">loss_all</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span>
        <span class="c1"># 计算loss对各个参数的梯度</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">])</span>

        <span class="c1"># 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 参数w1自更新</span>
        <span class="n">b1</span><span class="o">.</span><span class="n">assign_sub</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># 参数b自更新</span>

    <span class="c1"># 每个epoch，打印loss信息</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Epoch {}, loss: {}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss_all</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">train_loss_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_all</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># 将4个step的loss求平均记录在此变量中</span>
    <span class="n">loss_all</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># loss_all归零，为记录下一个epoch的loss做准备</span>

    <span class="c1"># 测试部分</span>
    <span class="c1"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span>
    <span class="n">total_correct</span><span class="p">,</span> <span class="n">total_number</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="ow">in</span> <span class="n">test_db</span><span class="p">:</span>
        <span class="c1"># 使用更新后的参数进行预测</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 返回y中最大值的索引，即预测的分类</span>
        <span class="c1"># 将pred转换为y_test的数据类型</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y_test</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="c1"># 将每个batch的correct数加起来</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>
        <span class="c1"># 将所有batch中的correct数加起来</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>
        <span class="c1"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span>
        <span class="n">total_number</span> <span class="o">+=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># 总的准确率等于total_correct/total_number</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_number</span>
    <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Test_acc:&#34;</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&#34;--------------------------&#34;</span><span class="p">)</span>

<span class="c1"># 绘制 loss 曲线</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss Function Curve&#39;</span><span class="p">)</span>  <span class="c1"># 图片标题</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>  <span class="c1"># x轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>  <span class="c1"># y轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss_results</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;$Loss$&#34;</span><span class="p">)</span>  <span class="c1"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>  <span class="c1"># 画出曲线图标</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># 画出图像</span>

<span class="c1"># 绘制 Accuracy 曲线</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Acc Curve&#39;</span><span class="p">)</span>  <span class="c1"># 图片标题</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>  <span class="c1"># x轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Acc&#39;</span><span class="p">)</span>  <span class="c1"># y轴变量名称</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;$Accuracy$&#34;</span><span class="p">)</span>  <span class="c1"># 逐点画出test_acc值并连线，连线图标是Accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></td></tr></table>
</div>
</div><p><strong><font size=5><a href="https://www.cnblogs.com/moonspace/p/12879489.html" target="_blank" rel="noopener noreffer">博客园链接</a></font></strong></p>
</div><footer>
                        <div class="post">


<div class="post-share"><div class="share-link">
        <a class="share-icon share-twitter" href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://blog.aimoon.top/firstnn_iris/" data-title="TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）" data-via="MoonXiaomu" data-hashtags="NN,神经网络"><span class="svg-social-icon icon-twitter"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-facebook" href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.aimoon.top/firstnn_iris/" data-hashtag="NN"><span class="svg-social-icon icon-facebook"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-whatsapp" href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://blog.aimoon.top/firstnn_iris/" data-title="TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）" data-web><span class="svg-social-icon icon-whatsapp"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-blogger" href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="https://blog.aimoon.top/firstnn_iris/" data-title="TensorFlow2.1入门学习笔记(5)——构建第一个神经网络，鸢尾花分类问题（附源码）" data-description=""><span class="svg-social-icon icon-blogger"></span></a>
    </div></div>

<div class="footer-post-author">
    <div class="author-avatar"><a href="https://aimoon.top" target="_blank"><img alt="Undergraduate Student of Artificial Intelligence 😜" src="https://blog.aimoon.top/images/avatars.png"></a></div>
    <div class="author-info">
        <div class="name"><a href="https://aimoon.top" target="_blank">Wang Yuexin</a></div>
        <div class="number-posts">Undergraduate Student of Artificial Intelligence 😜</span></div>
    </div>
</div><div class="post-tags"><a href="/tags/nn/" class="tag">NN</a><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="tag">神经网络</a></div></div>
                </footer></div>
        <div id="toc-final"></div>
        </div>

    
    </article>
    <section class="page single comments content-block-position">
        <h1 class="display-hidden">Комментарии</h1><div id="comments"></div></section></div>

</main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><div><span id="timeDate">正在烧脑计算建站时间...</span><span id="times"></span><script>var now = new Date();function createtime(){var grt= new Date("05/20/2020 00:00:00");now.setTime(now.getTime()+250);days = (now - grt ) / 1000 / 60 / 60 / 24;dnum = Math.floor(days);hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);hnum = Math.floor(hours);if(String(hnum).length ==1 ){hnum = "0" + hnum; }minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);mnum = Math.floor(minutes);if(String(mnum).length ==1 ){mnum = "0" + mnum;}seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);snum = Math.round(seconds);if(String(snum).length ==1 ){snum = "0" + snum;}document.getElementById("timeDate").innerHTML = "&nbsp"+dnum+"&nbsp天";document.getElementById("times").innerHTML = hnum + "&nbsp小时&nbsp" + mnum + "&nbsp分&nbsp" + snum + "&nbsp秒";}setInterval("createtime()",250);</script></div></div><div class="footer-line"><i class="svg-icon icon-copyright"></i><span>2020 - 2021</span><span class="author">&nbsp;<a href="https://aimoon.top" target="_blank">MOON</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="http://www.beian.miit.gov.cn" target="_blank">CC BY-NC 4.0</a></span><span class="icp-splitter">&nbsp;|&nbsp;</span><br class="icp-br"/>
                    <span class="icp"><a href="https://blog.pangao.vip/icp/xmoon,info">🧑ICP证000000号</a></span></div>
        </div>
    </footer></div>

        <aside id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="svg-icon icon-arrow-up"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="svg-icon icon-comments-fixed"></i>
            </a>
        </aside><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><script src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lunr@2.3.8/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="https://cdn.jsdelivr.net/npm/twemoji@13.0.0/dist/twemoji.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js"></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js"></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":30,"type":"lunr"},"twemoji":true};</script><script src="/js/theme.min.js"></script><script>
                (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	        ga('create', 'UA-167439955-2', 'auto');
	        ga('set', 'anonymizeIp', true);
	        ga('send', 'pageview');
	    </script></body>
</html>
